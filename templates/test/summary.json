[
    {
        "question": "What are the unique aspects of each model mentioned?",
        "answer": "The INDUS models, developed for scientific applications, exhibit several unique aspects. INDUSBASE, following the RoBERTaBASE architecture, consists of 12 layers and 125 million parameters. It was trained using the INDUSBPE tokenizer, specifically designed for scientific texts through byte-pair encoding. This model was pretrained on curated scientific corpora and fine-tuned for sentence embeddings using contrastive learning, achieving high performance on tasks like BC5-chem with an F1 score of 93.3, outperforming RoBERTaBASE and SCIBERT (bhattacharjee2024induseffectiveand pages 3-4; bhattacharjee2024induseffectiveand pages 6-7).\n\nINDUSSMALL, a distilled version of INDUSBASE, features a 4-layer architecture with 38 million parameters. It was developed through knowledge distillation, using INDUSBASE as the teacher model, and optimized for a balance between performance and latency. This model also demonstrated superior performance on BC5-chem, with an F1 score of 90.7, surpassing MINILM and TINYBERT (bhattacharjee2024induseffectiveand pages 3-4; bhattacharjee2024induseffectiveand pages 6-7).\n\nThe INDUS models also include retrievers, such as INDUS-RETRIEVERBASE and INDUS-RETRIEVERSMALL, which have shown competitive results on the BLURB benchmark, highlighting their effectiveness in natural language understanding and reasoning tasks within the biomedical domain (bhattacharjee2024induseffectiveand pages 7-7).",
        "references": "1. (bhattacharjee2024induseffectiveand pages 6-7): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\n2. (bhattacharjee2024induseffectiveand pages 3-4): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\n3. (bhattacharjee2024induseffectiveand pages 7-7): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
        "cost": 0.05168249999999999,
        "token_counts": {
            "gpt-4o-2024-08-06": [
                13417,
                1814
            ]
        },
        "used_contexts": [
            "bhattacharjee2024induseffectiveand pages 6-7",
            "bhattacharjee2024induseffectiveand pages 7-7",
            "bhattacharjee2024induseffectiveand pages 3-4"
        ],
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "doc_details": {
                    "embedding": null,
                    "docname": "bhattacharjee2024induseffectiveand",
                    "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
                    "dockey": "2b647dabce87f56c",
                    "overwrite_fields_from_metadata": true,
                    "key": "bhattacharjee2024induseffectiveand",
                    "bibtex": "@article{bhattacharjee2024induseffectiveand,\n    author = \"Bhattacharjee, Bishwaranjan and Trivedi, Aashka and Muraoka, Masayasu and Ramasubramanian, Muthukumaran and Udagawa, Takuma and Gurung, I. and Zhang, Rong and Dandala, Bharath and Ramachandran, Rahul and Maskey, M. and Bugbee, Kayleen and Little, Mike and Fancher, Elizabeth and Sanders, Lauren and Costes, Sylvain and Blanco-Cuaresma, Sergi and Lockhart, Kelly and Allen, Thomas and Grazes, Felix and Ansdel, Megan and Accomazzi, A. and El-Kurdi, Yousef and Wertheimer, Davis and Pfitzmann, Birgit and Ramis, Cesar Berrospi and Dolfi, Michele and de Lima, Rafael Teixeira and Vegenas, Panos and Mukkavilli, S. K. and Staar, P. and Vahidinia, S. and McGranaghan, Ryan and Mehrabian, A. and Lee, Tsendgar\",\n    title = \"INDUS: Effective and Efficient Language Models for Scientific Applications\",\n    year = \"2024\",\n    journal = \"ArXiv\",\n    volume = \"abs/2405.10725\",\n    month = \"May\",\n    doi = \"10.48550/arxiv.2405.10725\",\n    url = \"https://doi.org/10.48550/arxiv.2405.10725\"\n}\n",
                    "authors": [
                        "Bishwaranjan Bhattacharjee",
                        "Aashka Trivedi",
                        "Masayasu Muraoka",
                        "Muthukumaran Ramasubramanian",
                        "Takuma Udagawa",
                        "I. Gurung",
                        "Rong Zhang",
                        "Bharath Dandala",
                        "Rahul Ramachandran",
                        "M. Maskey",
                        "Kayleen Bugbee",
                        "Mike Little",
                        "Elizabeth Fancher",
                        "Lauren Sanders",
                        "Sylvain Costes",
                        "Sergi Blanco-Cuaresma",
                        "Kelly Lockhart",
                        "Thomas Allen",
                        "Felix Grazes",
                        "Megan Ansdel",
                        "A. Accomazzi",
                        "Yousef El-Kurdi",
                        "Davis Wertheimer",
                        "Birgit Pfitzmann",
                        "Cesar Berrospi Ramis",
                        "Michele Dolfi",
                        "Rafael Teixeira de Lima",
                        "Panos Vegenas",
                        "S. K. Mukkavilli",
                        "P. Staar",
                        "S. Vahidinia",
                        "Ryan McGranaghan",
                        "A. Mehrabian",
                        "Tsendgar Lee"
                    ],
                    "publication_date": "2024-05-17T00:00:00",
                    "year": 2024,
                    "volume": "abs/2405.10725",
                    "issue": null,
                    "issn": null,
                    "pages": null,
                    "journal": "ArXiv",
                    "publisher": null,
                    "url": null,
                    "title": "INDUS: Effective and Efficient Language Models for Scientific Applications",
                    "citation_count": 4,
                    "bibtex_type": "article",
                    "source_quality": -1,
                    "is_retracted": null,
                    "doi": "10.48550/arxiv.2405.10725",
                    "doi_url": "https://doi.org/10.48550/arxiv.2405.10725",
                    "doc_id": "2b647dabce87f56c",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "d50eb618b75ce6f837fd65245393f1b28ab1d911",
                        "externalIds": {
                            "DBLP": "journals/corr/abs-2405-10725",
                            "ArXiv": "2405.10725",
                            "DOI": "10.48550/arXiv.2405.10725",
                            "CorpusId": 269899887
                        },
                        "venue": "arXiv.org",
                        "citationCount": 4,
                        "influentialCitationCount": 0,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle"
                        ],
                        "publicationDate": "2024-05-17",
                        "citationStyles": {
                            "bibtex": "@Article{Bhattacharjee2024INDUSEA,\n author = {Bishwaranjan Bhattacharjee and Aashka Trivedi and Masayasu Muraoka and Muthukumaran Ramasubramanian and Takuma Udagawa and I. Gurung and Rong Zhang and Bharath Dandala and Rahul Ramachandran and M. Maskey and Kayleen Bugbee and Mike Little and Elizabeth Fancher and Lauren Sanders and Sylvain Costes and Sergi Blanco-Cuaresma and Kelly Lockhart and Thomas Allen and Felix Grazes and Megan Ansdel and A. Accomazzi and Yousef El-Kurdi and Davis Wertheimer and Birgit Pfitzmann and Cesar Berrospi Ramis and Michele Dolfi and Rafael Teixeira de Lima and Panos Vegenas and S. K. Mukkavilli and P. Staar and S. Vahidinia and Ryan McGranaghan and A. Mehrabian and Tsendgar Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {INDUS: Effective and Efficient Language Models for Scientific Applications},\n volume = {abs/2405.10725},\n year = {2024}\n}\n"
                        },
                        "matchScore": 201.18681,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt describes two models: INDUSBASE and INDUSSMALL. INDUSBASE follows the RoBERTaBASE architecture with 12 layers and 125 million parameters, trained for 500,000 steps using 192 V100 GPUs. It uses the INDUSBPE tokenizer, which was trained from scratch. INDUSSMALL, a smaller model with 38 million parameters, was developed through knowledge distillation using INDUSBASE as the teacher. It employs a 4-layer architecture optimized by the Neural Architecture Search engine for a balance between performance and latency. The distillation objective is based on MiniLMv2, focusing on temperature-scaled cosine similarity.\n\n8",
                        "score": 8,
                        "text": "tem (ESDIS).  It contains text   model architecture follows RoBERTaBASE (Liu et al.,\n   descriptions of the NASA Earth science data    2019), which consists of 12 layers and has 125M\n   products.                                       parameters. We adopted the default hyperparame-\n                                                            ters7 but with an effective batch size of 92, 16. We\n3  Methodology: Encoder Models              trained the model for 500K steps using 192 V100\n                                         GPUs.\n3.1  INDUSBPE Tokenizer\n                                                    3.3  Knowledge Distillation for EfficientWe trained BPE tokenizer (Radford et al., 2019),\n                                              Encoder ModelINDUSBPE from scratch using a subset of our train-\n                                  We also trained a smaller model, INDUSSMALL, with\n   2https://www.ncbi.nlm.nih.gov/pmc             38M parameters through knowledge distillation\n   3https://www.ametsoc.org/index.cfm/ams/publications/\n    4https://agupubs.onlinelibrary.wiley.com/                 6We  used HF  tokenizers,  https://github.com/\n   5https://www.earthdata.nasa.gov/eosdis/science-system-    huggingface/tokenizers\ndescription/eosdis-components/cmr                        7We refer readers to Table 9 in (Liu et al., 2019). Input text\n novel tau biomarkers phosphorylated at t181, t217 or t231 rise in the initial stages of the preclinical\n alzheimer\u2019s continuum when only subtle changes in a pathology are detected\n Tokenization by RoBERTa\n <s> no vel t au biomark ers phosph ory lated at t 181 , t 217 , or t 231 rise in the initial stages of the preclinical\n  al z heimer \u2019 s continuum when only subtle changes in a pathology are detected </s>\n Tokenization by INDUSBPE\n <s> novel tau biomarkers phosphorylated at t 181 , t 217 , or t 231 rise in the initial stages of the preclinical\n alzheimer \u2019 s continuum when only subtle changes in a pathology are detected </s>\n\nTable 3: Tokenization comparison between RoBERTa and our tokenizers. Input text adapted from Su\u00e1rez-Calvet\net al. (2020).\n\n\n                   X   Xtechniques by using INDUSBASE as the teacher. IN-          Zi =      es(qi,pj) +     es(qj,pi)\nDUSSMALL follows a 4-layer architecture recom-                         j                  j\n                                                                                                        (2)\nmended by the Neural Architecture Search engine                   X es(qi,qj) + X es(pi,pj)                                  +\n(Trivedi et al., 2023) with an optimal trade-off be-\n                                                                                          j\u0338=i               j\u0338=i\ntween performance and latency. We adopted the\n                                               where s(q, p) is a measure of temperature-scaled\ndistillation objective proposed in MiniLMv2 (Wang\n                                                    cosine similarity betwee"
                    },
                    {
                        "context": "The excerpt discusses the evaluation of INDUS models against open-source models of similar sizes. INDUSBASE is compared to RoBERTaBASE and SCIBERT, while INDUSSMALL is compared to MINILM (6-layer) and TINYBERT (4-layer). Performance metrics are provided for various tasks, such as BC5-chem, BC5-disease, NCBI-disease, BC2GM, and JNLPBA, using the Entity F1 score. For instance, INDUSBASE achieves an F1 score of 93.3 on BC5-chem, outperforming RoBERTaBASE (90.3) and SCIBERT (91.4). Similarly, INDUSSMALL scores 90.7 on BC5-chem, surpassing MINILM (86.1) and TINYBERT (84.6).\n\n8",
                        "score": 9,
                        "text": "cts with these annotated paragraphs. On an\ncomplex taxonomies used in climate-related litera-\n                                                   average, each query is 12 words long, and each\nture. This dataset comprises 534 abstracts sourced\n                                                 paragraph is 120 words long. We used Recall@10\nfrom Semantic Scholar Academic Graph (Kinney\n                                                   as evaluation metric since each question has only\net al., 2023), collected using a seed set of climate-\n                                             one relevant document.\nrelated keywords such as wildfire or floods. The\nabstracts were annotated using the IOB (inside, out-                                       6  Experimental Results\nside, beginning) tagging scheme and encompasses\na diverse array of entity types, as shown in Table 4.   Baselines We compared INDUS models against\n                                            open source models of similar sizes:\n5.2  NASA-QA                                                               \u2022 INDUSBASE was compared to RoBERTaBASE11\nWe present NASA-QA9, an extractive question an-      and SCIBERT12.\nswering task focused on the Earth science domain.     \u2022 INDUSSMALL was compared to MINILM  (6-\nFirst, 39 paragraphs from Earth science papers        layer)13 and TINYBERT (4-layer)14.\nwhich appeared in AGU and AMS journals (\u00a72) were\n                                                                 10https://huggingface.co/datasets/nasa-impact/nasa-smd-\nsourced. Subject matter experts from NASA for-    IR-benchmark\nmulated questions and marked the corresponding       11https://huggingface.co/FacebookAI/roberta-base\n                                                                   12https://huggingface.co/allenai/scibert_scivocab_uncased\n   8https://huggingface.co/datasets/ibm/Climate-Change-        13https://huggingface.co/nreimers/MiniLM-L6-H384-\nNER                                                     uncased\n   9https://huggingface.co/datasets/nasa-impact/nasa-smd-        14https://huggingface.co/huawei-\nqa-benchmark                                         noah/TinyBERT_General_4L_312D                                             Base model (125M params.)          Small model (\u223c30M params.)\n      Task         Metric       Dataset     RoBERTa   SCIBERT   INDUSBASE  TINYBERT  MINILM   INDUSSMALL\n                          BC5-chem     90.3 (0.2)   91.4 (0.2)    93.3 (0.2)     84.6 (0.2)   86.1 (0.3)     90.7 (0.1)\n                             BC5-disease    81.5 (0.3)   83.7 (0.3)    85.2 (0.3)     74.0 (0.4)   77.4 (0.3)     81.3 (0.3)\n    NER        Entity F1   NCBI-disease    87.6 (0.6)   87.6 (0.4)    88.3 (0.4)     81.2 (0.4)   83.1 (0.5)     85.6 (0.6)\n                     BC2GM      82.1 (0.3)   82.3 (0.2)    84.0 (0.3)     74.7 (0.4)   77.1 (0.2)     79.7 (0.3)\n                       JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     "
                    },
                    {
                        "context": "The excerpt discusses the development of INDUS models, which include a general-purpose encoder model and a retriever, along with their distilled versions. Key contributions include the creation of INDUSBPE, a tokenizer tailored for scientific texts using byte-pair encoding, and the pretraining of encoder-only large language models (LLMs) on curated scientific corpora. These models were further fine-tuned to produce sentence embeddings through contrastive learning. The knowledge-distilled models demonstrated improved latency while maintaining strong performance across various benchmarks, such as NASA-QA and CLIMATE-CHANGE NER. The focus was on using high-quality, in-domain corpora for model training.\n\n8",
                        "score": 8,
                        "text": "                                                                                        Small\n\n\n\n\n        BC5-CHEM       BC5-Disease      NCBI-Disease       BC2GM\n                                                                     NASA-QA              TREC-Covid       NFCorpus        NQ           HotpotQA\n\n         JNLPBA         EBM-PICO        ChemProt           DDI\n                                                                                       Climate                 FiQA           Arguana         Touche         DBPedia        NASA-IR\n         GAD            HoC         PubMedQA         BioASQ         Change NER\n                                                                                                             SciDocs         FEVER           Climate FEVER            SciFact\n         BIOSSES         BLURB Benchmark                                                          BEIR Benchmark\n\n                             Natural Language Understanding Benchmarks                                     Information Retrieval Benchmarks\n\n\nFigure 1: Overview of INDUS models: the general-purpose encoder model and the retriever built from it, and their\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\nNASA-QA, CLIMATE-CHANGE NER and NASA-IR.\n\n\nenabling them in informed decision-making.            (Beltagy et al., 2019). We also show that the\n   Specifically, we make the following contribu-      knowledge-distilled models achieved a signifi-\ntions:                                                 cant boost in latency while maintaining strong\n1. Utilizing the byte-pair encoding algorithm,       empirical performance compared to the original\n  we constructed INDUSBPE, a customized to-      models on most of the benchmark tasks.\n   kenizer from the curated scientific corpus.\n                                       2  Data2. We pretrained multiple encoder-only LLMs us-\n   ing the curated scientific corpora and the IN-                                                        Sufficient high-quality in-domain corpora is essen-\n  DUSBPE tokenizer (\u00a72, \u00a73). We further created                                                                  tial to develop models that perform better than\n   sentence-embedding models by fine-tuning the                                                           their counterparts trained on open-domain corpora.\n   encoder-only models with a contrastive learn-                                 We meticulously identified corpora for each of the\n   ing objective to learn \u201cuniversal\u201d sentence em-                                                 aforementioned domains, and created English-only\n   beddings (Gao et al., 2021) (\u00a74). We also                                            models for the sake of containment. Specifically,\n   trained smaller, more efficient versions of                                                        for each of the domains, we used open-source dat"
                    },
                    {
                        "context": "The excerpt provides a comparison of various models based on their performance on the BEIR benchmark. The models include RoBERTaBASE, BGEBASE, INDUS-RETRIEVERBASE, MINILM-V2, BGESMALL, and INDUS-RETRIEVERSMALL. Each model's performance is evaluated across multiple datasets such as TREC-Covid, NFCorpus, NQ, HotPotQA, and others. For instance, BGEBASE achieves an average score of 0.52, with notable performance on FEVER (0.86) and HotPotQA (0.73). In contrast, INDUS-RETRIEVERBASE scores 0.41 on average, with a peak on SciFact (0.74). These scores highlight the strengths and weaknesses of each model across different tasks.\n\n8",
                        "score": 0,
                        "text": "  s2p\n PMC [title - abstract] (\u00a72)                              2585537     Title-Body           s2p\n StackExchange Duplicate Questions [title-body - title-body]\u2020  250460      Duplicate Questions  p2p\n StackExchange Duplicate Questions [body - body]\u2020          250519      Duplicate Questions  p2p\n StackExchange Duplicate Questions [title - title]\u2020            304525      Duplicate Questions   s2s\n WikiAnswer Pairs (Fader et al., 2014)                     77427422    Duplicate Questions   s2s\n Specter Pairs (Cohan et al., 2020)                        684100       Citation Pairs         s2s\n S2ORC Citation Pairs [Titles] (Lo et al., 2020)              52603982    Citation Pairs         s2s\n SQuAD (Rajpurkar et al., 2016)                         87599       Question Answers    s2p\n NQ (Kwiatkowski et al., 2019)                          100231      Question Answers    s2p\n SearchQA (Dunn et al., 2017)                           582261      Question Answers    s2p\n StackExchange [title - answer]\u2020                          4067139     Question Answers    s2p\n StackExchange [title-body - answer]\u2020                     187195      Question Answers    p2p\n PAQ (Lewis et al., 2021)                               64371441    Question Answers    s2p\n FEVER (Thorne et al., 2018)\u2217                           109810      Fact Verification      s2p\n HotpotQA (Yang et al., 2018)\u2217                           85000       Question Answering  s2p\n\nTable 10: Training Data for Embedding Models. The training data totals to around 360M pairs. Data Format denotes\ns2p for sentence-to-paragraph mappings, s2s for sentence-to-sentence mappings, and p2p for paragraph-to-paragraph\nmappings.  \u2020Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\n\u2217Only used for Distillation.\n\n\n\n\n\n Model                                                           BEIR Eval\n                    TREC-  NFCorpus  NQ   HotPotQA  FiQA  ArguaAna  Touche  DBPedia  Scidocs  FEVER  Climate   SciFact  AVG.\n                        Covid                                                                FEVER         BEIR\n RoBERTaBASE             0.47     0.30        0.54   0.34        0.38    0.52        0.18     0.25       0.22      0.46      0.14      0.67     0.37\n BGEBASE                  0.78     0.37        0.54   0.73        0.41    0.64        0.26     0.41       0.22      0.86      0.31      0.74     0.52\n INDUS-RETRIEVERBASE    0.56     0.32        0.54   0.49        0.36    0.54        0.17     0.31       0.21      0.56      0.14      0.74     0.41\n MINILM-V2              0.47     0.32        0.44   0.47        0.35    0.50        0.17     0.32       0.22      0.52      0.25      0.65     0.39\n BGESMALL                 0.76     0.34        0.50   0.70        0.40    0.60        0.26     0.40       0.21      0.87      0.32      0.71     0.51\n INDUS-RETRIEVERSMALL   0.55     0.31        0.53   0.48        0.29    0.50        0.21     0.33       0.23      0.61      0.23      0.71"
                    },
                    {
                        "context": "The excerpt provides evaluation results for various models on the BLURB benchmark, which focuses on natural language understanding and reasoning tasks in the biomedical domain. The models compared include INDUS-RETRIEVERBASE, INDUS-RETRIEVERSMALL, BGEBASE15, RoBERTaBASE, MINILM-V216, BGESMALL17, SCIBERT, TINYBERT, and MINILM. Performance metrics such as Macro F1, Micro F1, and accuracy are reported, with INDUS models generally showing competitive results. For instance, INDUSBASE achieved an F1 score of 64.0 (SD 1.0), while INDUSSMALL scored 54.8 (SD 0.8). The results highlight the effectiveness of INDUS models in specific tasks, with bold and underlined results indicating top performance and significant differences. \n\n8",
                        "score": 8,
                        "text": "  JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     PICO      Macro F1  EBM PICO     72.3 (0.3)   72.4 (0.3)    73.1 (0.2)     67.4 (0.2)   70.3 (0.1)     73.1 (0.2)\n                            ChemProt    50.4 (28.2)   73.9 (0.7)    76.9 (0.5)     56.2 (3.2)   55.9 (2.1)     71.7 (0.9)\n     Relation\n                 Micro F1     DDI        78.6 (1.5)   80.1 (1.0)    81.7 (0.5)     39.3 (5.3)   51.5 (2.9)     69.0 (1.2)    Extraction\n                     GAD        80.0 (1.1)   81.6 (1.2)    79.4 (5.6)     76.4 (1.3)   77.3 (1.0)     81.3 (0.7)\n   Document\n   Classification   Micro F1     HoC        82.2 (0.7)   83.1 (0.6)    83.7 (0.5)     41.6 (6.8)   62.8 (4.7)     80.2 (0.6)\n     Question             PubMedQA     53.1 (3.3)   54.3 (3.8)    58.2 (6.7)     50.3 (1.4)   51.6 (1.7)     56.1 (1.4)\n                 Accuracy\n   Answering              BioASQ      69.1 (4.8)   74.6 (4.5)    69.6 (5.8)     74.3 (3.6)   66.7 (2.3)     75.4 (3.3)\n     Sentence\n     Similarity      Pearson    BIOSSES     79.8 (6.3)   86.3 (3.5)    72.2 (9.5)     88.2 (1.1)   26.6 (8.7)     70.4 (3.3)\n Micro Average         -                 -          75.9 (3.7)   79.2 (1.3)    78.9 (2.4)     67.6 (1.9)   66.1 (1.9)     76.2 (1.0)\n Macro Average         -                 -          74.9 (3.7)   78.2 (1.6)    76.4 (3.2)     65.6 (2.4)   60.6 (3.0)     74.3 (1.3)\n\nTable 5: Evaluation results on BLURB. Results reported are averaged on 10 random seeds with standard deviation in\nparenthesis. Micro average is reported across datasets while macro average is computed by first averaging scores on\neach task (say, task average), followed by averaging the task average across tasks. Results in bold indicate highest\nperformance while underlined results indicate significant difference from second highest result by more than two\nstandard deviations in each model size.\n\n\n  \u2022 INDUS-RETRIEVERBASE  was  compared  to             Model        F1 (SD)\n   BGEBASE15 and a RoBERTaBASE model finetuned             RoBERTa     60.8 (0.8)\n   with the same method presented in \u00a74.1.                  SCIBERT      61.8 (0.7)\n  \u2022 INDUS-RETRIEVERSMALL was compared  to              INDUSBASE    64.0 (1.0)\n   MINILM-V216 and BGESMALL17.                        TINYBERT    34.3 (1.6)\n                                                 MINILM      44.7 (1.3)\n6.1  Natural Language Understanding                     INDUSSMALL   54.8 (0.8)\n    Benchmarks\n                                                     Table 6: CLIMATE-CHANGE NER benchmark results.\nWe evaluated our models on BLURB (Gu et al.,                                                    Standard deviation over 10 random seeds shown in\n2021), a benchmark suite for natural language un-    parenthesis. Results in bold and underline indicate high-\nderstanding and reasoning tasks in the biomedical     est performance and significant difference from second\ndomain. We followed the original work to compute    highest r"
                    },
                    {
                        "context": "The excerpt discusses the performance of various language models, specifically focusing on the INDUS models designed for scientific applications. INDUSBASE outperformed all models of similar sizes, achieving an F1 score of 68.2 with a standard deviation of 2.9, surpassing models like RoBERTa (66.8) and SCIBERT (63.5). INDUSSMALL, while smaller, showed strong performance with an F1 score of 47.4. The research highlights the use of a custom tokenizer and in-domain data for training, as well as the creation of smaller model versions using knowledge distillation techniques. These models are evaluated on NASA-IR and BEIR benchmarks, with INDUS models showing notable retrieval performance.\n\n8",
                        "score": 8,
                        "text": "gmented the training     MINILM-V2                 0.62         0.39         0.24\nset with relevant SQuAD pairs for fine-tuning. All     BGESMALL                    0.66         0.51         0.42\nmodels are fine tuned for 15 epochs, and the results     INDUS-RETRIEVERSMALL      0.73         0.42         0.26\nare shown in Table 7. We observed that INDUSBASE                                                     Table 8:  Evaluation results on NASA-IR and BEIR.\noutperformed all models of similar sizes, while IN-   NASA-IR showed Recall@10 while BEIR reported the\nDUSSMALL had relatively strong performance com-   average nDCG@10 across all tasks. Retrieval time per\npared to its counterparts.                            query on the NQ task from BEIR, reported in seconds.\n\n          Model        F1 (SD)\n                                       7  Conclusions\n          RoBERTa     66.8 (3.1)\n          SCIBERT      63.5 (1.9)                In this research, we presented INDUS, a constella-\n           INDUSBASE    68.2 (2.9)                 tion of models for use in the science domain. We\n          TINYBERT    43.2 (2.3)              demonstrated the effectiveness of a custom tok-\n         MINILM      59.2 (3.9)                enizer and in-domain data for training high qual-\n           INDUSSMALL   47.4 (1.8)                   ity encoder models and sentence embedding mod-\n                                                              els.  Further, we created smaller versions of the\nTable 7: NASA-QA benchmark results. Standard devi-   proposed models suitable for applications with la-\nation over 3 random seeds shown in parenthesis. Re-                                                  tency or resource constraints through state-of-the-\nsults in bold and underline indicate highest performance\n                                                             art knowledge distillation techniques. For the ben-\nand significant difference from second highest result by\n                                                                  efit of the scientific community, we will releasemore than two standard deviations in each model size,\nrespectively.                                         the developed models and benchmark datasets on\n                                           Hugging Face.\n\n  We saw that INDUSBASE outperformed all models\nof similar sizes, while INDUSSMALL had relatively                                          References\nstrong performance.\n                                          Dogu Araci. 2019. Finbert: Financial sentiment analy-\n                                                                      sis with pre-trained language models.\n6.4  Information Retrieval Benchmarks\n                                                            Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nWe evaluated our models on the NASA-IR dataset                                                 ERT: A pretrained language model for scientific text.\nas well as BEIR Bench"
                    },
                    {
                        "context": "The excerpt describes three unique aspects of the models developed in the research: (1) An encoder model trained with domain-specific vocabulary and corpora to enhance natural language understanding tasks. (2) A contrastive-learning-based general text embedding model trained on a diverse set of datasets from multiple sources to improve information retrieval tasks. (3) Smaller versions of these models created using knowledge distillation techniques to address applications with latency or resource constraints. Additionally, the research introduces three new scientific benchmark datasets\u2014CLIMATE-CHANGE NER, NASA-QA, and NASA-IR\u2014to support interdisciplinary fields like physics, Earth science, and astrophysics.\n\n8",
                        "score": 8,
                        "text": ": (1) an encoder model[cs.CL]\n                  trained using domain-specific vocabulary and      BIOBERT (Lee et al., 2019), MATBERT (Walker\n                corpora to address natural language under-        et al., 2021), BATTERYBERT (Huang and Cole,\n                 standing tasks, (2) a contrastive-learning-based       2022) and SCHOLARBERT (Hong et al., 2023) were\n                 general text embedding model trained using       developed with the goal of improving accuracy on\n                a diverse set of datasets drawn from multi-       in-domain NLP tasks (Lee et al., 2019; Araci, 2019;\n                 ple sources to address information retrieval                                      Wu et al., 2023).\n                  tasks and (3) smaller versions of these mod-\n                   els created using knowledge distillation tech-                                                                    In this research, we specifically focused on inter-\n                niques to address applications which have la-\n                                                                     disciplinary fields related to the Earth, celestial bod-\n                 tency or resource constraints. We also created\n                                                                             ies, the Sun, and planets within our solar system                  three new scientific benchmark datasets namely,\n                                                          such as physics, Earth science, astrophysics, helio-             CLIMATE-CHANGE NER (entity-recognition),\n             NASA-QA (extractive QA) and NASA-IR (IR) to        physics, planetary sciences and biology. While the\n                  accelerate research in these multi-disciplinary        training corpora of existing domain-specific mod-arXiv:2405.10725v2               fields.    Finally, we show  that our mod-        els such as SCIBERT, BIOBERT and SCHOLARBERT\n                   els outperform both general-purpose encoders        partially cover some of these fields, there is cur-\n              (RoBERTa) and existing domain-specific en-                                                                      rently no specific model available that encompasses\n                coders (SCIBERT) on these new tasks as well\n                                                                                 all of the fields of interest collectively. Further, the\n                 as existing benchmark tasks in the domains of\n                                                                    interdisciplinary nature of these domains of inter-                    interest.\n                                                                         est is reflected in a vast body of literature scattered\n         1  Introduction                                  across diverse sources. Thus, we developed INDUS,\n                                                           a collection of encoder-based LLMs focused on\n     "
                    },
                    {
                        "context": "The excerpt discusses the performance of various language models, specifically INDUS, RoBERTa, SCIBERT, TINYBERT, and MINILM. INDUSBASE outperforms RoBERTa in micro/macro averages and is competitive with SCIBERT in the bio-domain. INDUSSMALL surpasses TINYBERT and MINILM in tasks like NER, PICO, relation extraction, and document classification, highlighting the effectiveness of knowledge distillation from INDUSBASE. SCIBERT performs better on paired input-text tasks due to its next sentence prediction objective. In the CLIMATE-CHANGE NER task, INDUS models outperform baselines, indicating the benefit of training on large domain-specific data.\n\n8",
                        "score": 8,
                        "text": "and significant difference from second\ndomain. We followed the original work to compute    highest result by more than two standard deviations in\nthe overall score (i.e., macro average).               each model size, respectively.\n  Table 5 shows the evaluation results. Among\nbase models,  INDUSBASE  significantly  outper-                                   We also noticed SCIBERT tends to perform better\nforms the general-purpose RoBERTa model on mi-                                                 than our model on paired input-text tasks, such\ncro/macro average while achieving competitive per-                                                   as QA and semantic similarity tasks, although\nformance to the bio-domain-specific counterpart,                                                    the results have relatively large standard devia-\nSCIBERT.                                                          tions. We hypothesized that the additional next\n  As for smaller models, we noticed INDUSSMALL                                                 sentence prediction objective during training in\noutperformed  the  baselines,  TINYBERT  and                                                  BERT-style models (such as SCIBERT) in contrast to\nMINILM, by a large margin in most cases, show-                                                    the RoBERTa-style models (such as RoBERTaBASE\ning significant difference from second best models                                              and INDUS) may be beneficial for paired input-text\nin NER, PICO, relation extraction, and document                                                         tasks. This trend was consistent with the observa-\nclassification tasks. This demonstrates the effec-                                                        tions of Tinn et al. (2023).\ntiveness of knowledge distillation from our domain-\nspecific teacher model, INDUSBASE.                  6.2  CLIMATE-CHANGE NER\n                                      As shown in Table 6, our models clearly outper-   15https://huggingface.co/BAAI/bge-base-en-v1.5\n   16sentence-transformers/all-MiniLM-L6-v2             formed the corresponding baseline models on the\n   17https://huggingface.co/BAAI/bge-small-en-v1.5        CLIMATE-CHANGE NER task, suggesting the effec-tiveness of training on large domain-specific data.     Model                 NASA-IR \u2191  BEIR Avg. \u2191   Retrieval\n                                                                                                      Time \u2193\n                                                             RoBERTaBASE                0.66         0.37         1.20\n6.3  NASA-QA                                            BGEBASE                     0.67         0.52         1.18\n                                                           INDUS-RETRIEVERBASE       0.71         0.41         1.19\nAs mentioned in \u00a75, we augmented the training     MINILM-V2                 0.62         0.39         0.24\nset with relevant "
                    },
                    {
                        "context": "The excerpt discusses the development of INDUS, a suite of large language models (LLMs) specifically tailored for scientific domains such as Earth science, biology, physics, heliophysics, planetary sciences, and astrophysics. These models are trained using curated scientific corpora, addressing the limitations of general-purpose LLMs that often perform sub-optimally on domain-specific tasks due to vocabulary distributional shifts. The excerpt highlights that domain-focused corpora enhance performance on specialized tasks, as evidenced by models like SCIBERT. However, specific unique aspects of each INDUS model are not detailed in the provided text.\n\n5",
                        "score": 5,
                        "text": "        INDUS: Effective and Efficient Language Models for Scientific Applications\n                  Bishwaranjan Bhattacharjee1, Aashka Trivedi1, Masayasu Muraoka1,\n        Muthukumaran Ramasubramanian3, Takuma Udagawa1, Iksha Gurung3, Rong Zhang1,\n         Bharath Dandala1, Rahul Ramachandran2, Manil Maskey2, Kaylin Bugbee2, Mike Little4,\n      Elizabeth Fancher2, Lauren Sanders5, Sylvain Costes5, Sergi Blanco-Cuaresma6, Kelly Lockhart6,\n         Thomas Allen6, Felix Grezes6, Megan Ansdell7, Alberto Accomazzi6, Yousef El-Kurdi1,\n   Davis Wertheimer1, Birgit Pfitzmann1, Cesar Berrospi Ramis1, Michele Dolfi1, Rafael Teixeira de Lima1,\n     Panagiotis Vagenas1, S. Karthik Mukkavilli1, Peter Staar1, Sanaz Vahidinia7, Ryan McGranaghan8,\n                             Armin Mehrabian9, Tsendgar Lee7\n      1IBM Research AI, 2 NASA MFSC, 3 UAH, 4 Navteca, 5 NASA Ames, 6 Harvard-Smithsonian CfA,\n                                               7 NASA HQ, 8 JPL, 9 NASA GSFC\n\n\n                         Abstract                      generation tasks. Most popular LLMs rely on the\n                                                             transformer architecture (Vaswani et  al., 2017)\n                Large language models (LLMs) trained on gen-      and are trained using general-purpose corpora2024              eral domain corpora showed remarkable re-                                                                      like Wikipedia or CommonCrawl (Devlin et al.,\n                   sults on natural language processing (NLP)\n                                                         2019; Liu et al., 2019; Lewis et al., 2020; Raffel\n                   tasks.  However, previous research demon-\n                                                                        et al., 2020; Brown et al., 2020; Touvron et al.,                   strated LLMs trained using domain-focused cor-May\n                pora perform better on specialized tasks. In-       2023). Although these general-purpose models ex-\n                 spired by this pivotal insight, we developed        hibited strong performance, the distributional shift20\n               INDUS, a comprehensive suite of LLMs tailored       of vocabulary led to sub-optimal performance on\n                  for the Earth science, biology, physics, helio-       domain-specific natural language understanding\n                 physics, planetary sciences and astrophysics                                                      and generation tasks (Beltagy et al., 2019). Fol-\n               domains and trained using curated scientific\n                                                         lowing this observation, several domain-specific\n                corpora drawn from diverse data sources. The\n                                               LLMs such as SCIBERT (Beltagy et  al., 2019),                   suite of models include: (1) an encoder model[cs.CL]\n                  trained using domain-specific vocabulary and      BI"
                    }
                ]
            },
            "devlin2019bertpretrainingof": {
                "doc_details": {
                    "embedding": null,
                    "docname": "devlin2019bertpretrainingof",
                    "citation": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.",
                    "dockey": "1bd917d538893069",
                    "overwrite_fields_from_metadata": true,
                    "key": "devlin2019bertpretrainingof",
                    "bibtex": "@article{devlin2019bertpretrainingof,\n    author = \"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina\",\n    booktitle = \"North American Chapter of the Association for Computational Linguistics\",\n    pages = \"4171-4186\",\n    title = \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n    year = \"2019\",\n    journal = \"Unknown journal\",\n    doi = \"10.18653/v1/n19-1423\",\n    url = \"https://doi.org/10.18653/v1/n19-1423\"\n}\n",
                    "authors": [
                        "Jacob Devlin",
                        "Ming-Wei Chang",
                        "Kenton Lee",
                        "Kristina Toutanova"
                    ],
                    "publication_date": null,
                    "year": 2019,
                    "volume": null,
                    "issue": null,
                    "issn": null,
                    "pages": "4171-4186",
                    "journal": null,
                    "publisher": null,
                    "url": null,
                    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                    "citation_count": 82386,
                    "bibtex_type": "article",
                    "source_quality": null,
                    "is_retracted": null,
                    "doi": "10.18653/v1/n19-1423",
                    "doi_url": "https://doi.org/10.18653/v1/n19-1423",
                    "doc_id": "1bd917d538893069",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
                        "externalIds": {
                            "MAG": "2951055169",
                            "ACL": "N19-1423",
                            "DBLP": "journals/corr/abs-1810-04805",
                            "ArXiv": "1810.04805",
                            "DOI": "10.18653/v1/N19-1423",
                            "CorpusId": 52967399
                        },
                        "venue": "North American Chapter of the Association for Computational Linguistics",
                        "citationCount": 82386,
                        "influentialCitationCount": 19150,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle",
                            "Conference"
                        ],
                        "publicationDate": null,
                        "citationStyles": {
                            "bibtex": "@Article{Devlin2019BERTPO,\n author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {4171-4186},\n title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n year = {2019}\n}\n"
                        },
                        "matchScore": 241.06143,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt compares BERT, ELMo, and OpenAI GPT, highlighting their differences in architecture and approach. BERT and OpenAI GPT are fine-tuning approaches, whereas ELMo is a feature-based approach. BERT's design decisions were made to closely align with OpenAI GPT for minimal comparison, but BERT's core argument emphasizes its bidirectionality. The excerpt notes that large datasets are less sensitive to hyperparameter choices, and fine-tuning is fast, allowing for exhaustive parameter searches. Additionally, BERT's task-specific models incorporate one additional output layer, requiring minimal new parameter learning. \n\n8",
                        "score": 8,
                        "text": "                                                       TPU-now-offers-preemptible-pricing-and-global-\nfor 1,000,000 steps, which is approximately 40     availability.html   \u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5        To isolate the effect of these differences, we per-\n   \u2022 Number of epochs: 2, 3, 4                  form ablation experiments in Section 5.1 which\n                                                 demonstrate that the majority of the improvements\n  We also observed that large data sets  (e.g.,    are in fact coming from the two pre-training tasks\n100k+ labeled training examples) were far less   and the bidirectionality they enable.\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-   A.5  Illustrations of Fine-tuning on Different\nsonable to simply run an exhaustive search over        Tasks\nthe above parameters and choose the model that   The illustration of \ufb01ne-tuning BERT on different\nperforms best on the development set.                tasks can be seen in Figure 4. Our task-speci\ufb01c\n                                             models are formed by incorporating BERT with\nA.4  Comparison of BERT, ELMo ,and                                             one additional output layer, so a minimal num-\n    OpenAI GPT                                                   ber of parameters need to be learned from scratch.\nHere we studies the differences in recent popular   Among the tasks, (a) and (b) are sequence-level\nrepresentation learning models including ELMo,    tasks while (c) and (d) are token-level tasks.  In\nOpenAI GPT and BERT. The comparisons be-    the \ufb01gure, E represents the input embedding, Ti\ntween the model architectures are shown visually    represents the contextual representation of token i,\nin Figure 3. Note that in addition to the architec-   [CLS] is the special symbol for classi\ufb01cation out-\nture differences, BERT and OpenAI GPT are \ufb01ne-    put, and [SEP] is the special symbol to separate\ntuning approaches, while ELMo is a feature-based    non-consecutive token sequences.\napproach.\n                              B  Detailed Experimental Setup  The  most  comparable  existing  pre-training\nmethod to BERT is OpenAI GPT, which trains a                                              B.1  Detailed Descriptions for the GLUE\nleft-to-right Transformer LM on a large text cor-                                           Benchmark Experiments.\npus. In fact, many of the design decisions in BERT\n                                          Our GLUE  results  in  Table1  are  obtained\nwere intentionally made to make  it as close to\n                                            from    https://gluebenchmark.com/\nGPT as possible so that the two methods could be\n                                    leaderboard    and   https://blog.\nminimally compared. The core argument of this\n                                     openai.com/language-unsupervised.\nwork is that the bi-directionality and the t"
                    }
                ]
            }
        }
    },
    {
        "question": "What are the datasets used in each of the papers to train the models?",
        "answer": "The paper \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" by Liu et al. utilized several datasets for pretraining the RoBERTa model. These datasets include BOOKCORPUS and English WIKIPEDIA, which were also used for training BERT, totaling 16GB. Additionally, the CC-NEWS dataset, collected from the English portion of the CommonCrawl News dataset, contains 63 million English news articles from September 2016 to February 2019, amounting to 76GB after filtering. The OPENWEBTEXT dataset was also used, along with a dataset collected from URLs shared on Reddit with at least three upvotes, totaling 38GB. Another dataset, STORIES, introduced by Trinh and Le (2018), contains 31GB of data filtered to match the story-like style of Winograd schemas (liu2019robertaarobustly pages 3-3).\n\nThe paper \"Indus: Effective and Efficient Language Models for Scientific Applications\" by Bhattacharjee et al. employed a diverse set of datasets for training. The training corpora included English Wikipedia, BooksCorpus, and StackExchange. Specific scientific data from NASA and its data providers were also used, along with the SAO/NASA Astrophysics Data System (ADS). The total dataset comprised 66.2 billion tokens across various domains. Additionally, three new scientific benchmark datasets were created: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR (bhattacharjee2024induseffectiveand pages 2-3; bhattacharjee2024induseffectiveand pages 5-6).",
        "references": "1. (liu2019robertaarobustly pages 3-3): Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, Jul 2019.\n\n2. (bhattacharjee2024induseffectiveand pages 2-3): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\n3. (bhattacharjee2024induseffectiveand pages 5-6): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
        "cost": 0.0478675,
        "token_counts": {
            "gpt-4o-2024-08-06": [
                12263,
                1721
            ]
        },
        "used_contexts": [
            "bhattacharjee2024induseffectiveand pages 5-6",
            "bhattacharjee2024induseffectiveand pages 2-3",
            "liu2019robertaarobustly pages 3-3"
        ],
        "documents": {
            "liu2019robertaarobustly": {
                "doc_details": {
                    "embedding": null,
                    "docname": "liu2019robertaarobustly",
                    "citation": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, Jul 2019.",
                    "dockey": "ed4e2d9b9f48ef51",
                    "overwrite_fields_from_metadata": true,
                    "key": "liu2019robertaarobustly",
                    "bibtex": "@article{liu2019robertaarobustly,\n    author = \"Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, M. and Zettlemoyer, Luke and Stoyanov, Veselin\",\n    title = \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n    year = \"2019\",\n    journal = \"ArXiv\",\n    volume = \"abs/1907.11692\",\n    month = \"Jul\"\n}\n",
                    "authors": [
                        "Yinhan Liu",
                        "Myle Ott",
                        "Naman Goyal",
                        "Jingfei Du",
                        "Mandar Joshi",
                        "Danqi Chen",
                        "Omer Levy",
                        "M. Lewis",
                        "Luke Zettlemoyer",
                        "Veselin Stoyanov"
                    ],
                    "publication_date": "2019-07-26T00:00:00",
                    "year": 2019,
                    "volume": "abs/1907.11692",
                    "issue": null,
                    "issn": null,
                    "pages": null,
                    "journal": "ArXiv",
                    "publisher": null,
                    "url": null,
                    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                    "citation_count": 21175,
                    "bibtex_type": "article",
                    "source_quality": -1,
                    "is_retracted": null,
                    "doi": null,
                    "doi_url": null,
                    "doc_id": "ed4e2d9b9f48ef51",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
                        "externalIds": {
                            "DBLP": "journals/corr/abs-1907-11692",
                            "MAG": "2965373594",
                            "ArXiv": "1907.11692",
                            "CorpusId": 198953378
                        },
                        "venue": "arXiv.org",
                        "citationCount": 21175,
                        "influentialCitationCount": 4898,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle"
                        ],
                        "publicationDate": "2019-07-26",
                        "citationStyles": {
                            "bibtex": "@Article{Liu2019RoBERTaAR,\n author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},\n volume = {abs/1907.11692},\n year = {2019}\n}\n"
                        },
                        "matchScore": 221.36655,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt discusses the datasets used for pretraining the RoBERTa model, which include five English-language corpora totaling over 160GB of uncompressed text. Specifically, it mentions:\n\n1. BOOKCORPUS and English WIKIPEDIA, which are the original datasets used to train BERT, totaling 16GB.\n2. CC-NEWS, collected from the English portion of the CommonCrawl News dataset, containing 63 million English news articles from September 2016 to February 2019, amounting to 76GB after filtering.\n3. OPENWEBTEXT, which is another dataset used in the study.\n\nThese datasets were used to gather as much data as possible for experimentation, aiming to match the quality and quantity of data for comparison.\n\n8",
                        "score": 8,
                        "text": "results on the development sets after \ufb01netuningthan  the  original BERT (Radford et al., 2019;\n                                                     the pretrained models on the corresponding single-Yang et al., 2019; Zellers et al., 2019).  Unfortu-\n                                                      task training data (i.e., without multi-task trainingnately, not all of the additional datasets can be\n                                                    or ensembling). Our \ufb01netuning procedure followspublicly released. For our study, we focus on gath-\n                                                     the original BERT paper (Devlin et al., 2019).ering as much data as possible for experimenta-\n                                                      In Section 5 we additionally report test set re-tion, allowing us to match the overall quality and\n                                                           sults obtained from the public leaderboard. Thesequantity of data as appropriate for each compari-\n                                                          results depend on a several task-speci\ufb01c modi\ufb01ca-son.\n                                                            tions, which we describe in Section 5.1.  We consider \ufb01ve English-language corpora of\nvarying sizes and domains, totaling over 160GB\n                                SQuAD  The  Stanford  Question  Answering\nof uncompressed text. We use the following text\n                                                   Dataset (SQuAD) provides a paragraph of context\ncorpora:\n                                             and a question. The task is to answer the question\n\u2022 BOOKCORPUS (Zhu et al., 2015) plus English   by extracting the relevant span from the context.\n  WIKIPEDIA.  This is the original data used to  We evaluate on two versions of SQuAD: V1.1\n   train BERT. (16GB).                          and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1\n                                                     the context always contains an answer, whereas in\n\u2022 CC-NEWS, which we collected from the En-\n  glish  portion  of  the CommonCrawl News       5The authors and their af\ufb01liated institutions are not in any\n  dataset (Nagel, 2016).  The data contains 63   way af\ufb01liated with the creation of the OpenWebText dataset.\n                                                         6The  datasets  are:  CoLA  (Warstadt et al.,  2018),  million English news articles crawled between\n                                                             Stanford   Sentiment   Treebank   (SST)   (Socher et al.,\n  September 2016 and February 2019. (76GB af-    2013),     Microsoft    Research    Paragraph    Corpus\n   ter \ufb01ltering).4                                (MRPC)  (Dolan and Brockett,  2005),   Semantic  Tex-\n                                                                      tual  Similarity Benchmark  (STS)  (Agirre et al.,  2007),\n\u2022 OPENWEBTEXT (Gokaslan and Cohen, 2019),    Quora Question  Pairs (QQP)  (Iyer et al.,"
                    },
                    {
                        "context": "The excerpt mentions two datasets used for training models. The first dataset is collected from URLs shared on Reddit with at least three upvotes, amounting to 38GB of data. The second dataset, STORIES, was introduced by Trinh and Le (2018) and contains a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas, totaling 31GB. Additionally, the excerpt discusses the importance of large quantities of text for BERT-style pretraining and references  for demonstrating improved end-task performance with increased data size.\n\n8",
                        "score": 8,
                        "text": "cted from URLs shared on\nwarmup steps, which are tuned separately for each      Reddit with at least three upvotes. (38GB).5\nsetting. We additionally found training to be very\n                                           \u2022 STORIES, a dataset introduced in Trinh and Le\nsensitive to the Adam epsilon term, and in some\n                                                   (2018) containing a subset of CommonCrawl\ncases we obtained better performance or improved\n                                                      data \ufb01ltered to match the story-like style of\nstability after tuning it. Similarly, we found setting\n                                              Winograd schemas. (31GB).\n\u03b22 = 0.98 to improve stability when training with\nlarge batch sizes.                                                    3.3  Evaluation\n  We pretrain with sequences of at most T = 512\n                                                Following previous work, we evaluate our pre-tokens. Unlike Devlin et al. (2019), we do not ran-\n                                                       trained models on downstream tasks using the fol-domly inject short sequences, and we do not train\n                                               lowing three benchmarks.with a reduced sequence length for the \ufb01rst 90% of\nupdates. We train only with full-length sequences.\n                              GLUE  The  General  Language  Understand-\n  We train with mixed precision \ufb02oating point\n                                                   ing Evaluation (GLUE) benchmark (Wang et al.,\narithmetic on DGX-1 machines, each with 8 \u00d7\n                                              2019b) is a collection of 9 datasets for evaluating\n32GB Nvidia V100 GPUs interconnected by In-\n                                                       natural language understanding systems.6  Tasks\n\ufb01niband (Micikevicius et al., 2018).\n                                                     are framed as either single-sentence classi\ufb01cation\n3.2  Data                                          or sentence-pair classi\ufb01cation tasks. The GLUE\n                                                     organizers provide training and development data\nBERT-style pretraining crucially relies on large\n                                                              splits as well as a submission server and leader-\nquantities of text.  Baevski et al. (2019) demon-\n                                                board that allows participants to evaluate and com-\nstrate that increasing data size can result in im-\n                                                  pare their systems on private held-out test data.\nproved end-task performance.   Several  efforts\n                                                 For the replication study in Section 4, we reporthave trained on datasets larger and more diverse\n                                                          results on the development sets after \ufb01netuningthan  the  original BERT (Radford et al., 2019;\n     "
                    }
                ]
            },
            "bhattacharjee2024induseffectiveand": {
                "doc_details": {
                    "embedding": null,
                    "docname": "bhattacharjee2024induseffectiveand",
                    "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
                    "dockey": "2b647dabce87f56c",
                    "overwrite_fields_from_metadata": true,
                    "key": "bhattacharjee2024induseffectiveand",
                    "bibtex": "@article{bhattacharjee2024induseffectiveand,\n    author = \"Bhattacharjee, Bishwaranjan and Trivedi, Aashka and Muraoka, Masayasu and Ramasubramanian, Muthukumaran and Udagawa, Takuma and Gurung, I. and Zhang, Rong and Dandala, Bharath and Ramachandran, Rahul and Maskey, M. and Bugbee, Kayleen and Little, Mike and Fancher, Elizabeth and Sanders, Lauren and Costes, Sylvain and Blanco-Cuaresma, Sergi and Lockhart, Kelly and Allen, Thomas and Grazes, Felix and Ansdel, Megan and Accomazzi, A. and El-Kurdi, Yousef and Wertheimer, Davis and Pfitzmann, Birgit and Ramis, Cesar Berrospi and Dolfi, Michele and de Lima, Rafael Teixeira and Vegenas, Panos and Mukkavilli, S. K. and Staar, P. and Vahidinia, S. and McGranaghan, Ryan and Mehrabian, A. and Lee, Tsendgar\",\n    title = \"INDUS: Effective and Efficient Language Models for Scientific Applications\",\n    year = \"2024\",\n    journal = \"ArXiv\",\n    volume = \"abs/2405.10725\",\n    month = \"May\",\n    doi = \"10.48550/arxiv.2405.10725\",\n    url = \"https://doi.org/10.48550/arxiv.2405.10725\"\n}\n",
                    "authors": [
                        "Bishwaranjan Bhattacharjee",
                        "Aashka Trivedi",
                        "Masayasu Muraoka",
                        "Muthukumaran Ramasubramanian",
                        "Takuma Udagawa",
                        "I. Gurung",
                        "Rong Zhang",
                        "Bharath Dandala",
                        "Rahul Ramachandran",
                        "M. Maskey",
                        "Kayleen Bugbee",
                        "Mike Little",
                        "Elizabeth Fancher",
                        "Lauren Sanders",
                        "Sylvain Costes",
                        "Sergi Blanco-Cuaresma",
                        "Kelly Lockhart",
                        "Thomas Allen",
                        "Felix Grazes",
                        "Megan Ansdel",
                        "A. Accomazzi",
                        "Yousef El-Kurdi",
                        "Davis Wertheimer",
                        "Birgit Pfitzmann",
                        "Cesar Berrospi Ramis",
                        "Michele Dolfi",
                        "Rafael Teixeira de Lima",
                        "Panos Vegenas",
                        "S. K. Mukkavilli",
                        "P. Staar",
                        "S. Vahidinia",
                        "Ryan McGranaghan",
                        "A. Mehrabian",
                        "Tsendgar Lee"
                    ],
                    "publication_date": "2024-05-17T00:00:00",
                    "year": 2024,
                    "volume": "abs/2405.10725",
                    "issue": null,
                    "issn": null,
                    "pages": null,
                    "journal": "ArXiv",
                    "publisher": null,
                    "url": null,
                    "title": "INDUS: Effective and Efficient Language Models for Scientific Applications",
                    "citation_count": 4,
                    "bibtex_type": "article",
                    "source_quality": -1,
                    "is_retracted": null,
                    "doi": "10.48550/arxiv.2405.10725",
                    "doi_url": "https://doi.org/10.48550/arxiv.2405.10725",
                    "doc_id": "2b647dabce87f56c",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "d50eb618b75ce6f837fd65245393f1b28ab1d911",
                        "externalIds": {
                            "DBLP": "journals/corr/abs-2405-10725",
                            "ArXiv": "2405.10725",
                            "DOI": "10.48550/arXiv.2405.10725",
                            "CorpusId": 269899887
                        },
                        "venue": "arXiv.org",
                        "citationCount": 4,
                        "influentialCitationCount": 0,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle"
                        ],
                        "publicationDate": "2024-05-17",
                        "citationStyles": {
                            "bibtex": "@Article{Bhattacharjee2024INDUSEA,\n author = {Bishwaranjan Bhattacharjee and Aashka Trivedi and Masayasu Muraoka and Muthukumaran Ramasubramanian and Takuma Udagawa and I. Gurung and Rong Zhang and Bharath Dandala and Rahul Ramachandran and M. Maskey and Kayleen Bugbee and Mike Little and Elizabeth Fancher and Lauren Sanders and Sylvain Costes and Sergi Blanco-Cuaresma and Kelly Lockhart and Thomas Allen and Felix Grazes and Megan Ansdel and A. Accomazzi and Yousef El-Kurdi and Davis Wertheimer and Birgit Pfitzmann and Cesar Berrospi Ramis and Michele Dolfi and Rafael Teixeira de Lima and Panos Vegenas and S. K. Mukkavilli and P. Staar and S. Vahidinia and Ryan McGranaghan and A. Mehrabian and Tsendgar Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {INDUS: Effective and Efficient Language Models for Scientific Applications},\n volume = {abs/2405.10725},\n year = {2024}\n}\n"
                        },
                        "matchScore": 201.18681,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt describes the datasets used to train the Indus language models for scientific applications. The models were trained using open-source data with permissive licenses, augmented with specific data from NASA and its data providers. The training corpora included English Wikipedia to aid in learning general English. Three new scientific benchmark datasets were created: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR. The SAO/NASA Astrophysics Data System (ADS) was a significant data source, covering publications in astronomy, astrophysics, physics, and general science, including all arXiv e-prints. The total dataset comprised 66.2 billion tokens across various domains.\n\n8",
                        "score": 8,
                        "text": "                                                    for each of the domains, we used open-source data\n   these models using knowledge-distillation tech-                                           which has a permissive license, and further aug-\n   niques (\u00a73.3, \u00a74.2).                                            mented them with specific data from NASA and its\n3. We created three new scientific benchmark                                                   data providers. To aid in the learning of general\n   datasets, CLIMATE-CHANGE NER (an entity                                                   English, we also included English Wikipedia in\n   recognition task), NASA-QA (an extractive ques-                                                 our training corpora. We briefly describe each data\n   tion answering task) and NASA-IR (a retrieval                                                 source below, and present statistics of the data in\n   task) (\u00a75) to further accelerate research in this                                                 Table 1.\n   multi-disciplinary field.                                                               \u2022 SAO/NASA Astrophysics Data System (ADS)1:\n4. Through experimental results, we show strong\n                                         ADS is the biggest source of data, covering pub-\n   performance by our models on these benchmark\n                                                            lications in the areas of astronomy and astro-\n   tasks as well as on existing domain-specific\n                                                        physics, physics and general science including\n   benchmarks, outperforming general-purpose\n                                                                     all arXiv e-prints.\n   models like RoBERTa (Liu et  al., 2019) as\n   well as scientific-domain encoders like SCIBERT        1https://ui.adsabs.harvard.edu        Dataset         Domain    #Tokens Ratio         Tokenizer      ADS     PMC    Wikipedia\n NASA CMR           Earth Science     0.3B  1%       RoBERTa       12,867,439  7,549,075     15,859\n AMS and AGU papers  Earth Science     2.8B  4%         +lower_cased  12,862,227  7,557,868     16,901\n English Wikipedia       General       5.0B  8%      INDUSBPE     12,309,023  6,920,659     16,056\n PubMed Abstracts     Biomedical      6.9B 10%\n                                                     Table 2: Number of tokens produced by RoBERTa and PMC                 Biomedical     18.5B 28%\n                                         INDUSBPE tokenizers applied to 1k samples from each SAO/NASA ADS        Astronomy,     32.7B 49%\n                                                             dataset. Fewer tokens lead to a smaller computation                        Astrophysics,\n                                                                cost.                           Physics,\n                     General Science\n Total                              66.2B 100%                             "
                    },
                    {
                        "context": "The excerpt describes the training data and benchmark datasets used in the study. The pretraining step utilized data from English Wikipedia, BooksCorpus, and StackExchange. Additionally, three new datasets were introduced for benchmarking: a Named Entity Recognition (NER) task, a Question Answering (QA) task, and an Information Retrieval (IR) task. The QA task involved 29 paragraphs with 145 QA pairs for training and 10 paragraphs with 50 questions for evaluation. The training set was augmented with Earth science-related paragraphs and QA pairs from the SQuAD dataset, resulting in 686 paragraphs with 5,081 questions.\n\n8",
                        "score": 8,
                        "text": "on 2 A100 GPUs for\n    X X     =  LKD      \u2212         pt(xi, xj)logps(xi, xj)   (4)   300K steps with an effective batch size of 2,048,\n             i=1 j=1                           and learning rate of 7e \u22124. Through experimenta-\n                                                            tion, We found that \u03c4KD = 4 performed the best.\n                          ess(xi,xj)/\u03c4KD\n        ps(xi, xj) =                            (5)          Pm                    k=1 ess(xi,xk)/\u03c4KD         5  Creating Benchmarks\n                          est(xi,xj)/\u03c4KD            Benchmark datasets play a crucial role in assess-\n        pt(xi, xj) =                            (6)          Pm                    k=1 est(xi,xk)/\u03c4KD            ing the language understanding capabilities models.\n                                             However, to the best our knowledge, there is a no-\n  Here, ss(xi, xj) and st(xi, xj) represent the sim-\n                                                         ticeable absence of datasets tailored for the diverse\nilarity scores between two pairs {xi, xj}, defined in                                            and multidisciplinary field under study. Thus, to\nEquation 3 for the student and teacher respectively.\n                                                       effectively benchmark the proposed NLP models\nTraining Data  We first conducted a embedding-   and further accelerate research in this multidisci-\noriented pretraining step, as presented in Retro-   plinary domain, We introduced three new datasets,\nMAE (Xiao et al., 2022), on English Wikipedia,   an NER task, a QA task, and an IR task, described\nBooksCorpus, and StackExchange data, totalling    below.                      Train    Validation        Test          spans of the paragraph which answer the questions.\n   Num. Abstracts    382       77           75\n   Num. Tokens     32,031     6,443         5,850      We used 29 paragraphs (with 145 QA pairs in total)\n    Entity Labels                                        as the training set and the remaining 10 paragraphs\n    climate-nature, climate-greenhouse-gases, climate-assets,\n    climate-problem-origins, climate-mitigations,               (with 50 questions in total) as the evaluation set.\n    climate-properties, climate-impacts, climate-datasets,       The training set was further augmented with para-\n    climate-organizations, climate-observations,\n    climate-models, climate-hazards, climate-organisms          graphs and QA pairs related to Earth science from\n                                                      the SQuAD dataset (Rajpurkar et al., 2018). Specif-\nTable 4: CLIMATE-CHANGE NER statistics and entity    ically, those related to oxygen, Amazon rain forest\nlabels                                            and geology were used. This resulted in a pruned\n                                      SQuAD set comprising 686 paragraphs with 5,081\n                                                   questions (2,817 answer"
                    },
                    {
                        "context": "The excerpt provides details on datasets used for training embedding models, specifically in the context of sentence embedding training data. It mentions the use of approximately 360 million text-pair mappings, categorized as sentence-to-paragraph (s2p), sentence-to-sentence (s2s), and paragraph-to-paragraph (p2p). Specific datasets include StackOverflow (18,562,443 pairs, s2p), StackExchange Math (2,201,906 pairs, s2p), S2ORC title-abstract (41,769,185 pairs, s2p), S2ORC citation pairs (52,603,982 pairs, p2p), and others like Wikipedia, Arxiv, NASA ADS, PubMed, and PMC, all primarily in the s2p format.\n\n8",
                        "score": 8,
                        "text": ".   2022. RetroMAE: Pre-training retrieval-oriented lan-\n  guage models via masked auto-encoder. In Proceed-\n   ings of the 2022 Conference on Empirical Methods in\n  Natural Language Processing, pages 538\u2013548, Abu\n   Dhabi, United Arab Emirates. Association for Com-\n   putational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\n  Muennighoff. 2023.  C-pack: Packaged resources\n   to advance general chinese embedding.\n\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\n  2023. DistillCSE: Distilled contrastive learning for\n   sentence embeddings. In Findings of the Associa-\n   tion for Computational Linguistics: EMNLP 2023,\n  pages 8153\u20138165, Singapore. Association for Com-\n   putational Linguistics.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\n  William Cohen, Ruslan Salakhutdinov, and Christo-\n  pher D. Manning. 2018. HotpotQA: A dataset for\n   diverse, explainable multi-hop question answering.\n   In Proceedings of the 2018 Conference on Empiri-\n   cal Methods in Natural Language Processing, pages\n  2369\u20132380, Brussels, Belgium. Association for Com-\n   putational Linguistics.\n\nA  Sentence Embedding Training Data\n\nTable 10 shows the various data sources used for\ntraining embedding models. All data is presented\nin the form of text-pairs, where each item in the\npair may be a sentence or a paragraph. In the table,\nData Format denotes s2p for sentence-to-paragraph\nmappings, s2s for sentence-to-sentence mappings,\nand p2p for paragraph-to-paragraph mappings. We\nused about 360 million pairs for training and used\nin-batch negatives.\n\nB  Ablation Study: Stage-wise Distillation\n    for Embedding Model\n\nFor the distilled embedding models, we find that\nstage-wise distillation does not benefit performance\nas much as a one-step process, combining all the\nsupervised and unsupervised data. As shown in\nTable 9, the stage-wise approach underperformed\nthe one-stage approach by 1 percentage point for\nboth NASA-QA and on BEIR. Dataset                                        Num. Pairs  Data Category        Data Format\n StackOverflow\u2020                                       18562443    Title-Body           s2p\n StackExchange Math\u2020                                 2201906     Title-Body           s2p\n S2ORC [title - abstract] (Lo et al., 2020)                   41769185    Title-Body           s2p\n S2ORC Citation Pairs [Abstracts] (Lo et al., 2020)           52603982    Title-Body          p2p\n StackExchange [title - body]\u2020                            5415570     Title-Body           s2p\n Wikipedia (Fader et al., 2014)                           6458670     Title-Body           s2p\n Arxiv (Clement et al., 2019)                             2358545     Title-Body           s2p\n NASA ADS [title - abstract] (\u00a72)                         2633240     Title-Body           s2p\n PubMed [title - abstract] (\u00a72)                            24001387    Title-Body           s2p\n PMC [title - abstract] (\u00a72)                              2585537     Title-Body           s2p"
                    },
                    {
                        "context": "The excerpt mentions several datasets and benchmarks used in different studies. The BEIR benchmark is highlighted as a heterogeneous benchmark for zero-shot evaluation of information retrieval models, as noted in the work by Nandan . The FEVER dataset is referenced as a large-scale dataset for fact extraction, according to James . Additionally, the NASA-QA dataset is mentioned in the context of an ablation study evaluating recall metrics. These datasets are used to train and evaluate models in the respective studies mentioned in the excerpt.\n\n8",
                        "score": 8,
                        "text": "sa, Jordi Huguet, Paula Marne,     Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexan-\n  Tania Mench\u00f3n, Gr\u00e9gory Operto, Albina Polo, San-      der Dunn, Kristin Persson, Gerbrand Ceder, and\n   dra Pradas, Anna Soteras, Marc Vilanova, and Na-     Anubhav Jain. 2021. The impact of domain-specific\n   talia Vilor-Tejedor. 2020.  Novel tau biomarkers       pre-training on named entity recognition tasks in ma-\n  phosphorylated at t181, t217 or t231 rise in the ini-        terials science. Available at SSRN 3950755.\n   tial stages of the preclinical alzheimer&#x2019;s\n  <i>continuum</i> when only subtle changes in    Liang Wang, Nan Yang, Xiaolong Huang, Binxing\n  a&#x3b2; pathology are detected. EMBO Molec-       Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\n   ular Medicine, 12(12):e12921.                       and Furu Wei. 2022. Text embeddings by weakly-\n                                                          supervised contrastive pre-training.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\n  hishek Srivastava, and Iryna Gurevych. 2021. Beir:   Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\n A heterogenous benchmark for zero-shot evaluation      and Furu Wei. 2021. MiniLMv2: Multi-head self-\n   of information retrieval models.                           attention relation distillation for compressing pre-\n                                                              trained transformers. In Findings of the Association\nJames   Thorne,   Andreas   Vlachos,    Christos       for Computational Linguistics: ACL-IJCNLP 2021,\n   Christodoulopoulos,  and  Arpit  Mittal.  2018.      pages 2140\u20132151, Online. Association for Computa-\n  FEVER: a large-scale dataset for fact extraction       tional Linguistics.Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and     Model                    Training    NASA-IR  BEIR Avg.\n  Tie-Yan Liu. 2013. A theoretical analysis of ndcg     INDUS-RETRIEVERSMALL  One-Stage     0.73        0.42\n   type ranking measures. In Proceedings of the 26th     INDUS-RETRIEVERSMALL   Stagewise     0.72        0.41\n  Annual Conference on Learning Theory, volume 30\n                                                     Table 9: Ablation Study: Evaluation results on NASA-   of Proceedings of Machine Learning Research, pages\n  25\u201354, Princeton, NJ, USA. PMLR.              QA and BEIR. NASA-QA showed Recall10 while BEIR\n                                                         reported nDCG10.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\n  Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\n   badur, David Rosenberg, and Gideon Mann. 2023.  C  Complete Results on BEIR Benchmark\n  Bloomberggpt: A large language model for finance.\n                                                  Table 11 shows the per-dataset results on the BEIR\nShitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.\n                                                          tasks.   2022. RetroMAE: Pre-training retrieval-oriented lan-\n  guage models via masked auto-encoder. In "
                    },
                    {
                        "context": "The excerpt discusses the use of the BEIR Benchmark, which includes 12 retrieval tasks across various domains, and employs the Normalized Cumulative Discount Gain (nDCG@10) as the primary metric. The performance of domain-specific sentence embedding models is evaluated against baselines, particularly highlighting their success on the NASA-IR task and several BEIR tasks. Additionally, the INDUS-RETRIEVERSMALL model is noted for its superior performance and speed compared to INDUS-RETRIEVERBASE. The excerpt does not provide specific datasets used for training but focuses on evaluation metrics and model performance.\n\n5",
                        "score": 5,
                        "text": "                         ERT: A pretrained language model for scientific text.\nas well as BEIR Benchmark (Thakur et al., 2021),      In Proceedings of the 2019 Conference on Empirical\nwhich consists of 12 retrieval tasks spanning a vari-     Methods in Natural Language Processing and the\n                                                         9th International Joint Conference on Natural Lan-ety of domains. The BEIR benchmark used the Nor-\n                                                  guage Processing (EMNLP-IJCNLP), pages 3615\u2013\nmalized Cumulative Discount Gain (nDCG@10)                                                       3620, Hong Kong, China. Association for Computa-\n(Wang et al., 2013) as their main metric. Table 8       tional Linguistics.\nshows the performance of our domain-specific sen-\n                                      Tom Brown, Benjamin Mann, Nick Ryder, Melanie\ntence embedding models, along with our baselines.                                                         Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nAs shown, both of our sentence embedding mod-      Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nels significantly outperformed the baselines on the       Askell,  Sandhini Agarwal,  Ariel  Herbert-Voss,\nNASA-IR task while still maintaining good perfor-      Gretchen Krueger, Tom Henighan, Rewon Child,\n                                                        Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nmance on several of the BEIR tasks. (We presented\n                                                          Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nresults for each BEIR task in Appendix C).              teusz Litwin, Scott Gray, Benjamin Chess, Jack\n  We also measured the average time per query       Clark, Christopher Berner, Sam McCandlish, Alec\nfor retrieval on the 4,202 test queries of the natural      Radford, Ilya Sutskever, and Dario Amodei. 2020.\n                                                 Language models are few-shot learners.   In Ad-\nquestions set of BEIR, on a single A100 GPU. This                                                      vances in Neural Information Processing Systems,\ntime includes the time to encode the query, cor-     volume 33, pages 1877\u20131901. Curran Associates,\npus, and time to retrieve relevant documents. No-       Inc.\ntably, INDUS-RETRIEVERSMALL outperformed IN-                                                    Colin B. Clement, Matthew Bierbaum,  Kevin  P.\nDUS-RETRIEVERBASE, on both NASA-IR and BEIR,      O\u2019Keeffe, and Alexander A. Alemi. 2019. On the\nwhile being about 4.6x faster.                           use of arxiv as a dataset.Arman Cohan, Sergey Feldman, Iz Beltagy, Doug       Sarna,  Yonglong  Tian,   Phillip  Isola,  Aaron\n  Downey, and Daniel S. Weld. 2020.  SPECTER:      Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\n  Document-level  Representation  Learning  using       pervised contrastive learning. In Advances in Neural\n   Citation-informed "
                    },
                    {
                        "context": "The excerpt details the datasets used for pretraining the INDUS language model. The datasets include:\n\n1. **PubMed Central (PMC)**: Utilized the portion with a commercial-friendly license and PubMed abstracts.\n2. **American Meteorological Society (AMS)**: Full-text journal documents covering Earth systems, meteorology, climatology, oceanography, atmospheric sciences, climate, hydrometeorology, weather forecasting, and societal impacts.\n3. **American Geophysical Union (AGU)**: Journal documents on atmospheres, biogeosciences, Earth surface, machine learning, computation, oceans, planets, solid earth, and space physics.\n4. **NASA Common Metadata Repository (CMR)**: Text descriptions from NASA\u2019s Earth Science Data and Information System.\n\n8",
                        "score": 8,
                        "text": "         General Science\n Total                              66.2B 100%                                                    ing dataset (\u00a72)6. For a fair comparison, we set the\n  Table 1: Basic statistics of our pretraining dataset.      vocabulary size to 50, 265, which is equal to that of\n                                                     the RoBERTa tokenizer (Liu et al., 2019) and used\n                                                     the uncased variation of both the tokenizers.\n  \u2022 PubMed Central (PMC)2 : PMC is a full-text    We performed a brief analysis to understand the\n   archive of biomedical and life science journal    differences between the vocabularies of INDUS-\n    literature maintained by National Library of  BPE and the RoBERTa tokenizer. Out of 50, 265\n   Medicine and National Institutes of Health. We    tokens, 22, 355(44.5%) tokens are common in both\n   used the portion of PMC that has a commercial-   the tokenizers while the remaining 27, 910(55.5%)\n   friendly license, along with the PubMed ab-   tokens are included only in either tokenizer, indi-\n    stracts of all the articles in PMC.                  cating a significant distributional shift in domain.\n  \u2022 American Meteorological Society (AMS)3:   To further understand the effect, we applied both\n  We used full-text journal documents spanning   RoBERTa and INDUSBPE on 1, 000 randomly sam-\n   topics in Earth systems, Earth interactions, ap-   pled text fragments from our datasets. These text\n   plied meteorology and climatology, physical   fragments varied from full documents to abstracts\n   oceanography, atmospheric sciences, climate,    to single sentences. As shown in Table 2, INDUS-\n   hydrometeorology, weather and forecasting,  BPE tokenizer produced fewer tokens than the\n   and societal impacts.                      RoBERTa tokenizer, leading to \u02dc8% drop in com-\n  \u2022 American Geophysical Union (AGU)4: The    putation cost during training.\n  AGU dataset included journal documents across      Table 3 compares the RoBERTa tokenzier and IN-\n   the topics of atmospheres,  biogeosciences,  DUSBPE tokenizer, illustrating that the proposed\n   Earth surface, machine learning and compu-   tokenizer treated scientific terms (such as biomak-\n    tation, oceans, planets, solid earth, and space    ers, phosphorylated, alzheimer) as single tokens\n   physics.                                     while RoBERTa tokenizer splits these words into\n  \u2022 NASA  Common  Metadata  Repository    multiple subword pieces.\n   (CMR)5: CMR is a high-performance, high-\n                                                    3.2  Encoder Model    quality, continuously evolving metadata system\n    that catalogs all data and service metadata  We first trained an encoder-only model, INDUSBASE,\n   records for NASA\u2019s Earth Science Data and    using a masked language modeling objective. The\n   Information System (ESDIS).  It contains text   model architecture follows RoBERTaBASE (Liu et al.,\n   descriptions"
                    },
                    {
                        "context": "The excerpt discusses the evaluation of the INDUS models on specific datasets, namely the NASA-IR and BEIR benchmarks. It mentions that the training set was augmented with relevant SQuAD pairs for fine-tuning. The performance of various models, including INDUSBASE and INDUSSMALL, was compared using these datasets. The results are presented in terms of metrics like Recall@10 for NASA-IR and average nDCG@10 for BEIR. Additionally, the NASA-QA benchmark results are provided, showing F1 scores for different models, with INDUSBASE achieving the highest score of 68.2. The excerpt does not specify other datasets used for initial training.\n\n7",
                        "score": 7,
                        "text": "gmented the training     MINILM-V2                 0.62         0.39         0.24\nset with relevant SQuAD pairs for fine-tuning. All     BGESMALL                    0.66         0.51         0.42\nmodels are fine tuned for 15 epochs, and the results     INDUS-RETRIEVERSMALL      0.73         0.42         0.26\nare shown in Table 7. We observed that INDUSBASE                                                     Table 8:  Evaluation results on NASA-IR and BEIR.\noutperformed all models of similar sizes, while IN-   NASA-IR showed Recall@10 while BEIR reported the\nDUSSMALL had relatively strong performance com-   average nDCG@10 across all tasks. Retrieval time per\npared to its counterparts.                            query on the NQ task from BEIR, reported in seconds.\n\n          Model        F1 (SD)\n                                       7  Conclusions\n          RoBERTa     66.8 (3.1)\n          SCIBERT      63.5 (1.9)                In this research, we presented INDUS, a constella-\n           INDUSBASE    68.2 (2.9)                 tion of models for use in the science domain. We\n          TINYBERT    43.2 (2.3)              demonstrated the effectiveness of a custom tok-\n         MINILM      59.2 (3.9)                enizer and in-domain data for training high qual-\n           INDUSSMALL   47.4 (1.8)                   ity encoder models and sentence embedding mod-\n                                                              els.  Further, we created smaller versions of the\nTable 7: NASA-QA benchmark results. Standard devi-   proposed models suitable for applications with la-\nation over 3 random seeds shown in parenthesis. Re-                                                  tency or resource constraints through state-of-the-\nsults in bold and underline indicate highest performance\n                                                             art knowledge distillation techniques. For the ben-\nand significant difference from second highest result by\n                                                                  efit of the scientific community, we will releasemore than two standard deviations in each model size,\nrespectively.                                         the developed models and benchmark datasets on\n                                           Hugging Face.\n\n  We saw that INDUSBASE outperformed all models\nof similar sizes, while INDUSSMALL had relatively                                          References\nstrong performance.\n                                          Dogu Araci. 2019. Finbert: Financial sentiment analy-\n                                                                      sis with pre-trained language models.\n6.4  Information Retrieval Benchmarks\n                                                            Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nWe evaluated our models on the NASA-IR dataset                                                 ERT: A pretrained language model for scientific text.\nas well as BEIR Bench"
                    },
                    {
                        "context": "The excerpt from  discusses the use of specific datasets for training their language models. They utilized \"ADS data and a sample of the S2ORC data\" to enhance domain-specific signals. The training process involved large batch sizes and in-batch negatives to better approximate the contrastive learning objective. The model, referred to as INDUS-RETRIEVERBASE, was fine-tuned using a bi-encoder framework and employed mean pooling of contextualized transformer representations. The training was conducted on 2 A100 GPUs with an effective batch size of 1,024, initially using unsupervised data for 300K steps, followed by an additional 100K steps with supervised data.\n\n8",
                        "score": 8,
                        "text": "Kwiatkowski et al., 2019), SQuAD (Rajpurkar\nnon-relevant passage.\n                                                              et  al., 2016), SPECTER pairs (Cohan et  al.,\n  Inspired by recent work (Li et al., 2023), we                                                    2020), etc. We included the aforementioned\nused an improved contrastive loss by introducing                                        ADS data and a sample of the S2ORC data in\nan additional bidirectional signal. Specifically, for                                                               this step, to boost domain-specific signals.\na triple {q, p+, P \u2212} of a query, a relevant (posi-   Appendix A contains comprehensive details about\ntive) passage, and a set of non-relevant (negative)\n                                                    the datasets used in training.  For both training\npassages P \u2212= {p\u2212 j }m j=1, We define the InfoNCE    stages, we used large batch sizes and in-batch nega-\nloss(van den Oord et al., 2019) as:\n                                                           tives to better approximate the contrastive learning\n                                                       objective. During training, we sampled batches\n                     n                                                            i )      X                      from each data source proportionately to its size,        =         loges(qi,p+          (1)        LIC  \u22121              n         Zi                  similar to Li et al. (2023).                      i=1Model Specifications We created our sentence   approximately 56M sentences. This masked auto-\nembedding model by fine-tuning INDUSBASE. Here-   encoder model consisted of a full encoder along\nafter, we refer to the resulting retriever model as   with a shallow decoder. The model uses masked\nINDUS-RETRIEVERBASE. We followed a bi-encoder   language modeling with a training objective to re-\nframework (Reimers and Gurevych, 2019), and ex-   cover the original sentence based on the decoder\u2019s\nperimented with multiple pooling strategies and   masked input and the sentence embedding gener-\nfound that the mean pooling of the contextualized    ated from the encoder\u2019s masked input, via masked\ntransformer representations performed the best.      language modelling. There is no distillation loss\n                                                      contributing to this step, which can be viewed as an\nTraining Details We trained each stage on 2\n                                                extended pretraining mechanism. We find that the\nA100 GPUs with an effective batch size of 1,024.\n                                        RetroMAE pretraining does not give us good gains\nWe first trained with unsupervised data for 300K\n                                                       in the larger model but improves the performance\nsteps followed by an additional 100K steps with the\n                                                    of the smaller model.\nsup"
                    }
                ]
            }
        }
    },
    {
        "question": "What are the evaluation done for each model?",
        "answer": "The evaluation of the INDUS models was conducted by comparing them against open-source models of similar sizes using specific datasets and metrics. The models evaluated include INDUSBASE, INDUSSMALL, RoBERTaBASE, SCIBERT, MINILM, and TINYBERT. The primary evaluation metric used was the Entity F1 score across various datasets such as BC5-chem, BC5-disease, NCBI-disease, BC2GM, and JNLPBA. For instance, INDUSBASE achieved an Entity F1 score of 93.3 on BC5-chem, outperforming RoBERTaBASE (90.3) and SCIBERT (91.4). Similarly, INDUSSMALL scored 90.7 on BC5-chem, surpassing MINILM (86.1) and TINYBERT (84.6) (bhattacharjee2024induseffectiveand pages 6-7).\n\nAdditionally, the models were evaluated on the NASA-QA benchmark and information retrieval tasks. INDUSBASE achieved an F1 score of 68.2, outperforming RoBERTa (66.8) and SCIBERT (63.5), while INDUSSMALL scored 47.4, showing strong performance relative to its counterparts (bhattacharjee2024induseffectiveand pages 8-8).\n\nRoBERTa was evaluated using the GLUE, SQuAD, and RACE benchmarks, achieving state-of-the-art results across all nine GLUE tasks. For example, RoBERTa scored 90.2 on MNLI, 94.7 on QNLI, and 92.2 on QQP (liu2019robertaarobustly pages 7-8).",
        "references": "1. (bhattacharjee2024induseffectiveand pages 6-7): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\n2. (liu2019robertaarobustly pages 7-8): Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, Jul 2019.\n\n3. (bhattacharjee2024induseffectiveand pages 8-8): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
        "cost": 0.051530000000000006,
        "token_counts": {
            "gpt-4o-2024-08-06": [
                13056,
                1889
            ]
        },
        "used_contexts": [
            "bhattacharjee2024induseffectiveand pages 6-7",
            "bhattacharjee2024induseffectiveand pages 8-8",
            "liu2019robertaarobustly pages 7-8"
        ],
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "doc_details": {
                    "embedding": null,
                    "docname": "bhattacharjee2024induseffectiveand",
                    "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
                    "dockey": "2b647dabce87f56c",
                    "overwrite_fields_from_metadata": true,
                    "key": "bhattacharjee2024induseffectiveand",
                    "bibtex": "@article{bhattacharjee2024induseffectiveand,\n    author = \"Bhattacharjee, Bishwaranjan and Trivedi, Aashka and Muraoka, Masayasu and Ramasubramanian, Muthukumaran and Udagawa, Takuma and Gurung, I. and Zhang, Rong and Dandala, Bharath and Ramachandran, Rahul and Maskey, M. and Bugbee, Kayleen and Little, Mike and Fancher, Elizabeth and Sanders, Lauren and Costes, Sylvain and Blanco-Cuaresma, Sergi and Lockhart, Kelly and Allen, Thomas and Grazes, Felix and Ansdel, Megan and Accomazzi, A. and El-Kurdi, Yousef and Wertheimer, Davis and Pfitzmann, Birgit and Ramis, Cesar Berrospi and Dolfi, Michele and de Lima, Rafael Teixeira and Vegenas, Panos and Mukkavilli, S. K. and Staar, P. and Vahidinia, S. and McGranaghan, Ryan and Mehrabian, A. and Lee, Tsendgar\",\n    title = \"INDUS: Effective and Efficient Language Models for Scientific Applications\",\n    year = \"2024\",\n    journal = \"ArXiv\",\n    volume = \"abs/2405.10725\",\n    month = \"May\",\n    doi = \"10.48550/arxiv.2405.10725\",\n    url = \"https://doi.org/10.48550/arxiv.2405.10725\"\n}\n",
                    "authors": [
                        "Bishwaranjan Bhattacharjee",
                        "Aashka Trivedi",
                        "Masayasu Muraoka",
                        "Muthukumaran Ramasubramanian",
                        "Takuma Udagawa",
                        "I. Gurung",
                        "Rong Zhang",
                        "Bharath Dandala",
                        "Rahul Ramachandran",
                        "M. Maskey",
                        "Kayleen Bugbee",
                        "Mike Little",
                        "Elizabeth Fancher",
                        "Lauren Sanders",
                        "Sylvain Costes",
                        "Sergi Blanco-Cuaresma",
                        "Kelly Lockhart",
                        "Thomas Allen",
                        "Felix Grazes",
                        "Megan Ansdel",
                        "A. Accomazzi",
                        "Yousef El-Kurdi",
                        "Davis Wertheimer",
                        "Birgit Pfitzmann",
                        "Cesar Berrospi Ramis",
                        "Michele Dolfi",
                        "Rafael Teixeira de Lima",
                        "Panos Vegenas",
                        "S. K. Mukkavilli",
                        "P. Staar",
                        "S. Vahidinia",
                        "Ryan McGranaghan",
                        "A. Mehrabian",
                        "Tsendgar Lee"
                    ],
                    "publication_date": "2024-05-17T00:00:00",
                    "year": 2024,
                    "volume": "abs/2405.10725",
                    "issue": null,
                    "issn": null,
                    "pages": null,
                    "journal": "ArXiv",
                    "publisher": null,
                    "url": null,
                    "title": "INDUS: Effective and Efficient Language Models for Scientific Applications",
                    "citation_count": 4,
                    "bibtex_type": "article",
                    "source_quality": -1,
                    "is_retracted": null,
                    "doi": "10.48550/arxiv.2405.10725",
                    "doi_url": "https://doi.org/10.48550/arxiv.2405.10725",
                    "doc_id": "2b647dabce87f56c",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "d50eb618b75ce6f837fd65245393f1b28ab1d911",
                        "externalIds": {
                            "DBLP": "journals/corr/abs-2405-10725",
                            "ArXiv": "2405.10725",
                            "DOI": "10.48550/arXiv.2405.10725",
                            "CorpusId": 269899887
                        },
                        "venue": "arXiv.org",
                        "citationCount": 4,
                        "influentialCitationCount": 0,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle"
                        ],
                        "publicationDate": "2024-05-17",
                        "citationStyles": {
                            "bibtex": "@Article{Bhattacharjee2024INDUSEA,\n author = {Bishwaranjan Bhattacharjee and Aashka Trivedi and Masayasu Muraoka and Muthukumaran Ramasubramanian and Takuma Udagawa and I. Gurung and Rong Zhang and Bharath Dandala and Rahul Ramachandran and M. Maskey and Kayleen Bugbee and Mike Little and Elizabeth Fancher and Lauren Sanders and Sylvain Costes and Sergi Blanco-Cuaresma and Kelly Lockhart and Thomas Allen and Felix Grazes and Megan Ansdel and A. Accomazzi and Yousef El-Kurdi and Davis Wertheimer and Birgit Pfitzmann and Cesar Berrospi Ramis and Michele Dolfi and Rafael Teixeira de Lima and Panos Vegenas and S. K. Mukkavilli and P. Staar and S. Vahidinia and Ryan McGranaghan and A. Mehrabian and Tsendgar Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {INDUS: Effective and Efficient Language Models for Scientific Applications},\n volume = {abs/2405.10725},\n year = {2024}\n}\n"
                        },
                        "matchScore": 201.18681,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt provides evaluation results for various models on the BLURB benchmark, which focuses on natural language understanding and reasoning tasks in the biomedical domain. The models compared include INDUS-RETRIEVERBASE, INDUS-RETRIEVERSMALL, BGEBASE15, RoBERTaBASE, MINILM-V216, and BGESMALL17. Performance metrics such as Macro F1, Micro F1, and accuracy are reported across tasks like JNLPBA, PICO, ChemProt, DDI, GAD, HoC, PubMedQA, BioASQ, and BIOSSES. Results are averaged over 10 random seeds with standard deviations provided. INDUS models generally show competitive performance, with specific scores highlighted in bold for highest performance and underlined for significant differences.\n\n8",
                        "score": 8,
                        "text": "  JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     PICO      Macro F1  EBM PICO     72.3 (0.3)   72.4 (0.3)    73.1 (0.2)     67.4 (0.2)   70.3 (0.1)     73.1 (0.2)\n                            ChemProt    50.4 (28.2)   73.9 (0.7)    76.9 (0.5)     56.2 (3.2)   55.9 (2.1)     71.7 (0.9)\n     Relation\n                 Micro F1     DDI        78.6 (1.5)   80.1 (1.0)    81.7 (0.5)     39.3 (5.3)   51.5 (2.9)     69.0 (1.2)    Extraction\n                     GAD        80.0 (1.1)   81.6 (1.2)    79.4 (5.6)     76.4 (1.3)   77.3 (1.0)     81.3 (0.7)\n   Document\n   Classification   Micro F1     HoC        82.2 (0.7)   83.1 (0.6)    83.7 (0.5)     41.6 (6.8)   62.8 (4.7)     80.2 (0.6)\n     Question             PubMedQA     53.1 (3.3)   54.3 (3.8)    58.2 (6.7)     50.3 (1.4)   51.6 (1.7)     56.1 (1.4)\n                 Accuracy\n   Answering              BioASQ      69.1 (4.8)   74.6 (4.5)    69.6 (5.8)     74.3 (3.6)   66.7 (2.3)     75.4 (3.3)\n     Sentence\n     Similarity      Pearson    BIOSSES     79.8 (6.3)   86.3 (3.5)    72.2 (9.5)     88.2 (1.1)   26.6 (8.7)     70.4 (3.3)\n Micro Average         -                 -          75.9 (3.7)   79.2 (1.3)    78.9 (2.4)     67.6 (1.9)   66.1 (1.9)     76.2 (1.0)\n Macro Average         -                 -          74.9 (3.7)   78.2 (1.6)    76.4 (3.2)     65.6 (2.4)   60.6 (3.0)     74.3 (1.3)\n\nTable 5: Evaluation results on BLURB. Results reported are averaged on 10 random seeds with standard deviation in\nparenthesis. Micro average is reported across datasets while macro average is computed by first averaging scores on\neach task (say, task average), followed by averaging the task average across tasks. Results in bold indicate highest\nperformance while underlined results indicate significant difference from second highest result by more than two\nstandard deviations in each model size.\n\n\n  \u2022 INDUS-RETRIEVERBASE  was  compared  to             Model        F1 (SD)\n   BGEBASE15 and a RoBERTaBASE model finetuned             RoBERTa     60.8 (0.8)\n   with the same method presented in \u00a74.1.                  SCIBERT      61.8 (0.7)\n  \u2022 INDUS-RETRIEVERSMALL was compared  to              INDUSBASE    64.0 (1.0)\n   MINILM-V216 and BGESMALL17.                        TINYBERT    34.3 (1.6)\n                                                 MINILM      44.7 (1.3)\n6.1  Natural Language Understanding                     INDUSSMALL   54.8 (0.8)\n    Benchmarks\n                                                     Table 6: CLIMATE-CHANGE NER benchmark results.\nWe evaluated our models on BLURB (Gu et al.,                                                    Standard deviation over 10 random seeds shown in\n2021), a benchmark suite for natural language un-    parenthesis. Results in bold and underline indicate high-\nderstanding and reasoning tasks in the biomedical     est performance and significant difference from second\ndomain. We followed the original work to compute    highest r"
                    },
                    {
                        "context": "The excerpt provides evaluation results for various models on the NASA-QA benchmark and information retrieval tasks. Specifically, it mentions that models were fine-tuned for 15 epochs, with results shown in Table 7. The F1 scores for different models are reported: RoBERTa (66.8), SCIBERT (63.5), INDUSBASE (68.2), TINYBERT (43.2), MINILM (59.2), and INDUSSMALL (47.4). INDUSBASE outperformed all models of similar sizes, while INDUSSMALL showed strong performance relative to its counterparts. Additionally, the NASA-IR dataset evaluation showed Recall@10, and BEIR reported the average nDCG@10 across all tasks, with retrieval time per query on the NQ task from BEIR reported in seconds.\n\n8",
                        "score": 8,
                        "text": "gmented the training     MINILM-V2                 0.62         0.39         0.24\nset with relevant SQuAD pairs for fine-tuning. All     BGESMALL                    0.66         0.51         0.42\nmodels are fine tuned for 15 epochs, and the results     INDUS-RETRIEVERSMALL      0.73         0.42         0.26\nare shown in Table 7. We observed that INDUSBASE                                                     Table 8:  Evaluation results on NASA-IR and BEIR.\noutperformed all models of similar sizes, while IN-   NASA-IR showed Recall@10 while BEIR reported the\nDUSSMALL had relatively strong performance com-   average nDCG@10 across all tasks. Retrieval time per\npared to its counterparts.                            query on the NQ task from BEIR, reported in seconds.\n\n          Model        F1 (SD)\n                                       7  Conclusions\n          RoBERTa     66.8 (3.1)\n          SCIBERT      63.5 (1.9)                In this research, we presented INDUS, a constella-\n           INDUSBASE    68.2 (2.9)                 tion of models for use in the science domain. We\n          TINYBERT    43.2 (2.3)              demonstrated the effectiveness of a custom tok-\n         MINILM      59.2 (3.9)                enizer and in-domain data for training high qual-\n           INDUSSMALL   47.4 (1.8)                   ity encoder models and sentence embedding mod-\n                                                              els.  Further, we created smaller versions of the\nTable 7: NASA-QA benchmark results. Standard devi-   proposed models suitable for applications with la-\nation over 3 random seeds shown in parenthesis. Re-                                                  tency or resource constraints through state-of-the-\nsults in bold and underline indicate highest performance\n                                                             art knowledge distillation techniques. For the ben-\nand significant difference from second highest result by\n                                                                  efit of the scientific community, we will releasemore than two standard deviations in each model size,\nrespectively.                                         the developed models and benchmark datasets on\n                                           Hugging Face.\n\n  We saw that INDUSBASE outperformed all models\nof similar sizes, while INDUSSMALL had relatively                                          References\nstrong performance.\n                                          Dogu Araci. 2019. Finbert: Financial sentiment analy-\n                                                                      sis with pre-trained language models.\n6.4  Information Retrieval Benchmarks\n                                                            Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nWe evaluated our models on the NASA-IR dataset                                                 ERT: A pretrained language model for scientific text.\nas well as BEIR Bench"
                    },
                    {
                        "context": "Not applicable\n\n1",
                        "score": 0,
                        "text": "       0.53   0.48        0.29    0.50        0.21     0.33       0.23      0.61      0.23      0.71     0.42\n\n                                    Table 11: Evaluation results BEIR."
                    },
                    {
                        "context": "The excerpt provides evaluation results for the INDUS-RETRIEVERSMALL model, specifically in the context of NASA-QA and BEIR benchmarks. The evaluation metrics reported include Recall10 for NASA-QA and nDCG10 for BEIR. The results are presented in Table 9, showing that the INDUS-RETRIEVERSMALL model achieved a Recall10 of 0.73 in a one-stage training setup and 0.72 in a stagewise setup for NASA-QA. For BEIR, the nDCG10 scores were 0.42 for the one-stage and 0.41 for the stagewise setup. These metrics provide a quantitative assessment of the model's performance across different tasks.\n\n8",
                        "score": 8,
                        "text": "sa, Jordi Huguet, Paula Marne,     Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexan-\n  Tania Mench\u00f3n, Gr\u00e9gory Operto, Albina Polo, San-      der Dunn, Kristin Persson, Gerbrand Ceder, and\n   dra Pradas, Anna Soteras, Marc Vilanova, and Na-     Anubhav Jain. 2021. The impact of domain-specific\n   talia Vilor-Tejedor. 2020.  Novel tau biomarkers       pre-training on named entity recognition tasks in ma-\n  phosphorylated at t181, t217 or t231 rise in the ini-        terials science. Available at SSRN 3950755.\n   tial stages of the preclinical alzheimer&#x2019;s\n  <i>continuum</i> when only subtle changes in    Liang Wang, Nan Yang, Xiaolong Huang, Binxing\n  a&#x3b2; pathology are detected. EMBO Molec-       Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\n   ular Medicine, 12(12):e12921.                       and Furu Wei. 2022. Text embeddings by weakly-\n                                                          supervised contrastive pre-training.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\n  hishek Srivastava, and Iryna Gurevych. 2021. Beir:   Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\n A heterogenous benchmark for zero-shot evaluation      and Furu Wei. 2021. MiniLMv2: Multi-head self-\n   of information retrieval models.                           attention relation distillation for compressing pre-\n                                                              trained transformers. In Findings of the Association\nJames   Thorne,   Andreas   Vlachos,    Christos       for Computational Linguistics: ACL-IJCNLP 2021,\n   Christodoulopoulos,  and  Arpit  Mittal.  2018.      pages 2140\u20132151, Online. Association for Computa-\n  FEVER: a large-scale dataset for fact extraction       tional Linguistics.Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and     Model                    Training    NASA-IR  BEIR Avg.\n  Tie-Yan Liu. 2013. A theoretical analysis of ndcg     INDUS-RETRIEVERSMALL  One-Stage     0.73        0.42\n   type ranking measures. In Proceedings of the 26th     INDUS-RETRIEVERSMALL   Stagewise     0.72        0.41\n  Annual Conference on Learning Theory, volume 30\n                                                     Table 9: Ablation Study: Evaluation results on NASA-   of Proceedings of Machine Learning Research, pages\n  25\u201354, Princeton, NJ, USA. PMLR.              QA and BEIR. NASA-QA showed Recall10 while BEIR\n                                                         reported nDCG10.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\n  Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\n   badur, David Rosenberg, and Gideon Mann. 2023.  C  Complete Results on BEIR Benchmark\n  Bloomberggpt: A large language model for finance.\n                                                  Table 11 shows the per-dataset results on the BEIR\nShitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.\n                                                          tasks.   2022. RetroMAE: Pre-training retrieval-oriented lan-\n  guage models via masked auto-encoder. In "
                    },
                    {
                        "context": "The excerpt discusses the evaluation of INDUS models against open-source models of similar sizes using specific datasets and metrics. The models compared include INDUSBASE against RoBERTaBASE and SCIBERT, and INDUSSMALL against MINILM and TINYBERT. The evaluation metric used is Entity F1 across various datasets such as BC5-chem, BC5-disease, NCBI-disease, BC2GM, and JNLPBA. For instance, INDUSBASE achieved an Entity F1 score of 93.3 on BC5-chem, outperforming RoBERTaBASE (90.3) and SCIBERT (91.4). Similarly, INDUSSMALL scored 90.7 on BC5-chem, surpassing MINILM (86.1) and TINYBERT (84.6).\n\n9",
                        "score": 9,
                        "text": "cts with these annotated paragraphs. On an\ncomplex taxonomies used in climate-related litera-\n                                                   average, each query is 12 words long, and each\nture. This dataset comprises 534 abstracts sourced\n                                                 paragraph is 120 words long. We used Recall@10\nfrom Semantic Scholar Academic Graph (Kinney\n                                                   as evaluation metric since each question has only\net al., 2023), collected using a seed set of climate-\n                                             one relevant document.\nrelated keywords such as wildfire or floods. The\nabstracts were annotated using the IOB (inside, out-                                       6  Experimental Results\nside, beginning) tagging scheme and encompasses\na diverse array of entity types, as shown in Table 4.   Baselines We compared INDUS models against\n                                            open source models of similar sizes:\n5.2  NASA-QA                                                               \u2022 INDUSBASE was compared to RoBERTaBASE11\nWe present NASA-QA9, an extractive question an-      and SCIBERT12.\nswering task focused on the Earth science domain.     \u2022 INDUSSMALL was compared to MINILM  (6-\nFirst, 39 paragraphs from Earth science papers        layer)13 and TINYBERT (4-layer)14.\nwhich appeared in AGU and AMS journals (\u00a72) were\n                                                                 10https://huggingface.co/datasets/nasa-impact/nasa-smd-\nsourced. Subject matter experts from NASA for-    IR-benchmark\nmulated questions and marked the corresponding       11https://huggingface.co/FacebookAI/roberta-base\n                                                                   12https://huggingface.co/allenai/scibert_scivocab_uncased\n   8https://huggingface.co/datasets/ibm/Climate-Change-        13https://huggingface.co/nreimers/MiniLM-L6-H384-\nNER                                                     uncased\n   9https://huggingface.co/datasets/nasa-impact/nasa-smd-        14https://huggingface.co/huawei-\nqa-benchmark                                         noah/TinyBERT_General_4L_312D                                             Base model (125M params.)          Small model (\u223c30M params.)\n      Task         Metric       Dataset     RoBERTa   SCIBERT   INDUSBASE  TINYBERT  MINILM   INDUSSMALL\n                          BC5-chem     90.3 (0.2)   91.4 (0.2)    93.3 (0.2)     84.6 (0.2)   86.1 (0.3)     90.7 (0.1)\n                             BC5-disease    81.5 (0.3)   83.7 (0.3)    85.2 (0.3)     74.0 (0.4)   77.4 (0.3)     81.3 (0.3)\n    NER        Entity F1   NCBI-disease    87.6 (0.6)   87.6 (0.4)    88.3 (0.4)     81.2 (0.4)   83.1 (0.5)     85.6 (0.6)\n                     BC2GM      82.1 (0.3)   82.3 (0.2)    84.0 (0.3)     74.7 (0.4)   77.1 (0.2)     79.7 (0.3)\n                       JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     "
                    },
                    {
                        "context": "The excerpt discusses the evaluation of various language models, specifically focusing on INDUS models compared to others like RoBERTa, SCIBERT, TINYBERT, and MINILM. It highlights that \"INDUSBASE significantly outperforms the general-purpose RoBERTa model on micro/macro average\" and shows competitive performance to SCIBERT. For smaller models, \"INDUSSMALL outperformed the baselines, TINYBERT and MINILM, by a large margin\" in tasks such as NER, PICO, relation extraction, and document classification. The effectiveness of knowledge distillation from the domain-specific teacher model, INDUSBASE, is emphasized. Additionally, INDUS models excel in the CLIMATE-CHANGE NER task, outperforming baseline models.\n\n8",
                        "score": 8,
                        "text": "and significant difference from second\ndomain. We followed the original work to compute    highest result by more than two standard deviations in\nthe overall score (i.e., macro average).               each model size, respectively.\n  Table 5 shows the evaluation results. Among\nbase models,  INDUSBASE  significantly  outper-                                   We also noticed SCIBERT tends to perform better\nforms the general-purpose RoBERTa model on mi-                                                 than our model on paired input-text tasks, such\ncro/macro average while achieving competitive per-                                                   as QA and semantic similarity tasks, although\nformance to the bio-domain-specific counterpart,                                                    the results have relatively large standard devia-\nSCIBERT.                                                          tions. We hypothesized that the additional next\n  As for smaller models, we noticed INDUSSMALL                                                 sentence prediction objective during training in\noutperformed  the  baselines,  TINYBERT  and                                                  BERT-style models (such as SCIBERT) in contrast to\nMINILM, by a large margin in most cases, show-                                                    the RoBERTa-style models (such as RoBERTaBASE\ning significant difference from second best models                                              and INDUS) may be beneficial for paired input-text\nin NER, PICO, relation extraction, and document                                                         tasks. This trend was consistent with the observa-\nclassification tasks. This demonstrates the effec-                                                        tions of Tinn et al. (2023).\ntiveness of knowledge distillation from our domain-\nspecific teacher model, INDUSBASE.                  6.2  CLIMATE-CHANGE NER\n                                      As shown in Table 6, our models clearly outper-   15https://huggingface.co/BAAI/bge-base-en-v1.5\n   16sentence-transformers/all-MiniLM-L6-v2             formed the corresponding baseline models on the\n   17https://huggingface.co/BAAI/bge-small-en-v1.5        CLIMATE-CHANGE NER task, suggesting the effec-tiveness of training on large domain-specific data.     Model                 NASA-IR \u2191  BEIR Avg. \u2191   Retrieval\n                                                                                                      Time \u2193\n                                                             RoBERTaBASE                0.66         0.37         1.20\n6.3  NASA-QA                                            BGEBASE                     0.67         0.52         1.18\n                                                           INDUS-RETRIEVERBASE       0.71         0.41         1.19\nAs mentioned in \u00a75, we augmented the training     MINILM-V2                 0.62         0.39         0.24\nset with relevant "
                    },
                    {
                        "context": "The excerpt does not provide specific details on the evaluation metrics or results for the models discussed. It primarily focuses on the methodology, including the architecture of the models and the tokenization process. The INDUS model architecture is based on RoBERTaBASE with 12 layers and 125 million parameters, trained for 500,000 steps using 192 V100 GPUs. A smaller model, INDUSSMALL, with 38 million parameters, was trained using knowledge distillation from INDUSBASE. The excerpt also compares tokenization between RoBERTa and INDUSBPE but does not mention evaluation results or metrics.\n\n1",
                        "score": 1,
                        "text": "tem (ESDIS).  It contains text   model architecture follows RoBERTaBASE (Liu et al.,\n   descriptions of the NASA Earth science data    2019), which consists of 12 layers and has 125M\n   products.                                       parameters. We adopted the default hyperparame-\n                                                            ters7 but with an effective batch size of 92, 16. We\n3  Methodology: Encoder Models              trained the model for 500K steps using 192 V100\n                                         GPUs.\n3.1  INDUSBPE Tokenizer\n                                                    3.3  Knowledge Distillation for EfficientWe trained BPE tokenizer (Radford et al., 2019),\n                                              Encoder ModelINDUSBPE from scratch using a subset of our train-\n                                  We also trained a smaller model, INDUSSMALL, with\n   2https://www.ncbi.nlm.nih.gov/pmc             38M parameters through knowledge distillation\n   3https://www.ametsoc.org/index.cfm/ams/publications/\n    4https://agupubs.onlinelibrary.wiley.com/                 6We  used HF  tokenizers,  https://github.com/\n   5https://www.earthdata.nasa.gov/eosdis/science-system-    huggingface/tokenizers\ndescription/eosdis-components/cmr                        7We refer readers to Table 9 in (Liu et al., 2019). Input text\n novel tau biomarkers phosphorylated at t181, t217 or t231 rise in the initial stages of the preclinical\n alzheimer\u2019s continuum when only subtle changes in a pathology are detected\n Tokenization by RoBERTa\n <s> no vel t au biomark ers phosph ory lated at t 181 , t 217 , or t 231 rise in the initial stages of the preclinical\n  al z heimer \u2019 s continuum when only subtle changes in a pathology are detected </s>\n Tokenization by INDUSBPE\n <s> novel tau biomarkers phosphorylated at t 181 , t 217 , or t 231 rise in the initial stages of the preclinical\n alzheimer \u2019 s continuum when only subtle changes in a pathology are detected </s>\n\nTable 3: Tokenization comparison between RoBERTa and our tokenizers. Input text adapted from Su\u00e1rez-Calvet\net al. (2020).\n\n\n                   X   Xtechniques by using INDUSBASE as the teacher. IN-          Zi =      es(qi,pj) +     es(qj,pi)\nDUSSMALL follows a 4-layer architecture recom-                         j                  j\n                                                                                                        (2)\nmended by the Neural Architecture Search engine                   X es(qi,qj) + X es(pi,pj)                                  +\n(Trivedi et al., 2023) with an optimal trade-off be-\n                                                                                          j\u0338=i               j\u0338=i\ntween performance and latency. We adopted the\n                                               where s(q, p) is a measure of temperature-scaled\ndistillation objective proposed in MiniLMv2 (Wang\n                                                    cosine similarity betwee"
                    },
                    {
                        "context": "The excerpt describes the evaluation of INDUS models using various benchmarks. The models include a general-purpose encoder and a retriever, along with their distilled versions. Benchmarks used for evaluation include BC5-CHEM, BC5-Disease, NCBI-Disease, BC2GM, NASA-QA, TREC-Covid, NFCorpus, NQ, HotpotQA, JNLPBA, EBM-PICO, ChemProt, DDI, GAD, HoC, PubMedQA, BioASQ, Climate Change NER, FiQA, Arguana, Touche, DBPedia, NASA-IR, SciDocs, FEVER, Climate FEVER, and SciFact. The excerpt highlights new benchmarks such as NASA-QA, CLIMATE-CHANGE NER, and NASA-IR. The knowledge-distilled models showed a significant boost in latency while maintaining strong empirical performance on most tasks.\n\n8",
                        "score": 8,
                        "text": "                                                                                        Small\n\n\n\n\n        BC5-CHEM       BC5-Disease      NCBI-Disease       BC2GM\n                                                                     NASA-QA              TREC-Covid       NFCorpus        NQ           HotpotQA\n\n         JNLPBA         EBM-PICO        ChemProt           DDI\n                                                                                       Climate                 FiQA           Arguana         Touche         DBPedia        NASA-IR\n         GAD            HoC         PubMedQA         BioASQ         Change NER\n                                                                                                             SciDocs         FEVER           Climate FEVER            SciFact\n         BIOSSES         BLURB Benchmark                                                          BEIR Benchmark\n\n                             Natural Language Understanding Benchmarks                                     Information Retrieval Benchmarks\n\n\nFigure 1: Overview of INDUS models: the general-purpose encoder model and the retriever built from it, and their\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\nNASA-QA, CLIMATE-CHANGE NER and NASA-IR.\n\n\nenabling them in informed decision-making.            (Beltagy et al., 2019). We also show that the\n   Specifically, we make the following contribu-      knowledge-distilled models achieved a signifi-\ntions:                                                 cant boost in latency while maintaining strong\n1. Utilizing the byte-pair encoding algorithm,       empirical performance compared to the original\n  we constructed INDUSBPE, a customized to-      models on most of the benchmark tasks.\n   kenizer from the curated scientific corpus.\n                                       2  Data2. We pretrained multiple encoder-only LLMs us-\n   ing the curated scientific corpora and the IN-                                                        Sufficient high-quality in-domain corpora is essen-\n  DUSBPE tokenizer (\u00a72, \u00a73). We further created                                                                  tial to develop models that perform better than\n   sentence-embedding models by fine-tuning the                                                           their counterparts trained on open-domain corpora.\n   encoder-only models with a contrastive learn-                                 We meticulously identified corpora for each of the\n   ing objective to learn \u201cuniversal\u201d sentence em-                                                 aforementioned domains, and created English-only\n   beddings (Gao et al., 2021) (\u00a74). We also                                            models for the sake of containment. Specifically,\n   trained smaller, more efficient versions of                                                        for each of the domains, we used open-source dat"
                    }
                ]
            },
            "liu2019robertaarobustly": {
                "doc_details": {
                    "embedding": null,
                    "docname": "liu2019robertaarobustly",
                    "citation": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, Jul 2019.",
                    "dockey": "7fa1f2d78384bec5",
                    "overwrite_fields_from_metadata": true,
                    "key": "liu2019robertaarobustly",
                    "bibtex": "@article{liu2019robertaarobustly,\n    author = \"Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, M. and Zettlemoyer, Luke and Stoyanov, Veselin\",\n    title = \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n    year = \"2019\",\n    journal = \"ArXiv\",\n    volume = \"abs/1907.11692\",\n    month = \"Jul\"\n}\n",
                    "authors": [
                        "Yinhan Liu",
                        "Myle Ott",
                        "Naman Goyal",
                        "Jingfei Du",
                        "Mandar Joshi",
                        "Danqi Chen",
                        "Omer Levy",
                        "M. Lewis",
                        "Luke Zettlemoyer",
                        "Veselin Stoyanov"
                    ],
                    "publication_date": "2019-07-26T00:00:00",
                    "year": 2019,
                    "volume": "abs/1907.11692",
                    "issue": null,
                    "issn": null,
                    "pages": null,
                    "journal": "ArXiv",
                    "publisher": null,
                    "url": null,
                    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                    "citation_count": 21175,
                    "bibtex_type": "article",
                    "source_quality": -1,
                    "is_retracted": null,
                    "doi": null,
                    "doi_url": null,
                    "doc_id": "7fa1f2d78384bec5",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
                        "externalIds": {
                            "DBLP": "journals/corr/abs-1907-11692",
                            "MAG": "2965373594",
                            "ArXiv": "1907.11692",
                            "CorpusId": 198953378
                        },
                        "venue": "arXiv.org",
                        "citationCount": 21175,
                        "influentialCitationCount": 4898,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle"
                        ],
                        "publicationDate": "2019-07-26",
                        "citationStyles": {
                            "bibtex": "@Article{Liu2019RoBERTaAR,\n author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},\n volume = {abs/1907.11692},\n year = {2019}\n}\n"
                        },
                        "matchScore": 221.36655,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt provides development set results for RoBERTa on the GLUE benchmark, detailing performance metrics for different configurations. RoBERTaBASE with all data and 500k steps achieved scores such as 87.6 on MNLI, 92.8 on QNLI, and 91.9 on QQP. RoBERTaLARGE, trained with BOOKS and WIKI, scored 89.0 on MNLI, 93.9 on QNLI, and 91.9 on QQP. Further enhancements, like additional data and extended pretraining (300k and 500k steps), improved scores, with the highest being 90.2 on MNLI, 94.7 on QNLI, and 92.2 on QQP. These results illustrate the model's performance across various tasks.\n\n9",
                        "score": 9,
                        "text": "Finetuning Hyperparameters\nAlex Wang,  Yada  Pruksachatkun,  Nikita  Nangia,\n  Amanpreet Singh, Julian Michael, Felix Hill, Omer    Finetuning hyperparameters for RACE, SQuAD\n  Levy, and Samuel R. Bowman. 2019a. SuperGLUE:\n                                             and GLUE are given in Table 10. We select the  A stickier benchmark for general-purpose language\n  understanding systems. arXiv preprint 1905.00537.    best hyperparameter values based on the median\n                                                    of 5 random seeds for each task.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\n   Hill, Omer Levy, and Samuel R. Bowman. 2019b.\n  GLUE: A multi-task benchmark and analysis plat-\n  form for natural language understanding. In Inter-\n   national Conference on Learning Representations\n  (ICLR).\n\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\n  man. 2018.   Neural network acceptability judg-\n  ments. arXiv preprint 1805.12471.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n  2018. A broad-coverage challenge corpus for sen-\n   tence understanding through inference.  In North\n  American Association for Computational Linguis-\n   tics (NAACL).\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\n   bonell, Ruslan Salakhutdinov, and Quoc V Le.\n  2019.  Xlnet: Generalized autoregressive pretrain-\n   ing for language understanding.   arXiv preprint\n  arXiv:1906.08237.\n\nYang You, Jing Li, Jonathan Hseu, Xiaodan Song,\n  James Demmel, and Cho-Jui Hsieh. 2019. Reduc-\n   ing bert pre-training time from 3 days to 76 minutes.\n  arXiv preprint arXiv:1904.00962.\n\nRowan  Zellers,  Ari  Holtzman,  Hannah  Rashkin,\n  Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\n   Yejin Choi. 2019.  Defending against neural fake\n  news. arXiv preprint arXiv:1905.12616.\n\nYukun Zhu, Ryan  Kiros,  Richard Zemel,  Ruslan\n   Salakhutdinov, Raquel Urtasun, Antonio Torralba,\n  and Sanja Fidler. 2015. Aligning books and movies:\n  Towards story-like visual explanations by watch-\n   ing movies and reading books.  In arXiv preprint\n  arXiv:1506.06724.\n\n\nAppendix for \u201cRoBERTa: A Robustly\nOptimized BERT Pretraining Approach\u201d\n\nA  Full results on GLUE\n\nIn Table 8 we present the full set of development\nset results for RoBERTa. We present results for\na LARGE con\ufb01guration that follows BERTLARGE,\nas well as a BASE con\ufb01guration  that follows\nBERTBASE.                 MNLI  QNLI QQP  RTE  SST MRPC  CoLA  STS\n\nRoBERTaBASE\n  + all data + 500k steps    87.6     92.8    91.9   78.7   94.8    90.2     63.6    91.2\n\nRoBERTaLARGE\n  with BOOKS + WIKI      89.0     93.9    91.9   84.5   95.3    90.2     66.3    91.6\n  + additional data (\u00a73.2)    89.3     94.0    92.0   82.7   95.6    91.4     66.1    92.2\n  + pretrain longer 300k    90.0     94.5    92.2   83.3   96.1    91.1     67.4    92.3\n  + pretrain longer 500k    90.2     94.7    92.2   86.6   96.4    90.9     68.0    92.4\n\n\n      Table 8: Development set results on GLUE tasks for various con\ufb01gurations of RoBERTa.\n\n\n\n\n\n             Hype"
                    },
                    {
                        "context": "The excerpt evaluates RoBERTa against other models using the GLUE, SQuAD, and RACE benchmarks. Specifically, RoBERTa is compared to BERTLARGE and XLNetLARGE on the GLUE development set, achieving state-of-the-art results across all nine tasks. The results for single-task models on the development set are: RoBERTa (e.g., MNLI: 90.2, QNLI: 94.7, QQP: 92.2, RTE: 86.6, SST: 96.4, MRPC: 90.9, CoLA: 68.0, STS: 92.4, WNLI: 91.3). On the test set, RoBERTa ensembles achieve an average score of 88.5, outperforming other models like ALICE and MT-DNN. The evaluation also notes the use of single-task fine-tuning and task-specific modifications for certain GLUE tasks.\n\n9",
                        "score": 9,
                        "text": "the second setting (ensembles, test), we com-\nels outperform XLNetLARGE across most tasks. We    pare RoBERTa to other approaches on the test set\nnote that even our longest-trained model does not    via the GLUE leaderboard. While many submis-\nappear to over\ufb01t our data and would likely bene\ufb01t    sions to the GLUE leaderboard depend on multi-\nfrom additional training.                             task \ufb01netuning, our submission depends only on\n  In the rest of the paper, we evaluate our best    single-task \ufb01netuning. For RTE, STS and MRPC\nRoBERTa model on the three different bench-   we found it helpful to \ufb01netune starting from the\nmarks: GLUE, SQuaD and RACE. Speci\ufb01cally  MNLI single-task model, rather than the baseline\n                                                      pretrained RoBERTa. We explore a slightly wider\n   9Our experiments con\ufb02ate increases in data size and di-\n                                                hyperparameter space, described in the Appendix,versity. We leave a more careful analysis of these two dimen-\nsions to future work.                               and ensemble between 5 and 7 models per task.           MNLI   QNLI QQP  RTE  SST MRPC  CoLA  STS  WNLI  Avg\n\n  Single-task single models on dev\n BERTLARGE     86.6/-     92.3    91.3   70.4   93.2    88.0     60.6    90.0       -         -\n XLNetLARGE     89.8/-     93.9    91.8   83.8   95.6    89.2     63.6    91.8       -         -\n RoBERTa     90.2/90.2   94.7    92.2   86.6   96.4    90.9     68.0    92.4    91.3       -\n\n  Ensembles on test (from leaderboard as of July 25, 2019)\n ALICE       88.2/87.9   95.7    90.7   83.5   95.2    92.6     68.6    91.1    80.8    86.3\n MT-DNN     87.9/87.4   96.0    89.9   86.3   96.5    92.7     68.4    91.1    89.0    87.6\n XLNet        90.2/89.8   98.6    90.3   86.3   96.8    93.0     67.8    91.6    90.4    88.4\n RoBERTa     90.8/90.2   98.9    90.2   88.2   96.7    92.3     67.8    92.2    89.0    88.5\n\n\nTable 5: Results on GLUE. All results are based on a 24-layer architecture. BERTLARGE and XLNetLARGE results\nare from Devlin et al. (2019) and Yang et al. (2019), respectively. RoBERTa results on the development set are a\nmedian over \ufb01ve runs. RoBERTa results on the test set are ensembles of single-task models. For RTE, STS and\nMRPC we \ufb01netune starting from the MNLI model instead of the baseline pretrained model. Averages are obtained\nfrom the GLUE leaderboard.\n\n\nTask-speci\ufb01c modi\ufb01cations  Two of the GLUE   Results  We present our results in Table 5. In the\ntasks require task-speci\ufb01c \ufb01netuning approaches    \ufb01rst setting (single-task, dev), RoBERTa achieves\nto achieve competitive leaderboard results.            state-of-the-art  results on  all 9 of  the GLUE\n  QNLI:  Recent  submissions on  the GLUE    task development sets. Crucially, RoBERTa uses\nleaderboard adopt a pairwise ranking formulation    the same masked language modeling  pretrain-\nfor the QNLI task, in which candidate answers    ing objective and architectur"
                    }
                ]
            }
        }
    },
    {
        "question": "What is the most strong aspect of indus?",
        "answer": "The most strong aspect of the INDUS models is their superior performance in domain-specific tasks, particularly in scientific applications. These models are specifically designed for interdisciplinary fields such as physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. They are trained using domain-specific vocabulary and corpora, which allows them to outperform both general-purpose encoders like RoBERTa and existing domain-specific encoders such as SCIBERT on new scientific benchmark datasets (bhattacharjee2024induseffectiveand pages 1-1).\n\nINDUS models excel in entity recognition tasks, as demonstrated by INDUSBASE outperforming RoBERTaBASE and SCIBERT on datasets like BC5-chem and BC5-disease. Similarly, INDUSSMALL shows superior performance over MINILM and TINYBERT on the same datasets (bhattacharjee2024induseffectiveand pages 6-7). Additionally, INDUSBASE achieved the highest F1 score on the NASA-QA benchmark, surpassing other models of similar sizes (bhattacharjee2024induseffectiveand pages 8-8).\n\nThe models also utilize advanced techniques such as contrastive learning for text embedding and knowledge distillation to create smaller, efficient versions without significant performance loss. This balance of efficiency and empirical performance is further enhanced by the use of a custom tokenizer, INDUSBPE, tailored for scientific texts (bhattacharjee2024induseffectiveand pages 2-2).",
        "references": "1. (bhattacharjee2024induseffectiveand pages 6-7): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\n2. (bhattacharjee2024induseffectiveand pages 1-1): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\n3. (bhattacharjee2024induseffectiveand pages 8-8): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\n4. (bhattacharjee2024induseffectiveand pages 2-2): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
        "cost": 0.04943000000000001,
        "token_counts": {
            "gpt-4o-2024-08-06": [
                12796,
                1744
            ]
        },
        "used_contexts": [
            "bhattacharjee2024induseffectiveand pages 6-7",
            "bhattacharjee2024induseffectiveand pages 8-8",
            "bhattacharjee2024induseffectiveand pages 2-2",
            "bhattacharjee2024induseffectiveand pages 1-1"
        ],
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "doc_details": {
                    "embedding": null,
                    "docname": "bhattacharjee2024induseffectiveand",
                    "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
                    "dockey": "2b647dabce87f56c",
                    "overwrite_fields_from_metadata": true,
                    "key": "bhattacharjee2024induseffectiveand",
                    "bibtex": "@article{bhattacharjee2024induseffectiveand,\n    author = \"Bhattacharjee, Bishwaranjan and Trivedi, Aashka and Muraoka, Masayasu and Ramasubramanian, Muthukumaran and Udagawa, Takuma and Gurung, I. and Zhang, Rong and Dandala, Bharath and Ramachandran, Rahul and Maskey, M. and Bugbee, Kayleen and Little, Mike and Fancher, Elizabeth and Sanders, Lauren and Costes, Sylvain and Blanco-Cuaresma, Sergi and Lockhart, Kelly and Allen, Thomas and Grazes, Felix and Ansdel, Megan and Accomazzi, A. and El-Kurdi, Yousef and Wertheimer, Davis and Pfitzmann, Birgit and Ramis, Cesar Berrospi and Dolfi, Michele and de Lima, Rafael Teixeira and Vegenas, Panos and Mukkavilli, S. K. and Staar, P. and Vahidinia, S. and McGranaghan, Ryan and Mehrabian, A. and Lee, Tsendgar\",\n    title = \"INDUS: Effective and Efficient Language Models for Scientific Applications\",\n    year = \"2024\",\n    journal = \"ArXiv\",\n    volume = \"abs/2405.10725\",\n    month = \"May\",\n    doi = \"10.48550/arxiv.2405.10725\",\n    url = \"https://doi.org/10.48550/arxiv.2405.10725\"\n}\n",
                    "authors": [
                        "Bishwaranjan Bhattacharjee",
                        "Aashka Trivedi",
                        "Masayasu Muraoka",
                        "Muthukumaran Ramasubramanian",
                        "Takuma Udagawa",
                        "I. Gurung",
                        "Rong Zhang",
                        "Bharath Dandala",
                        "Rahul Ramachandran",
                        "M. Maskey",
                        "Kayleen Bugbee",
                        "Mike Little",
                        "Elizabeth Fancher",
                        "Lauren Sanders",
                        "Sylvain Costes",
                        "Sergi Blanco-Cuaresma",
                        "Kelly Lockhart",
                        "Thomas Allen",
                        "Felix Grazes",
                        "Megan Ansdel",
                        "A. Accomazzi",
                        "Yousef El-Kurdi",
                        "Davis Wertheimer",
                        "Birgit Pfitzmann",
                        "Cesar Berrospi Ramis",
                        "Michele Dolfi",
                        "Rafael Teixeira de Lima",
                        "Panos Vegenas",
                        "S. K. Mukkavilli",
                        "P. Staar",
                        "S. Vahidinia",
                        "Ryan McGranaghan",
                        "A. Mehrabian",
                        "Tsendgar Lee"
                    ],
                    "publication_date": "2024-05-17T00:00:00",
                    "year": 2024,
                    "volume": "abs/2405.10725",
                    "issue": null,
                    "issn": null,
                    "pages": null,
                    "journal": "ArXiv",
                    "publisher": null,
                    "url": null,
                    "title": "INDUS: Effective and Efficient Language Models for Scientific Applications",
                    "citation_count": 4,
                    "bibtex_type": "article",
                    "source_quality": -1,
                    "is_retracted": null,
                    "doi": "10.48550/arxiv.2405.10725",
                    "doi_url": "https://doi.org/10.48550/arxiv.2405.10725",
                    "doc_id": "2b647dabce87f56c",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "d50eb618b75ce6f837fd65245393f1b28ab1d911",
                        "externalIds": {
                            "DBLP": "journals/corr/abs-2405-10725",
                            "ArXiv": "2405.10725",
                            "DOI": "10.48550/arXiv.2405.10725",
                            "CorpusId": 269899887
                        },
                        "venue": "arXiv.org",
                        "citationCount": 4,
                        "influentialCitationCount": 0,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle"
                        ],
                        "publicationDate": "2024-05-17",
                        "citationStyles": {
                            "bibtex": "@Article{Bhattacharjee2024INDUSEA,\n author = {Bishwaranjan Bhattacharjee and Aashka Trivedi and Masayasu Muraoka and Muthukumaran Ramasubramanian and Takuma Udagawa and I. Gurung and Rong Zhang and Bharath Dandala and Rahul Ramachandran and M. Maskey and Kayleen Bugbee and Mike Little and Elizabeth Fancher and Lauren Sanders and Sylvain Costes and Sergi Blanco-Cuaresma and Kelly Lockhart and Thomas Allen and Felix Grazes and Megan Ansdel and A. Accomazzi and Yousef El-Kurdi and Davis Wertheimer and Birgit Pfitzmann and Cesar Berrospi Ramis and Michele Dolfi and Rafael Teixeira de Lima and Panos Vegenas and S. K. Mukkavilli and P. Staar and S. Vahidinia and Ryan McGranaghan and A. Mehrabian and Tsendgar Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {INDUS: Effective and Efficient Language Models for Scientific Applications},\n volume = {abs/2405.10725},\n year = {2024}\n}\n"
                        },
                        "matchScore": 201.18681,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt discusses the development of INDUS, a suite of large language models (LLMs) specifically tailored for scientific domains such as Earth science, biology, physics, heliophysics, planetary sciences, and astrophysics. These models are trained using curated scientific corpora, addressing the limitations of general-purpose LLMs that often perform sub-optimally on domain-specific tasks due to distributional shifts in vocabulary. The strong aspect of INDUS is its focus on domain-specific vocabulary and data, which enhances its performance on specialized natural language understanding and generation tasks compared to general LLMs.\n\n8",
                        "score": 8,
                        "text": "        INDUS: Effective and Efficient Language Models for Scientific Applications\n                  Bishwaranjan Bhattacharjee1, Aashka Trivedi1, Masayasu Muraoka1,\n        Muthukumaran Ramasubramanian3, Takuma Udagawa1, Iksha Gurung3, Rong Zhang1,\n         Bharath Dandala1, Rahul Ramachandran2, Manil Maskey2, Kaylin Bugbee2, Mike Little4,\n      Elizabeth Fancher2, Lauren Sanders5, Sylvain Costes5, Sergi Blanco-Cuaresma6, Kelly Lockhart6,\n         Thomas Allen6, Felix Grezes6, Megan Ansdell7, Alberto Accomazzi6, Yousef El-Kurdi1,\n   Davis Wertheimer1, Birgit Pfitzmann1, Cesar Berrospi Ramis1, Michele Dolfi1, Rafael Teixeira de Lima1,\n     Panagiotis Vagenas1, S. Karthik Mukkavilli1, Peter Staar1, Sanaz Vahidinia7, Ryan McGranaghan8,\n                             Armin Mehrabian9, Tsendgar Lee7\n      1IBM Research AI, 2 NASA MFSC, 3 UAH, 4 Navteca, 5 NASA Ames, 6 Harvard-Smithsonian CfA,\n                                               7 NASA HQ, 8 JPL, 9 NASA GSFC\n\n\n                         Abstract                      generation tasks. Most popular LLMs rely on the\n                                                             transformer architecture (Vaswani et  al., 2017)\n                Large language models (LLMs) trained on gen-      and are trained using general-purpose corpora2024              eral domain corpora showed remarkable re-                                                                      like Wikipedia or CommonCrawl (Devlin et al.,\n                   sults on natural language processing (NLP)\n                                                         2019; Liu et al., 2019; Lewis et al., 2020; Raffel\n                   tasks.  However, previous research demon-\n                                                                        et al., 2020; Brown et al., 2020; Touvron et al.,                   strated LLMs trained using domain-focused cor-May\n                pora perform better on specialized tasks. In-       2023). Although these general-purpose models ex-\n                 spired by this pivotal insight, we developed        hibited strong performance, the distributional shift20\n               INDUS, a comprehensive suite of LLMs tailored       of vocabulary led to sub-optimal performance on\n                  for the Earth science, biology, physics, helio-       domain-specific natural language understanding\n                 physics, planetary sciences and astrophysics                                                      and generation tasks (Beltagy et al., 2019). Fol-\n               domains and trained using curated scientific\n                                                         lowing this observation, several domain-specific\n                corpora drawn from diverse data sources. The\n                                               LLMs such as SCIBERT (Beltagy et  al., 2019),                   suite of models include: (1) an encoder model[cs.CL]\n                  trained using domain-specific vocabulary and      BI"
                    },
                    {
                        "context": "The excerpt highlights the performance of the INDUS models, particularly INDUSBASE and INDUSSMALL, in scientific applications. INDUSBASE outperformed all models of similar sizes on the NASA-QA benchmark, achieving an F1 score of 68.2 with a standard deviation of 2.9, surpassing models like RoBERTa (66.8) and SCIBERT (63.5). INDUSSMALL also demonstrated strong performance relative to its counterparts. The research emphasizes the effectiveness of a custom tokenizer and in-domain data for training high-quality encoder models and sentence embedding models. Additionally, smaller versions of the models were developed for applications with latency or resource constraints using knowledge distillation techniques.\n\n8",
                        "score": 8,
                        "text": "gmented the training     MINILM-V2                 0.62         0.39         0.24\nset with relevant SQuAD pairs for fine-tuning. All     BGESMALL                    0.66         0.51         0.42\nmodels are fine tuned for 15 epochs, and the results     INDUS-RETRIEVERSMALL      0.73         0.42         0.26\nare shown in Table 7. We observed that INDUSBASE                                                     Table 8:  Evaluation results on NASA-IR and BEIR.\noutperformed all models of similar sizes, while IN-   NASA-IR showed Recall@10 while BEIR reported the\nDUSSMALL had relatively strong performance com-   average nDCG@10 across all tasks. Retrieval time per\npared to its counterparts.                            query on the NQ task from BEIR, reported in seconds.\n\n          Model        F1 (SD)\n                                       7  Conclusions\n          RoBERTa     66.8 (3.1)\n          SCIBERT      63.5 (1.9)                In this research, we presented INDUS, a constella-\n           INDUSBASE    68.2 (2.9)                 tion of models for use in the science domain. We\n          TINYBERT    43.2 (2.3)              demonstrated the effectiveness of a custom tok-\n         MINILM      59.2 (3.9)                enizer and in-domain data for training high qual-\n           INDUSSMALL   47.4 (1.8)                   ity encoder models and sentence embedding mod-\n                                                              els.  Further, we created smaller versions of the\nTable 7: NASA-QA benchmark results. Standard devi-   proposed models suitable for applications with la-\nation over 3 random seeds shown in parenthesis. Re-                                                  tency or resource constraints through state-of-the-\nsults in bold and underline indicate highest performance\n                                                             art knowledge distillation techniques. For the ben-\nand significant difference from second highest result by\n                                                                  efit of the scientific community, we will releasemore than two standard deviations in each model size,\nrespectively.                                         the developed models and benchmark datasets on\n                                           Hugging Face.\n\n  We saw that INDUSBASE outperformed all models\nof similar sizes, while INDUSSMALL had relatively                                          References\nstrong performance.\n                                          Dogu Araci. 2019. Finbert: Financial sentiment analy-\n                                                                      sis with pre-trained language models.\n6.4  Information Retrieval Benchmarks\n                                                            Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nWe evaluated our models on the NASA-IR dataset                                                 ERT: A pretrained language model for scientific text.\nas well as BEIR Bench"
                    },
                    {
                        "context": "The excerpt highlights the development of INDUS models, focusing on their efficiency and effectiveness for scientific applications. Key contributions include the creation of INDUSBPE, a tokenizer tailored for scientific texts using byte-pair encoding, and the pretraining of encoder-only large language models (LLMs) on curated scientific corpora. These models were further refined to produce sentence embeddings through contrastive learning. The knowledge-distilled versions of these models demonstrated a significant reduction in latency while maintaining strong performance across various benchmarks, such as NASA-QA and CLIMATE-CHANGE NER. This suggests that INDUS models excel in balancing efficiency and empirical performance.\n\n8",
                        "score": 8,
                        "text": "                                                                                        Small\n\n\n\n\n        BC5-CHEM       BC5-Disease      NCBI-Disease       BC2GM\n                                                                     NASA-QA              TREC-Covid       NFCorpus        NQ           HotpotQA\n\n         JNLPBA         EBM-PICO        ChemProt           DDI\n                                                                                       Climate                 FiQA           Arguana         Touche         DBPedia        NASA-IR\n         GAD            HoC         PubMedQA         BioASQ         Change NER\n                                                                                                             SciDocs         FEVER           Climate FEVER            SciFact\n         BIOSSES         BLURB Benchmark                                                          BEIR Benchmark\n\n                             Natural Language Understanding Benchmarks                                     Information Retrieval Benchmarks\n\n\nFigure 1: Overview of INDUS models: the general-purpose encoder model and the retriever built from it, and their\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\nNASA-QA, CLIMATE-CHANGE NER and NASA-IR.\n\n\nenabling them in informed decision-making.            (Beltagy et al., 2019). We also show that the\n   Specifically, we make the following contribu-      knowledge-distilled models achieved a signifi-\ntions:                                                 cant boost in latency while maintaining strong\n1. Utilizing the byte-pair encoding algorithm,       empirical performance compared to the original\n  we constructed INDUSBPE, a customized to-      models on most of the benchmark tasks.\n   kenizer from the curated scientific corpus.\n                                       2  Data2. We pretrained multiple encoder-only LLMs us-\n   ing the curated scientific corpora and the IN-                                                        Sufficient high-quality in-domain corpora is essen-\n  DUSBPE tokenizer (\u00a72, \u00a73). We further created                                                                  tial to develop models that perform better than\n   sentence-embedding models by fine-tuning the                                                           their counterparts trained on open-domain corpora.\n   encoder-only models with a contrastive learn-                                 We meticulously identified corpora for each of the\n   ing objective to learn \u201cuniversal\u201d sentence em-                                                 aforementioned domains, and created English-only\n   beddings (Gao et al., 2021) (\u00a74). We also                                            models for the sake of containment. Specifically,\n   trained smaller, more efficient versions of                                                        for each of the domains, we used open-source dat"
                    },
                    {
                        "context": "The excerpt discusses the performance of INDUS models in comparison to other open-source models of similar sizes. INDUSBASE and INDUSSMALL are evaluated against RoBERTaBASE, SCIBERT, MINILM, and TINYBERT using the Entity F1 metric across various datasets. INDUSBASE outperforms RoBERTaBASE and SCIBERT on datasets like BC5-chem (93.3 vs. 90.3 and 91.4) and BC5-disease (85.2 vs. 81.5 and 83.7). Similarly, INDUSSMALL shows superior performance over MINILM and TINYBERT on the same datasets, with scores of 90.7 and 81.3, respectively. This indicates the strong aspect of INDUS models is their superior performance in entity recognition tasks.\n\n9",
                        "score": 9,
                        "text": "cts with these annotated paragraphs. On an\ncomplex taxonomies used in climate-related litera-\n                                                   average, each query is 12 words long, and each\nture. This dataset comprises 534 abstracts sourced\n                                                 paragraph is 120 words long. We used Recall@10\nfrom Semantic Scholar Academic Graph (Kinney\n                                                   as evaluation metric since each question has only\net al., 2023), collected using a seed set of climate-\n                                             one relevant document.\nrelated keywords such as wildfire or floods. The\nabstracts were annotated using the IOB (inside, out-                                       6  Experimental Results\nside, beginning) tagging scheme and encompasses\na diverse array of entity types, as shown in Table 4.   Baselines We compared INDUS models against\n                                            open source models of similar sizes:\n5.2  NASA-QA                                                               \u2022 INDUSBASE was compared to RoBERTaBASE11\nWe present NASA-QA9, an extractive question an-      and SCIBERT12.\nswering task focused on the Earth science domain.     \u2022 INDUSSMALL was compared to MINILM  (6-\nFirst, 39 paragraphs from Earth science papers        layer)13 and TINYBERT (4-layer)14.\nwhich appeared in AGU and AMS journals (\u00a72) were\n                                                                 10https://huggingface.co/datasets/nasa-impact/nasa-smd-\nsourced. Subject matter experts from NASA for-    IR-benchmark\nmulated questions and marked the corresponding       11https://huggingface.co/FacebookAI/roberta-base\n                                                                   12https://huggingface.co/allenai/scibert_scivocab_uncased\n   8https://huggingface.co/datasets/ibm/Climate-Change-        13https://huggingface.co/nreimers/MiniLM-L6-H384-\nNER                                                     uncased\n   9https://huggingface.co/datasets/nasa-impact/nasa-smd-        14https://huggingface.co/huawei-\nqa-benchmark                                         noah/TinyBERT_General_4L_312D                                             Base model (125M params.)          Small model (\u223c30M params.)\n      Task         Metric       Dataset     RoBERTa   SCIBERT   INDUSBASE  TINYBERT  MINILM   INDUSSMALL\n                          BC5-chem     90.3 (0.2)   91.4 (0.2)    93.3 (0.2)     84.6 (0.2)   86.1 (0.3)     90.7 (0.1)\n                             BC5-disease    81.5 (0.3)   83.7 (0.3)    85.2 (0.3)     74.0 (0.4)   77.4 (0.3)     81.3 (0.3)\n    NER        Entity F1   NCBI-disease    87.6 (0.6)   87.6 (0.4)    88.3 (0.4)     81.2 (0.4)   83.1 (0.5)     85.6 (0.6)\n                     BC2GM      82.1 (0.3)   82.3 (0.2)    84.0 (0.3)     74.7 (0.4)   77.1 (0.2)     79.7 (0.3)\n                       JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     "
                    },
                    {
                        "context": "The excerpt highlights the effectiveness of the INDUS language model, particularly its tokenizer, INDUSBPE. The INDUSBPE tokenizer demonstrates a significant advantage by producing fewer tokens than the RoBERTa tokenizer, resulting in an approximate 8% reduction in computation cost during training. This efficiency is attributed to INDUSBPE's ability to treat scientific terms as single tokens, unlike RoBERTa, which splits them into multiple subword pieces. The dataset used for pretraining includes diverse scientific sources such as PubMed Central, American Meteorological Society, American Geophysical Union, and NASA's Common Metadata Repository, ensuring a comprehensive vocabulary for scientific applications.\n\n8",
                        "score": 8,
                        "text": "         General Science\n Total                              66.2B 100%                                                    ing dataset (\u00a72)6. For a fair comparison, we set the\n  Table 1: Basic statistics of our pretraining dataset.      vocabulary size to 50, 265, which is equal to that of\n                                                     the RoBERTa tokenizer (Liu et al., 2019) and used\n                                                     the uncased variation of both the tokenizers.\n  \u2022 PubMed Central (PMC)2 : PMC is a full-text    We performed a brief analysis to understand the\n   archive of biomedical and life science journal    differences between the vocabularies of INDUS-\n    literature maintained by National Library of  BPE and the RoBERTa tokenizer. Out of 50, 265\n   Medicine and National Institutes of Health. We    tokens, 22, 355(44.5%) tokens are common in both\n   used the portion of PMC that has a commercial-   the tokenizers while the remaining 27, 910(55.5%)\n   friendly license, along with the PubMed ab-   tokens are included only in either tokenizer, indi-\n    stracts of all the articles in PMC.                  cating a significant distributional shift in domain.\n  \u2022 American Meteorological Society (AMS)3:   To further understand the effect, we applied both\n  We used full-text journal documents spanning   RoBERTa and INDUSBPE on 1, 000 randomly sam-\n   topics in Earth systems, Earth interactions, ap-   pled text fragments from our datasets. These text\n   plied meteorology and climatology, physical   fragments varied from full documents to abstracts\n   oceanography, atmospheric sciences, climate,    to single sentences. As shown in Table 2, INDUS-\n   hydrometeorology, weather and forecasting,  BPE tokenizer produced fewer tokens than the\n   and societal impacts.                      RoBERTa tokenizer, leading to \u02dc8% drop in com-\n  \u2022 American Geophysical Union (AGU)4: The    putation cost during training.\n  AGU dataset included journal documents across      Table 3 compares the RoBERTa tokenzier and IN-\n   the topics of atmospheres,  biogeosciences,  DUSBPE tokenizer, illustrating that the proposed\n   Earth surface, machine learning and compu-   tokenizer treated scientific terms (such as biomak-\n    tation, oceans, planets, solid earth, and space    ers, phosphorylated, alzheimer) as single tokens\n   physics.                                     while RoBERTa tokenizer splits these words into\n  \u2022 NASA  Common  Metadata  Repository    multiple subword pieces.\n   (CMR)5: CMR is a high-performance, high-\n                                                    3.2  Encoder Model    quality, continuously evolving metadata system\n    that catalogs all data and service metadata  We first trained an encoder-only model, INDUSBASE,\n   records for NASA\u2019s Earth Science Data and    using a masked language modeling objective. The\n   Information System (ESDIS).  It contains text   model architecture follows RoBERTaBASE (Liu et al.,\n   descriptions"
                    },
                    {
                        "context": "The excerpt describes Indus as a collection of encoder-based large language models (LLMs) specifically designed for scientific applications. These models are trained on meticulously curated corpora from diverse sources, focusing on domains such as Earth Science, Biomedical Data, Astrophysics, Astronomy, and General Science. The training process involves techniques like masked language modeling, contrastive learning, and knowledge distillation. Indus aims to facilitate research organizations and enterprises by providing efficient access to relevant literature, enhancing capabilities in tasks like open question answering, fact verification, and citation pair analysis. The emphasis on domain-specific corpora and advanced training methodologies highlights the model's strength in scientific applications.\n\n8",
                        "score": 8,
                        "text": "                                                 a collection of encoder-based LLMs focused on\n           Large language models (LLMs) trained on huge    these domains of interest (Figure 1) trained using\n          amounts of data have demonstrated impressive ca-   meticulously curated corpora from diverse sources.\n             pabilities on natural language understanding and  We believe this work will facilitate research organi-\n                                                                zations and enterprises working in these fields by                  Contact: bhatta@us.ibm.com,aashka.trivedi@ibm.com,\n             muthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov   providing efficient access to relevant literature and          Earth Science Data                Pretraining\n                                                                                                            Finetuning                                       Scientific Corpora\n                                   (Masked\n           BioMedical Data              Language                  Indus-Base                              (Contrastive\n                                                                                                              Learning)                              Open QA                                       Modelling)\n\n           Astrophysics Data\n                                                                                                                                                                                 Duplicate Pairs\n\n          Astronomy Data\n                                                               Representation                       Indus-Retriever-Base                                Citation Pairs\n           General Science                                                      Distillation\n                Data\n                                                                                                                                                                              Fact Verification\n           General English\n                Data\n                                                                                                                                                                         Generic Corpora\n                                                                                                     Output\n    Encoder Training Corpus                                     Indus-Small                                   Distillation                     Embedding Training\n                                                                                                                                      Corpus\n\n\n\n             Data                   Teacher (KD)\n\n                 Initialization            Output                                                            Indus-Retriever-\n                                                                                                        Small\n\n\n\n\n  "
                    },
                    {
                        "context": "The excerpt highlights that INDUS is a collection of encoder-based large language models (LLMs) specifically designed for scientific applications across interdisciplinary fields such as physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. INDUS models are trained using domain-specific vocabulary and corpora, and they outperform both general-purpose encoders (e.g., RoBERTa) and existing domain-specific encoders (e.g., SCIBERT) on new scientific benchmark datasets like CLIMATE-CHANGE NER, NASA-QA, and NASA-IR. The models also utilize contrastive learning for text embedding and knowledge distillation to create smaller versions for applications with latency or resource constraints.\n\n9",
                        "score": 9,
                        "text": ": (1) an encoder model[cs.CL]\n                  trained using domain-specific vocabulary and      BIOBERT (Lee et al., 2019), MATBERT (Walker\n                corpora to address natural language under-        et al., 2021), BATTERYBERT (Huang and Cole,\n                 standing tasks, (2) a contrastive-learning-based       2022) and SCHOLARBERT (Hong et al., 2023) were\n                 general text embedding model trained using       developed with the goal of improving accuracy on\n                a diverse set of datasets drawn from multi-       in-domain NLP tasks (Lee et al., 2019; Araci, 2019;\n                 ple sources to address information retrieval                                      Wu et al., 2023).\n                  tasks and (3) smaller versions of these mod-\n                   els created using knowledge distillation tech-                                                                    In this research, we specifically focused on inter-\n                niques to address applications which have la-\n                                                                     disciplinary fields related to the Earth, celestial bod-\n                 tency or resource constraints. We also created\n                                                                             ies, the Sun, and planets within our solar system                  three new scientific benchmark datasets namely,\n                                                          such as physics, Earth science, astrophysics, helio-             CLIMATE-CHANGE NER (entity-recognition),\n             NASA-QA (extractive QA) and NASA-IR (IR) to        physics, planetary sciences and biology. While the\n                  accelerate research in these multi-disciplinary        training corpora of existing domain-specific mod-arXiv:2405.10725v2               fields.    Finally, we show  that our mod-        els such as SCIBERT, BIOBERT and SCHOLARBERT\n                   els outperform both general-purpose encoders        partially cover some of these fields, there is cur-\n              (RoBERTa) and existing domain-specific en-                                                                      rently no specific model available that encompasses\n                coders (SCIBERT) on these new tasks as well\n                                                                                 all of the fields of interest collectively. Further, the\n                 as existing benchmark tasks in the domains of\n                                                                    interdisciplinary nature of these domains of inter-                    interest.\n                                                                         est is reflected in a vast body of literature scattered\n         1  Introduction                                  across diverse sources. Thus, we developed INDUS,\n                                                           a collection of encoder-based LLMs focused on\n     "
                    },
                    {
                        "context": "The excerpt provides evaluation results of the INDUS models on the BLURB benchmark, which focuses on natural language understanding and reasoning tasks in the biomedical domain. INDUS models, specifically INDUS-RETRIEVERBASE and INDUS-RETRIEVERSMALL, were compared against other models like RoBERTaBASE, SCIBERT, MINILM-V2, and TINYBERT. INDUSBASE achieved an F1 score of 64.0 (1.0), outperforming RoBERTa (60.8) and SCIBERT (61.8). INDUSSMALL scored 54.8 (0.8), surpassing MINILM (44.7) and TINYBERT (34.3). The results indicate that INDUS models demonstrate strong performance, particularly in the biomedical domain, as evidenced by their higher F1 scores compared to other models.\n\n8",
                        "score": 8,
                        "text": "  JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     PICO      Macro F1  EBM PICO     72.3 (0.3)   72.4 (0.3)    73.1 (0.2)     67.4 (0.2)   70.3 (0.1)     73.1 (0.2)\n                            ChemProt    50.4 (28.2)   73.9 (0.7)    76.9 (0.5)     56.2 (3.2)   55.9 (2.1)     71.7 (0.9)\n     Relation\n                 Micro F1     DDI        78.6 (1.5)   80.1 (1.0)    81.7 (0.5)     39.3 (5.3)   51.5 (2.9)     69.0 (1.2)    Extraction\n                     GAD        80.0 (1.1)   81.6 (1.2)    79.4 (5.6)     76.4 (1.3)   77.3 (1.0)     81.3 (0.7)\n   Document\n   Classification   Micro F1     HoC        82.2 (0.7)   83.1 (0.6)    83.7 (0.5)     41.6 (6.8)   62.8 (4.7)     80.2 (0.6)\n     Question             PubMedQA     53.1 (3.3)   54.3 (3.8)    58.2 (6.7)     50.3 (1.4)   51.6 (1.7)     56.1 (1.4)\n                 Accuracy\n   Answering              BioASQ      69.1 (4.8)   74.6 (4.5)    69.6 (5.8)     74.3 (3.6)   66.7 (2.3)     75.4 (3.3)\n     Sentence\n     Similarity      Pearson    BIOSSES     79.8 (6.3)   86.3 (3.5)    72.2 (9.5)     88.2 (1.1)   26.6 (8.7)     70.4 (3.3)\n Micro Average         -                 -          75.9 (3.7)   79.2 (1.3)    78.9 (2.4)     67.6 (1.9)   66.1 (1.9)     76.2 (1.0)\n Macro Average         -                 -          74.9 (3.7)   78.2 (1.6)    76.4 (3.2)     65.6 (2.4)   60.6 (3.0)     74.3 (1.3)\n\nTable 5: Evaluation results on BLURB. Results reported are averaged on 10 random seeds with standard deviation in\nparenthesis. Micro average is reported across datasets while macro average is computed by first averaging scores on\neach task (say, task average), followed by averaging the task average across tasks. Results in bold indicate highest\nperformance while underlined results indicate significant difference from second highest result by more than two\nstandard deviations in each model size.\n\n\n  \u2022 INDUS-RETRIEVERBASE  was  compared  to             Model        F1 (SD)\n   BGEBASE15 and a RoBERTaBASE model finetuned             RoBERTa     60.8 (0.8)\n   with the same method presented in \u00a74.1.                  SCIBERT      61.8 (0.7)\n  \u2022 INDUS-RETRIEVERSMALL was compared  to              INDUSBASE    64.0 (1.0)\n   MINILM-V216 and BGESMALL17.                        TINYBERT    34.3 (1.6)\n                                                 MINILM      44.7 (1.3)\n6.1  Natural Language Understanding                     INDUSSMALL   54.8 (0.8)\n    Benchmarks\n                                                     Table 6: CLIMATE-CHANGE NER benchmark results.\nWe evaluated our models on BLURB (Gu et al.,                                                    Standard deviation over 10 random seeds shown in\n2021), a benchmark suite for natural language un-    parenthesis. Results in bold and underline indicate high-\nderstanding and reasoning tasks in the biomedical     est performance and significant difference from second\ndomain. We followed the original work to compute    highest r"
                    },
                    {
                        "context": "The excerpt highlights the strong performance of the INDUS models in various tasks. INDUSBASE significantly outperforms the general-purpose RoBERTa model in micro/macro average scores and shows competitive performance to SCIBERT, a bio-domain-specific model. INDUSSMALL outperforms baselines TINYBERT and MINILM by a large margin in tasks like NER, PICO, relation extraction, and document classification, demonstrating the effectiveness of knowledge distillation from the domain-specific teacher model, INDUSBASE. Additionally, INDUS models excel in the CLIMATE-CHANGE NER task, outperforming baseline models, which suggests the effectiveness of training on large domain-specific data.\n\n8",
                        "score": 8,
                        "text": "and significant difference from second\ndomain. We followed the original work to compute    highest result by more than two standard deviations in\nthe overall score (i.e., macro average).               each model size, respectively.\n  Table 5 shows the evaluation results. Among\nbase models,  INDUSBASE  significantly  outper-                                   We also noticed SCIBERT tends to perform better\nforms the general-purpose RoBERTa model on mi-                                                 than our model on paired input-text tasks, such\ncro/macro average while achieving competitive per-                                                   as QA and semantic similarity tasks, although\nformance to the bio-domain-specific counterpart,                                                    the results have relatively large standard devia-\nSCIBERT.                                                          tions. We hypothesized that the additional next\n  As for smaller models, we noticed INDUSSMALL                                                 sentence prediction objective during training in\noutperformed  the  baselines,  TINYBERT  and                                                  BERT-style models (such as SCIBERT) in contrast to\nMINILM, by a large margin in most cases, show-                                                    the RoBERTa-style models (such as RoBERTaBASE\ning significant difference from second best models                                              and INDUS) may be beneficial for paired input-text\nin NER, PICO, relation extraction, and document                                                         tasks. This trend was consistent with the observa-\nclassification tasks. This demonstrates the effec-                                                        tions of Tinn et al. (2023).\ntiveness of knowledge distillation from our domain-\nspecific teacher model, INDUSBASE.                  6.2  CLIMATE-CHANGE NER\n                                      As shown in Table 6, our models clearly outper-   15https://huggingface.co/BAAI/bge-base-en-v1.5\n   16sentence-transformers/all-MiniLM-L6-v2             formed the corresponding baseline models on the\n   17https://huggingface.co/BAAI/bge-small-en-v1.5        CLIMATE-CHANGE NER task, suggesting the effec-tiveness of training on large domain-specific data.     Model                 NASA-IR \u2191  BEIR Avg. \u2191   Retrieval\n                                                                                                      Time \u2193\n                                                             RoBERTaBASE                0.66         0.37         1.20\n6.3  NASA-QA                                            BGEBASE                     0.67         0.52         1.18\n                                                           INDUS-RETRIEVERBASE       0.71         0.41         1.19\nAs mentioned in \u00a75, we augmented the training     MINILM-V2                 0.62         0.39         0.24\nset with relevant "
                    },
                    {
                        "context": "The excerpt discusses the development and training of the INDUS language model, specifically focusing on the INDUS-RETRIEVERBASE. The model employs a bi-encoder framework and utilizes mean pooling of contextualized transformer representations, which was found to be the most effective strategy. Training involved large batch sizes and in-batch negatives to enhance the contrastive learning objective, using a combination of ADS and S2ORC data. The model was trained on 2 A100 GPUs with an effective batch size of 1,024, initially for 300K steps with unsupervised data, followed by 100K steps with supervised data. The use of an improved contrastive loss with a bidirectional signal is highlighted as a key methodological innovation.\n\n8",
                        "score": 8,
                        "text": "Kwiatkowski et al., 2019), SQuAD (Rajpurkar\nnon-relevant passage.\n                                                              et  al., 2016), SPECTER pairs (Cohan et  al.,\n  Inspired by recent work (Li et al., 2023), we                                                    2020), etc. We included the aforementioned\nused an improved contrastive loss by introducing                                        ADS data and a sample of the S2ORC data in\nan additional bidirectional signal. Specifically, for                                                               this step, to boost domain-specific signals.\na triple {q, p+, P \u2212} of a query, a relevant (posi-   Appendix A contains comprehensive details about\ntive) passage, and a set of non-relevant (negative)\n                                                    the datasets used in training.  For both training\npassages P \u2212= {p\u2212 j }m j=1, We define the InfoNCE    stages, we used large batch sizes and in-batch nega-\nloss(van den Oord et al., 2019) as:\n                                                           tives to better approximate the contrastive learning\n                                                       objective. During training, we sampled batches\n                     n                                                            i )      X                      from each data source proportionately to its size,        =         loges(qi,p+          (1)        LIC  \u22121              n         Zi                  similar to Li et al. (2023).                      i=1Model Specifications We created our sentence   approximately 56M sentences. This masked auto-\nembedding model by fine-tuning INDUSBASE. Here-   encoder model consisted of a full encoder along\nafter, we refer to the resulting retriever model as   with a shallow decoder. The model uses masked\nINDUS-RETRIEVERBASE. We followed a bi-encoder   language modeling with a training objective to re-\nframework (Reimers and Gurevych, 2019), and ex-   cover the original sentence based on the decoder\u2019s\nperimented with multiple pooling strategies and   masked input and the sentence embedding gener-\nfound that the mean pooling of the contextualized    ated from the encoder\u2019s masked input, via masked\ntransformer representations performed the best.      language modelling. There is no distillation loss\n                                                      contributing to this step, which can be viewed as an\nTraining Details We trained each stage on 2\n                                                extended pretraining mechanism. We find that the\nA100 GPUs with an effective batch size of 1,024.\n                                        RetroMAE pretraining does not give us good gains\nWe first trained with unsupervised data for 300K\n                                                       in the larger model but improves the performance\nsteps followed by an additional 100K steps with the\n                                                    of the smaller model.\nsup"
                    }
                ]
            }
        }
    },
    {
        "question": "What are the gaps presented in each paper, and their intersection?",
        "answer": "The paper by Bhattacharjee et al. (2024) identifies gaps in the development and evaluation of language models for scientific applications, particularly in the Earth science domain. The authors highlight the need for models that are specifically tailored to scientific text, as general-purpose models may not perform optimally in this context. They address this by developing the INDUS models, which are pretrained on curated scientific corpora and optimized for tasks such as extractive question answering and named entity recognition. The paper emphasizes the importance of high-quality, domain-specific corpora and the use of knowledge-distilled models to achieve reduced latency while maintaining strong performance (bhattacharjee2024induseffectiveand pages 2-2, 6-7).\n\nIn contrast, the paper by Devlin et al. (2019) discusses gaps related to the training methodologies of BERT compared to other models like GPT. The authors note that BERT's training on both the BooksCorpus and Wikipedia, along with its use of specific embeddings and task-specific fine-tuning, contributes to its superior performance. However, they also acknowledge BERT's instability on small datasets and the need for random restarts to mitigate this issue (devlin2019bertpretrainingof pages 6-6, 14-15).\n\nThe intersection of these gaps lies in the need for specialized training approaches and datasets to enhance model performance in specific domains, whether scientific or general language understanding. Both papers underscore the importance of tailored pretraining and fine-tuning strategies to address domain-specific challenges.",
        "references": "1. (devlin2019bertpretrainingof pages 6-6): Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\n2. (bhattacharjee2024induseffectiveand pages 2-2): Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
        "cost": 0.05057,
        "token_counts": {
            "gpt-4o-2024-08-06": [
                13444,
                1696
            ]
        },
        "used_contexts": [
            "Bhattacharjee et al. (2024",
            "devlin2019bertpretrainingof pages 6-6",
            "Devlin et al. (2019",
            "bhattacharjee2024induseffectiveand pages 2-2",
            "14-15",
            "6-7"
        ],
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "doc_details": {
                    "embedding": null,
                    "docname": "bhattacharjee2024induseffectiveand",
                    "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.",
                    "dockey": "2b647dabce87f56c",
                    "overwrite_fields_from_metadata": true,
                    "key": "bhattacharjee2024induseffectiveand",
                    "bibtex": "@article{bhattacharjee2024induseffectiveand,\n    author = \"Bhattacharjee, Bishwaranjan and Trivedi, Aashka and Muraoka, Masayasu and Ramasubramanian, Muthukumaran and Udagawa, Takuma and Gurung, I. and Zhang, Rong and Dandala, Bharath and Ramachandran, Rahul and Maskey, M. and Bugbee, Kayleen and Little, Mike and Fancher, Elizabeth and Sanders, Lauren and Costes, Sylvain and Blanco-Cuaresma, Sergi and Lockhart, Kelly and Allen, Thomas and Grazes, Felix and Ansdel, Megan and Accomazzi, A. and El-Kurdi, Yousef and Wertheimer, Davis and Pfitzmann, Birgit and Ramis, Cesar Berrospi and Dolfi, Michele and de Lima, Rafael Teixeira and Vegenas, Panos and Mukkavilli, S. K. and Staar, P. and Vahidinia, S. and McGranaghan, Ryan and Mehrabian, A. and Lee, Tsendgar\",\n    title = \"INDUS: Effective and Efficient Language Models for Scientific Applications\",\n    year = \"2024\",\n    journal = \"ArXiv\",\n    volume = \"abs/2405.10725\",\n    month = \"May\",\n    doi = \"10.48550/arxiv.2405.10725\",\n    url = \"https://doi.org/10.48550/arxiv.2405.10725\"\n}\n",
                    "authors": [
                        "Bishwaranjan Bhattacharjee",
                        "Aashka Trivedi",
                        "Masayasu Muraoka",
                        "Muthukumaran Ramasubramanian",
                        "Takuma Udagawa",
                        "I. Gurung",
                        "Rong Zhang",
                        "Bharath Dandala",
                        "Rahul Ramachandran",
                        "M. Maskey",
                        "Kayleen Bugbee",
                        "Mike Little",
                        "Elizabeth Fancher",
                        "Lauren Sanders",
                        "Sylvain Costes",
                        "Sergi Blanco-Cuaresma",
                        "Kelly Lockhart",
                        "Thomas Allen",
                        "Felix Grazes",
                        "Megan Ansdel",
                        "A. Accomazzi",
                        "Yousef El-Kurdi",
                        "Davis Wertheimer",
                        "Birgit Pfitzmann",
                        "Cesar Berrospi Ramis",
                        "Michele Dolfi",
                        "Rafael Teixeira de Lima",
                        "Panos Vegenas",
                        "S. K. Mukkavilli",
                        "P. Staar",
                        "S. Vahidinia",
                        "Ryan McGranaghan",
                        "A. Mehrabian",
                        "Tsendgar Lee"
                    ],
                    "publication_date": "2024-05-17T00:00:00",
                    "year": 2024,
                    "volume": "abs/2405.10725",
                    "issue": null,
                    "issn": null,
                    "pages": null,
                    "journal": "ArXiv",
                    "publisher": null,
                    "url": null,
                    "title": "INDUS: Effective and Efficient Language Models for Scientific Applications",
                    "citation_count": 4,
                    "bibtex_type": "article",
                    "source_quality": -1,
                    "is_retracted": null,
                    "doi": "10.48550/arxiv.2405.10725",
                    "doi_url": "https://doi.org/10.48550/arxiv.2405.10725",
                    "doc_id": "2b647dabce87f56c",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "d50eb618b75ce6f837fd65245393f1b28ab1d911",
                        "externalIds": {
                            "DBLP": "journals/corr/abs-2405-10725",
                            "ArXiv": "2405.10725",
                            "DOI": "10.48550/arXiv.2405.10725",
                            "CorpusId": 269899887
                        },
                        "venue": "arXiv.org",
                        "citationCount": 4,
                        "influentialCitationCount": 0,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle"
                        ],
                        "publicationDate": "2024-05-17",
                        "citationStyles": {
                            "bibtex": "@Article{Bhattacharjee2024INDUSEA,\n author = {Bishwaranjan Bhattacharjee and Aashka Trivedi and Masayasu Muraoka and Muthukumaran Ramasubramanian and Takuma Udagawa and I. Gurung and Rong Zhang and Bharath Dandala and Rahul Ramachandran and M. Maskey and Kayleen Bugbee and Mike Little and Elizabeth Fancher and Lauren Sanders and Sylvain Costes and Sergi Blanco-Cuaresma and Kelly Lockhart and Thomas Allen and Felix Grazes and Megan Ansdel and A. Accomazzi and Yousef El-Kurdi and Davis Wertheimer and Birgit Pfitzmann and Cesar Berrospi Ramis and Michele Dolfi and Rafael Teixeira de Lima and Panos Vegenas and S. K. Mukkavilli and P. Staar and S. Vahidinia and Ryan McGranaghan and A. Mehrabian and Tsendgar Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {INDUS: Effective and Efficient Language Models for Scientific Applications},\n volume = {abs/2405.10725},\n year = {2024}\n}\n"
                        },
                        "matchScore": 201.18681,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt provides details on the training data and evaluation metrics for language models used in scientific applications, specifically focusing on the Indus model. The training data comprises approximately 360 million pairs, categorized into sentence-to-paragraph (s2p), sentence-to-sentence (s2s), and paragraph-to-paragraph (p2p) mappings. The evaluation metrics for various models, including RoBERTaBASE, BGEBASE, INDUS-RETRIEVERBASE, MINILM-V2, BGESMALL, and INDUS-RETRIEVERSMALL, are presented across different datasets like TREC-Covid, NFCorpus, NQ, and others. The performance scores vary, with BGEBASE achieving the highest average score of 0.52, indicating its superior performance compared to others.\n\n7",
                        "score": 7,
                        "text": "  s2p\n PMC [title - abstract] (\u00a72)                              2585537     Title-Body           s2p\n StackExchange Duplicate Questions [title-body - title-body]\u2020  250460      Duplicate Questions  p2p\n StackExchange Duplicate Questions [body - body]\u2020          250519      Duplicate Questions  p2p\n StackExchange Duplicate Questions [title - title]\u2020            304525      Duplicate Questions   s2s\n WikiAnswer Pairs (Fader et al., 2014)                     77427422    Duplicate Questions   s2s\n Specter Pairs (Cohan et al., 2020)                        684100       Citation Pairs         s2s\n S2ORC Citation Pairs [Titles] (Lo et al., 2020)              52603982    Citation Pairs         s2s\n SQuAD (Rajpurkar et al., 2016)                         87599       Question Answers    s2p\n NQ (Kwiatkowski et al., 2019)                          100231      Question Answers    s2p\n SearchQA (Dunn et al., 2017)                           582261      Question Answers    s2p\n StackExchange [title - answer]\u2020                          4067139     Question Answers    s2p\n StackExchange [title-body - answer]\u2020                     187195      Question Answers    p2p\n PAQ (Lewis et al., 2021)                               64371441    Question Answers    s2p\n FEVER (Thorne et al., 2018)\u2217                           109810      Fact Verification      s2p\n HotpotQA (Yang et al., 2018)\u2217                           85000       Question Answering  s2p\n\nTable 10: Training Data for Embedding Models. The training data totals to around 360M pairs. Data Format denotes\ns2p for sentence-to-paragraph mappings, s2s for sentence-to-sentence mappings, and p2p for paragraph-to-paragraph\nmappings.  \u2020Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\n\u2217Only used for Distillation.\n\n\n\n\n\n Model                                                           BEIR Eval\n                    TREC-  NFCorpus  NQ   HotPotQA  FiQA  ArguaAna  Touche  DBPedia  Scidocs  FEVER  Climate   SciFact  AVG.\n                        Covid                                                                FEVER         BEIR\n RoBERTaBASE             0.47     0.30        0.54   0.34        0.38    0.52        0.18     0.25       0.22      0.46      0.14      0.67     0.37\n BGEBASE                  0.78     0.37        0.54   0.73        0.41    0.64        0.26     0.41       0.22      0.86      0.31      0.74     0.52\n INDUS-RETRIEVERBASE    0.56     0.32        0.54   0.49        0.36    0.54        0.17     0.31       0.21      0.56      0.14      0.74     0.41\n MINILM-V2              0.47     0.32        0.44   0.47        0.35    0.50        0.17     0.32       0.22      0.52      0.25      0.65     0.39\n BGESMALL                 0.76     0.34        0.50   0.70        0.40    0.60        0.26     0.40       0.21      0.87      0.32      0.71     0.51\n INDUS-RETRIEVERSMALL   0.55     0.31        0.53   0.48        0.29    0.50        0.21     0.33       0.23      0.61      0.23      0.71"
                    },
                    {
                        "context": "The excerpt discusses the evaluation of the INDUS language models for scientific applications, specifically focusing on extractive question answering in the Earth science domain. The dataset used includes 534 abstracts from the Semantic Scholar Academic Graph, annotated with the IOB tagging scheme. The NASA-QA task involves 39 paragraphs from Earth science papers, with questions formulated by NASA experts. The INDUS models are compared against open-source models like RoBERTaBASE, SCIBERT, MINILM, and TINYBERT. Performance metrics such as Recall@10 and Entity F1 scores are used, with INDUSBASE and INDUSSMALL showing superior results in various tasks.\n\n8",
                        "score": 8,
                        "text": "cts with these annotated paragraphs. On an\ncomplex taxonomies used in climate-related litera-\n                                                   average, each query is 12 words long, and each\nture. This dataset comprises 534 abstracts sourced\n                                                 paragraph is 120 words long. We used Recall@10\nfrom Semantic Scholar Academic Graph (Kinney\n                                                   as evaluation metric since each question has only\net al., 2023), collected using a seed set of climate-\n                                             one relevant document.\nrelated keywords such as wildfire or floods. The\nabstracts were annotated using the IOB (inside, out-                                       6  Experimental Results\nside, beginning) tagging scheme and encompasses\na diverse array of entity types, as shown in Table 4.   Baselines We compared INDUS models against\n                                            open source models of similar sizes:\n5.2  NASA-QA                                                               \u2022 INDUSBASE was compared to RoBERTaBASE11\nWe present NASA-QA9, an extractive question an-      and SCIBERT12.\nswering task focused on the Earth science domain.     \u2022 INDUSSMALL was compared to MINILM  (6-\nFirst, 39 paragraphs from Earth science papers        layer)13 and TINYBERT (4-layer)14.\nwhich appeared in AGU and AMS journals (\u00a72) were\n                                                                 10https://huggingface.co/datasets/nasa-impact/nasa-smd-\nsourced. Subject matter experts from NASA for-    IR-benchmark\nmulated questions and marked the corresponding       11https://huggingface.co/FacebookAI/roberta-base\n                                                                   12https://huggingface.co/allenai/scibert_scivocab_uncased\n   8https://huggingface.co/datasets/ibm/Climate-Change-        13https://huggingface.co/nreimers/MiniLM-L6-H384-\nNER                                                     uncased\n   9https://huggingface.co/datasets/nasa-impact/nasa-smd-        14https://huggingface.co/huawei-\nqa-benchmark                                         noah/TinyBERT_General_4L_312D                                             Base model (125M params.)          Small model (\u223c30M params.)\n      Task         Metric       Dataset     RoBERTa   SCIBERT   INDUSBASE  TINYBERT  MINILM   INDUSSMALL\n                          BC5-chem     90.3 (0.2)   91.4 (0.2)    93.3 (0.2)     84.6 (0.2)   86.1 (0.3)     90.7 (0.1)\n                             BC5-disease    81.5 (0.3)   83.7 (0.3)    85.2 (0.3)     74.0 (0.4)   77.4 (0.3)     81.3 (0.3)\n    NER        Entity F1   NCBI-disease    87.6 (0.6)   87.6 (0.4)    88.3 (0.4)     81.2 (0.4)   83.1 (0.5)     85.6 (0.6)\n                     BC2GM      82.1 (0.3)   82.3 (0.2)    84.0 (0.3)     74.7 (0.4)   77.1 (0.2)     79.7 (0.3)\n                       JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     "
                    },
                    {
                        "context": "The excerpt discusses the training of embedding models using various data sources, with a focus on sentence embedding training data and ablation studies. It highlights the use of approximately 360 million text-pair mappings, categorized as sentence-to-paragraph (s2p), sentence-to-sentence (s2s), and paragraph-to-paragraph (p2p). The ablation study reveals that a one-step distillation process, combining supervised and unsupervised data, outperforms stage-wise distillation by 1 percentage point on datasets like NASA-QA and BEIR. Specific datasets include StackOverflow, StackExchange Math, S2ORC, Wikipedia, Arxiv, NASA ADS, PubMed, and PMC, with varying numbers of pairs and data formats.\n\n7",
                        "score": 7,
                        "text": ".   2022. RetroMAE: Pre-training retrieval-oriented lan-\n  guage models via masked auto-encoder. In Proceed-\n   ings of the 2022 Conference on Empirical Methods in\n  Natural Language Processing, pages 538\u2013548, Abu\n   Dhabi, United Arab Emirates. Association for Com-\n   putational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\n  Muennighoff. 2023.  C-pack: Packaged resources\n   to advance general chinese embedding.\n\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\n  2023. DistillCSE: Distilled contrastive learning for\n   sentence embeddings. In Findings of the Associa-\n   tion for Computational Linguistics: EMNLP 2023,\n  pages 8153\u20138165, Singapore. Association for Com-\n   putational Linguistics.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\n  William Cohen, Ruslan Salakhutdinov, and Christo-\n  pher D. Manning. 2018. HotpotQA: A dataset for\n   diverse, explainable multi-hop question answering.\n   In Proceedings of the 2018 Conference on Empiri-\n   cal Methods in Natural Language Processing, pages\n  2369\u20132380, Brussels, Belgium. Association for Com-\n   putational Linguistics.\n\nA  Sentence Embedding Training Data\n\nTable 10 shows the various data sources used for\ntraining embedding models. All data is presented\nin the form of text-pairs, where each item in the\npair may be a sentence or a paragraph. In the table,\nData Format denotes s2p for sentence-to-paragraph\nmappings, s2s for sentence-to-sentence mappings,\nand p2p for paragraph-to-paragraph mappings. We\nused about 360 million pairs for training and used\nin-batch negatives.\n\nB  Ablation Study: Stage-wise Distillation\n    for Embedding Model\n\nFor the distilled embedding models, we find that\nstage-wise distillation does not benefit performance\nas much as a one-step process, combining all the\nsupervised and unsupervised data. As shown in\nTable 9, the stage-wise approach underperformed\nthe one-stage approach by 1 percentage point for\nboth NASA-QA and on BEIR. Dataset                                        Num. Pairs  Data Category        Data Format\n StackOverflow\u2020                                       18562443    Title-Body           s2p\n StackExchange Math\u2020                                 2201906     Title-Body           s2p\n S2ORC [title - abstract] (Lo et al., 2020)                   41769185    Title-Body           s2p\n S2ORC Citation Pairs [Abstracts] (Lo et al., 2020)           52603982    Title-Body          p2p\n StackExchange [title - body]\u2020                            5415570     Title-Body           s2p\n Wikipedia (Fader et al., 2014)                           6458670     Title-Body           s2p\n Arxiv (Clement et al., 2019)                             2358545     Title-Body           s2p\n NASA ADS [title - abstract] (\u00a72)                         2633240     Title-Body           s2p\n PubMed [title - abstract] (\u00a72)                            24001387    Title-Body           s2p\n PMC [title - abstract] (\u00a72)                              2585537     Title-Body           s2p"
                    },
                    {
                        "context": "The excerpt from  discusses the development of Indus, a language model designed for scientific applications, focusing on its effectiveness and efficiency. The paper includes an ablation study evaluating the model's performance on NASA-QA and BEIR datasets, with specific metrics such as Recall10 for NASA-QA and nDCG10 for BEIR. The results show that the INDUS-RETRIEVERSMALL model achieved a Recall10 of 0.73 in a one-stage training and 0.72 in stagewise training, while nDCG10 scores were 0.42 and 0.41, respectively. These metrics highlight the model's retrieval capabilities and potential areas for improvement.\n\n7",
                        "score": 7,
                        "text": "sa, Jordi Huguet, Paula Marne,     Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexan-\n  Tania Mench\u00f3n, Gr\u00e9gory Operto, Albina Polo, San-      der Dunn, Kristin Persson, Gerbrand Ceder, and\n   dra Pradas, Anna Soteras, Marc Vilanova, and Na-     Anubhav Jain. 2021. The impact of domain-specific\n   talia Vilor-Tejedor. 2020.  Novel tau biomarkers       pre-training on named entity recognition tasks in ma-\n  phosphorylated at t181, t217 or t231 rise in the ini-        terials science. Available at SSRN 3950755.\n   tial stages of the preclinical alzheimer&#x2019;s\n  <i>continuum</i> when only subtle changes in    Liang Wang, Nan Yang, Xiaolong Huang, Binxing\n  a&#x3b2; pathology are detected. EMBO Molec-       Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\n   ular Medicine, 12(12):e12921.                       and Furu Wei. 2022. Text embeddings by weakly-\n                                                          supervised contrastive pre-training.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\n  hishek Srivastava, and Iryna Gurevych. 2021. Beir:   Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\n A heterogenous benchmark for zero-shot evaluation      and Furu Wei. 2021. MiniLMv2: Multi-head self-\n   of information retrieval models.                           attention relation distillation for compressing pre-\n                                                              trained transformers. In Findings of the Association\nJames   Thorne,   Andreas   Vlachos,    Christos       for Computational Linguistics: ACL-IJCNLP 2021,\n   Christodoulopoulos,  and  Arpit  Mittal.  2018.      pages 2140\u20132151, Online. Association for Computa-\n  FEVER: a large-scale dataset for fact extraction       tional Linguistics.Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and     Model                    Training    NASA-IR  BEIR Avg.\n  Tie-Yan Liu. 2013. A theoretical analysis of ndcg     INDUS-RETRIEVERSMALL  One-Stage     0.73        0.42\n   type ranking measures. In Proceedings of the 26th     INDUS-RETRIEVERSMALL   Stagewise     0.72        0.41\n  Annual Conference on Learning Theory, volume 30\n                                                     Table 9: Ablation Study: Evaluation results on NASA-   of Proceedings of Machine Learning Research, pages\n  25\u201354, Princeton, NJ, USA. PMLR.              QA and BEIR. NASA-QA showed Recall10 while BEIR\n                                                         reported nDCG10.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\n  Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\n   badur, David Rosenberg, and Gideon Mann. 2023.  C  Complete Results on BEIR Benchmark\n  Bloomberggpt: A large language model for finance.\n                                                  Table 11 shows the per-dataset results on the BEIR\nShitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.\n                                                          tasks.   2022. RetroMAE: Pre-training retrieval-oriented lan-\n  guage models via masked auto-encoder. In "
                    },
                    {
                        "context": "Not applicable\n\n1",
                        "score": 0,
                        "text": "     Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick      Pengjun Xie, and Meishan Zhang. 2023. Towards\n  Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and       general text embeddings with multi-stage contrastive\n  Wen-tau Yih. 2020. Dense passage retrieval for open-       learning.\n  domain question answering. In Proceedings of the\n  2020 Conference on Empirical Methods in Natural    Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\n  Language Processing (EMNLP), pages 6769\u20136781,      dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n   Online. Association for Computational Linguistics.      Luke Zettlemoyer, and Veselin Stoyanov. 2019.\n                                                        Roberta: A robustly optimized bert pretraining ap-\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron       proach. arXiv preprint arXiv:1907.11692.Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-     and VERification.   In Proceedings of the 2018\n   ney, and Daniel Weld. 2020. S2ORC: The semantic      Conference  of the North American Chapter  of\n   scholar open research corpus. In Proceedings of the       the  Association  for  Computational  Linguistics:\n  58th Annual Meeting of the Association for Compu-    Human Language Technologies, Volume 1 (Long\n   tational Linguistics, pages 4969\u20134983, Online. Asso-      Papers), pages 809\u2013819, New Orleans, Louisiana.\n   ciation for Computational Linguistics.                   Association for Computational Linguistics.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan,   Robert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xi-\n  Dario Amodei, and Ilya Sutskever. 2019. Language      aodong Liu, Tristan Naumann, Jianfeng Gao, and\n  models are unsupervised multitask learners.             Hoifung Poon. 2023. Fine-tuning large neural lan-\n                                                   guage models for biomedical natural language pro-\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-       cessing. Patterns, 4(4).\n   ine Lee, Sharan Narang, Michael Matena, Yanqi\n  Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the   Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n   limits of transfer learning with a unified text-to-text       Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\n   transformer. Journal of Machine Learning Research,       Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\n   21(1).                                                 Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\n                                                       Grave, and Guillaume Lample. 2023. Llama: Open\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.                                                   and efficient foundation language models.\n  Know what you don\u2019t know: Unanswerable questions\n   for squad. CoRR, abs/1806.03822.                                              Aashka Trivedi, Takuma Udagawa, Michele Merler,\n                                              Rameswar Panda, Y"
                    },
                    {
                        "context": "The excerpt provides evaluation results of the INDUS language models on the BLURB benchmark, which focuses on natural language understanding and reasoning tasks in the biomedical domain. The results are presented in terms of F1 scores and standard deviations across various tasks such as JNLPBA, PICO, ChemProt, DDI, GAD, HoC, PubMedQA, BioASQ, and BIOSSES. The INDUS models, including INDUS-RETRIEVERBASE and INDUS-RETRIEVERSMALL, are compared against other models like RoBERTaBASE, SCIBERT, MINILM, and TINYBERT. The excerpt highlights the performance of INDUS models, with INDUSBASE achieving an F1 score of 64.0 (1.0) and INDUSSMALL scoring 54.8 (0.8). The results indicate areas where INDUS models outperform or underperform compared to other models, which can be considered gaps in performance.\n\n7",
                        "score": 7,
                        "text": "  JNLPBA      79.1 (0.2)   78.2 (0.2)    80.3 (0.2)     70.3 (0.2)   73.4 (0.3)     75.7 (0.2)\n     PICO      Macro F1  EBM PICO     72.3 (0.3)   72.4 (0.3)    73.1 (0.2)     67.4 (0.2)   70.3 (0.1)     73.1 (0.2)\n                            ChemProt    50.4 (28.2)   73.9 (0.7)    76.9 (0.5)     56.2 (3.2)   55.9 (2.1)     71.7 (0.9)\n     Relation\n                 Micro F1     DDI        78.6 (1.5)   80.1 (1.0)    81.7 (0.5)     39.3 (5.3)   51.5 (2.9)     69.0 (1.2)    Extraction\n                     GAD        80.0 (1.1)   81.6 (1.2)    79.4 (5.6)     76.4 (1.3)   77.3 (1.0)     81.3 (0.7)\n   Document\n   Classification   Micro F1     HoC        82.2 (0.7)   83.1 (0.6)    83.7 (0.5)     41.6 (6.8)   62.8 (4.7)     80.2 (0.6)\n     Question             PubMedQA     53.1 (3.3)   54.3 (3.8)    58.2 (6.7)     50.3 (1.4)   51.6 (1.7)     56.1 (1.4)\n                 Accuracy\n   Answering              BioASQ      69.1 (4.8)   74.6 (4.5)    69.6 (5.8)     74.3 (3.6)   66.7 (2.3)     75.4 (3.3)\n     Sentence\n     Similarity      Pearson    BIOSSES     79.8 (6.3)   86.3 (3.5)    72.2 (9.5)     88.2 (1.1)   26.6 (8.7)     70.4 (3.3)\n Micro Average         -                 -          75.9 (3.7)   79.2 (1.3)    78.9 (2.4)     67.6 (1.9)   66.1 (1.9)     76.2 (1.0)\n Macro Average         -                 -          74.9 (3.7)   78.2 (1.6)    76.4 (3.2)     65.6 (2.4)   60.6 (3.0)     74.3 (1.3)\n\nTable 5: Evaluation results on BLURB. Results reported are averaged on 10 random seeds with standard deviation in\nparenthesis. Micro average is reported across datasets while macro average is computed by first averaging scores on\neach task (say, task average), followed by averaging the task average across tasks. Results in bold indicate highest\nperformance while underlined results indicate significant difference from second highest result by more than two\nstandard deviations in each model size.\n\n\n  \u2022 INDUS-RETRIEVERBASE  was  compared  to             Model        F1 (SD)\n   BGEBASE15 and a RoBERTaBASE model finetuned             RoBERTa     60.8 (0.8)\n   with the same method presented in \u00a74.1.                  SCIBERT      61.8 (0.7)\n  \u2022 INDUS-RETRIEVERSMALL was compared  to              INDUSBASE    64.0 (1.0)\n   MINILM-V216 and BGESMALL17.                        TINYBERT    34.3 (1.6)\n                                                 MINILM      44.7 (1.3)\n6.1  Natural Language Understanding                     INDUSSMALL   54.8 (0.8)\n    Benchmarks\n                                                     Table 6: CLIMATE-CHANGE NER benchmark results.\nWe evaluated our models on BLURB (Gu et al.,                                                    Standard deviation over 10 random seeds shown in\n2021), a benchmark suite for natural language un-    parenthesis. Results in bold and underline indicate high-\nderstanding and reasoning tasks in the biomedical     est performance and significant difference from second\ndomain. We followed the original work to compute    highest r"
                    },
                    {
                        "context": "The excerpt discusses the development of INDUS, a set of language models tailored for scientific applications. Key contributions include the creation of INDUSBPE, a tokenizer optimized for scientific text, and the pretraining of encoder-only large language models (LLMs) using curated scientific corpora. These models were further refined to produce sentence embeddings through contrastive learning. The paper highlights the importance of high-quality, domain-specific corpora for model performance, noting that knowledge-distilled models achieved reduced latency while maintaining strong empirical performance. Benchmarks used for evaluation include NASA-QA, CLIMATE-CHANGE NER, and NASA-IR, among others.\n\n8",
                        "score": 8,
                        "text": "                                                                                        Small\n\n\n\n\n        BC5-CHEM       BC5-Disease      NCBI-Disease       BC2GM\n                                                                     NASA-QA              TREC-Covid       NFCorpus        NQ           HotpotQA\n\n         JNLPBA         EBM-PICO        ChemProt           DDI\n                                                                                       Climate                 FiQA           Arguana         Touche         DBPedia        NASA-IR\n         GAD            HoC         PubMedQA         BioASQ         Change NER\n                                                                                                             SciDocs         FEVER           Climate FEVER            SciFact\n         BIOSSES         BLURB Benchmark                                                          BEIR Benchmark\n\n                             Natural Language Understanding Benchmarks                                     Information Retrieval Benchmarks\n\n\nFigure 1: Overview of INDUS models: the general-purpose encoder model and the retriever built from it, and their\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\nNASA-QA, CLIMATE-CHANGE NER and NASA-IR.\n\n\nenabling them in informed decision-making.            (Beltagy et al., 2019). We also show that the\n   Specifically, we make the following contribu-      knowledge-distilled models achieved a signifi-\ntions:                                                 cant boost in latency while maintaining strong\n1. Utilizing the byte-pair encoding algorithm,       empirical performance compared to the original\n  we constructed INDUSBPE, a customized to-      models on most of the benchmark tasks.\n   kenizer from the curated scientific corpus.\n                                       2  Data2. We pretrained multiple encoder-only LLMs us-\n   ing the curated scientific corpora and the IN-                                                        Sufficient high-quality in-domain corpora is essen-\n  DUSBPE tokenizer (\u00a72, \u00a73). We further created                                                                  tial to develop models that perform better than\n   sentence-embedding models by fine-tuning the                                                           their counterparts trained on open-domain corpora.\n   encoder-only models with a contrastive learn-                                 We meticulously identified corpora for each of the\n   ing objective to learn \u201cuniversal\u201d sentence em-                                                 aforementioned domains, and created English-only\n   beddings (Gao et al., 2021) (\u00a74). We also                                            models for the sake of containment. Specifically,\n   trained smaller, more efficient versions of                                                        for each of the domains, we used open-source dat"
                    }
                ]
            },
            "devlin2019bertpretrainingof": {
                "doc_details": {
                    "embedding": null,
                    "docname": "devlin2019bertpretrainingof",
                    "citation": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.",
                    "dockey": "1bd917d538893069",
                    "overwrite_fields_from_metadata": true,
                    "key": "devlin2019bertpretrainingof",
                    "bibtex": "@article{devlin2019bertpretrainingof,\n    author = \"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina\",\n    booktitle = \"North American Chapter of the Association for Computational Linguistics\",\n    pages = \"4171-4186\",\n    title = \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n    year = \"2019\",\n    journal = \"Unknown journal\",\n    doi = \"10.18653/v1/n19-1423\",\n    url = \"https://doi.org/10.18653/v1/n19-1423\"\n}\n",
                    "authors": [
                        "Jacob Devlin",
                        "Ming-Wei Chang",
                        "Kenton Lee",
                        "Kristina Toutanova"
                    ],
                    "publication_date": null,
                    "year": 2019,
                    "volume": null,
                    "issue": null,
                    "issn": null,
                    "pages": "4171-4186",
                    "journal": null,
                    "publisher": null,
                    "url": null,
                    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                    "citation_count": 82386,
                    "bibtex_type": "article",
                    "source_quality": null,
                    "is_retracted": null,
                    "doi": "10.18653/v1/n19-1423",
                    "doi_url": "https://doi.org/10.18653/v1/n19-1423",
                    "doc_id": "1bd917d538893069",
                    "file_location": null,
                    "license": null,
                    "pdf_url": null,
                    "other": {
                        "bibtex_source": [
                            "self_generated"
                        ],
                        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
                        "externalIds": {
                            "MAG": "2951055169",
                            "ACL": "N19-1423",
                            "DBLP": "journals/corr/abs-1810-04805",
                            "ArXiv": "1810.04805",
                            "DOI": "10.18653/v1/N19-1423",
                            "CorpusId": 52967399
                        },
                        "venue": "North American Chapter of the Association for Computational Linguistics",
                        "citationCount": 82386,
                        "influentialCitationCount": 19150,
                        "isOpenAccess": false,
                        "openAccessPdf": null,
                        "publicationTypes": [
                            "JournalArticle",
                            "Conference"
                        ],
                        "publicationDate": null,
                        "citationStyles": {
                            "bibtex": "@Article{Devlin2019BERTPO,\n author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {4171-4186},\n title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n year = {2019}\n}\n"
                        },
                        "matchScore": 241.06143,
                        "client_source": [
                            "semantic_scholar"
                        ]
                    }
                },
                "contexts": [
                    {
                        "context": "The excerpt discusses the fine-tuning process of BERT for GLUE tasks, highlighting the exclusion of the WNLI set and the use of specific metrics: F1 scores for QQP and MRPC, Spearman correlations for STS-B, and accuracy for other tasks. BERT's fine-tuning involves a batch size of 32 and 3 epochs, with learning rates selected from 5e-5 to 2e-5. BERTLARGE's instability on small datasets is addressed with random restarts. BERTBASE and BERTLARGE outperform previous systems, with BERT achieving a 4.6% accuracy improvement on MNLI. The architecture of BERTBASE and OpenAI GPT is similar, differing mainly in attention masking.\n\n8",
                        "score": 8,
                        "text": "e, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\n\n\n  We use a batch size of 32 and \ufb01ne-tune for 3   Wikipedia containing the answer, the task is to\nepochs over the data for all GLUE tasks. For each    predict the answer text span in the passage.\ntask, we selected the best \ufb01ne-tuning learning rate     As shown in Figure 1, in the question answer-\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.    ing task, we represent the input question and pas-\nAdditionally, for BERTLARGE we found that \ufb01ne-    sage as a single packed sequence, with the ques-\ntuning was sometimes unstable on small datasets,    tion using the A embedding and the passage using\nso we ran several random restarts and selected the    the B embedding. We only introduce a start vec-\nbest model on the Dev set. With random restarts,    tor S \u2208RH and an end vector E \u2208RH during\nwe use the same pre-trained checkpoint but per-    \ufb01ne-tuning. The probability of word i being the\nform different \ufb01ne-tuning data shuf\ufb02ing and clas-     start of the answer span is computed as a dot prod-\nsi\ufb01er layer initialization.9                            uct between Ti and S followed by a softmax over\n                                                                                                                                                                     .  Results  are  presented  in  Table  1.    Both     all of the words in the paragraph: Pi = P eS\u00b7Ti                                                                                                                                           j eS\u00b7TjBERTBASE and BERTLARGE outperform all sys-                                          The analogous formula is used for the end of the\ntems on all tasks by a substantial margin, obtaining\n                                              answer span. The score of a candidate span from\n4.5% and 7.0% respective average accuracy im-\n                                                      position i to position j is de\ufb01ned as S\u00b7Ti + E\u00b7Tj,provement over the prior state of the art. Note that\n                                             and the maximum scoring span where j \u2265i isBERTBASE and OpenAI GPT are nearly identical                                               used as a prediction. The training objective is the\nin terms of model architecture apart from the at-\n                                       sum of the log-likelihoods of the correct start and\ntention masking. For the largest and most widely\n                                             end positions. We \ufb01ne-tune for 3 epochs with a\nreported GLUE task, MNLI, BERT obtains a 4.6%\n                                                     learning rate of 5e-5 and a batch size of 32.\nabsolute accuracy improvement. On the "
                    },
                    {
                        "context": "The excerpt highlights differences in training methodologies between BERT and GPT, which may contribute to performance gaps. BERT is trained on both the BooksCorpus (800M words) and Wikipedia (2,500M words), while GPT uses only the BooksCorpus. BERT incorporates [SEP], [CLS], and sentence A/B embeddings during pre-training, unlike GPT, which introduces these at fine-tuning. BERT is trained for 1M steps with a batch size of 128,000 words, compared to GPT's 32,000 words. Additionally, BERT uses a task-specific fine-tuning learning rate, whereas GPT uses a uniform rate of 5e-5. These differences may account for empirical improvements noted in BERT.\n\n8",
                        "score": 8,
                        "text": "                      openai.com/language-unsupervised.\nwork is that the bi-directionality and the two pre-\n                                          The GLUE benchmark includes the following\ntraining tasks presented in Section 3.1 account for\n                                                         datasets, the descriptions of which were originally\nthe majority of the empirical improvements, but\n                                            summarized in Wang et al. (2018a):\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:      MNLI  Multi-Genre Natural Language Inference\n                                                                 is a large-scale, crowdsourced entailment classi\ufb01-\n   \u2022 GPT is trained on the BooksCorpus (800M    cation task (Williams et al., 2018). Given a pair of\n     words); BERT is trained on the BooksCor-    sentences, the goal is to predict whether the sec-\n    pus (800M words) and Wikipedia (2,500M   ond sentence is an entailment, contradiction, or\n     words).                                        neutral with respect to the \ufb01rst one.\n\n   \u2022 GPT uses a sentence separator ([SEP]) and  QQP  Quora Question Pairs is a binary classi\ufb01-\n     classi\ufb01er token ([CLS]) which are only in-    cation task where the goal is to determine if two\n     troduced at \ufb01ne-tuning time; BERT learns    questions asked on Quora are semantically equiv-\n    [SEP], [CLS] and sentence A/B embed-    alent (Chen et al., 2018).\n     dings during pre-training.\n                                  QNLI  Question Natural Language Inference is\n                                                 a version of the Stanford Question Answering   \u2022 GPT was trained for 1M steps with a batch\n                                                  Dataset (Rajpurkar et al., 2016) which has been     size of 32,000 words; BERT was trained for\n                                                  converted to a binary classi\ufb01cation task (Wang   1M steps with a batch size of 128,000 words.\n                                                            et al., 2018a). The positive examples are (ques-\n   \u2022 GPT used the same learning rate of 5e-5 for    tion, sentence) pairs which do contain the correct\n      all \ufb01ne-tuning experiments; BERT chooses a    answer, and the negative examples are (question,\n     task-speci\ufb01c \ufb01ne-tuning learning rate which    sentence) from the same paragraph which do not\n    performs the best on the development set.       contain the answer.             Class                                                            Class\n             Label                                                              Label\n\n             C     T1     ...      TN     T[SEP]     T1\u2019     ...      TM\u2019                 C       T1        T2                ...            TN\n\n\n               BERT                        BERT\n\n\n                           E[CLS]    E1     ...      EN      E[SEP]     E1\u2019     ...    EM\u2019                              E["
                    },
                    {
                        "context": "The excerpt discusses BERT's use of self-attention to unify bidirectional cross-attention between sentence pairs, enhancing tasks like paraphrasing, entailment, and question answering. It introduces new parameters during fine-tuning, specifically classification layer weights \\( W \\in \\mathbb{R}^{K \\times H} \\), where \\( K \\) is the number of labels. BERT's performance on the GLUE benchmark is highlighted, with BERTBASE achieving an average score of 79.6 and BERTLARGE scoring 82.1, surpassing previous models like OpenAI GPT. The excerpt does not explicitly mention gaps in the research, focusing instead on BERT's methodology and performance metrics.\n\n5",
                        "score": 5,
                        "text": "e input\nfore applying bidirectional cross attention, such\n                                               sequence (for single sentence or sentence pairs)\nas Parikh et al. (2016); Seo et al. (2017). BERT\n                                                    as described in Section 3, and use the \ufb01nal hid-\ninstead uses the self-attention mechanism to unify\n                                             den vector C \u2208RH corresponding to the \ufb01rstthese two stages, as encoding a concatenated text                                                    input token ([CLS]) as the aggregate representa-\npair with self-attention effectively includes bidi-\n                                                            tion. The only new parameters introduced during\nrectional cross attention between two sentences.\n                                                 \ufb01ne-tuning are classi\ufb01cation layer weights W                                                             \u2208  For each task, we simply plug in the task-                                    RK\u00d7H, where K is the number of labels. We com-\nspeci\ufb01c inputs and outputs into BERT and \ufb01ne-                                                  pute a standard classi\ufb01cation loss with C and W,\ntune all the parameters end-to-end.  At the in-                        T                                                                         i.e., log(softmax(CW  )).\nput, sentence A and sentence B from pre-training\n                                                               7For example, the BERT SQuAD model can be trained inare analogous to (1) sentence pairs in paraphras-\n                                                         around 30 minutes on a single Cloud TPU to achieve a Dev\ning, (2) hypothesis-premise pairs in entailment, (3)    F1 score of 91.0%.\nquestion-passage pairs in question answering, and       8See (10) in https://gluebenchmark.com/faq. System             MNLI-(m/mm)   QQP   QNLI   SST-2   CoLA   STS-B  MRPC   RTE   Average\n                         392k        363k    108k     67k      8.5k      5.7k      3.5k     2.5k          -\n Pre-OpenAI SOTA         80.6/80.1       66.1     82.3     93.2      35.0      81.0      86.0     61.7      74.0\n BiLSTM+ELMo+Attn     76.4/76.1       64.8     79.8     90.4      36.0      73.3      84.9     56.8      71.0\n OpenAI GPT              82.1/81.4       70.3     87.4     91.3      45.4      80.0      82.3     56.0      75.1\n BERTBASE                 84.6/83.4       71.2     90.5     93.5      52.1      85.8      88.9     66.4      79.6\n BERTLARGE               86.7/85.9       72.1     92.7     94.9      60.5      86.5      89.3     70.1      82.1\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\nThe number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different\nthan the of\ufb01cial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-\nmodel, single task. F"
                    }
                ]
            }
        }
    }
]
