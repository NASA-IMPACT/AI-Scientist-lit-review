[
    {
        "question": "What datasets and benchmarks have been specifically developed for evaluating NLP models in scientific domains?",
        "answer": "The development of specialized datasets and benchmarks for evaluating NLP models in scientific domains has been a focus of recent research efforts. Notably, three new scientific benchmark datasets have been introduced: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR. These datasets are designed for tasks such as entity recognition, extractive question answering, and information retrieval, respectively. They aim to advance research in scientific NLP applications across interdisciplinary fields, including physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology (bhattacharjee2024induseffectiveand pages 1-1; bhattacharjee2024induseffectiveand pages 2-3).\n\nThe CLIMATE-CHANGE NER dataset focuses on named entity recognition, while NASA-QA is tailored for extractive question answering, and NASA-IR is used for information retrieval tasks. These datasets were created using open-source data with permissive licenses, supplemented by specific data from NASA and its providers, and include English Wikipedia for general English learning (bhattacharjee2024induseffectiveand pages 2-3).\n\nAdditionally, the BEIR Benchmark, which consists of 12 retrieval tasks across various domains, is used to evaluate NLP models. It employs the Normalized Cumulative Discount Gain (nDCG@10) as its primary metric, providing a comprehensive evaluation framework for domain-specific sentence embedding models (bhattacharjee2024induseffectiveand pages 8-9).",
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "context": "bhattacharjee2024induseffectiveand pages 2-2: The excerpt highlights the development of the INDUS models, which include a general-purpose encoder model and a retriever, along with their distilled versions. These models are evaluated using various benchmarks, specifically mentioning new benchmarks such as NASA-QA, CLIMATE-CHANGE NER, and NASA-IR. Additionally, the excerpt lists several other benchmarks used for evaluation, including BC5-CHEM, BC5-Disease, NCBI-Disease, BC2GM, TREC-Covid, NFCorpus, NQ, HotpotQA, JNLPBA, EBM-PICO, ChemProt, DDI, GAD, HoC, PubMedQA, BioASQ, FiQA, Arguana, Touche, DBPedia, SciDocs, FEVER, Climate FEVER, and SciFact. These benchmarks are categorized under Natural Language Understanding and Information Retrieval.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt describes the creation of three new scientific benchmark datasets: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR. These datasets are designed for entity recognition, extractive question answering, and retrieval tasks, respectively, to advance research in scientific NLP applications. The models were trained using open-source data with permissive licenses, augmented with specific data from NASA and its providers, and included English Wikipedia for general English learning. The SAO/NASA Astrophysics Data System (ADS) is highlighted as a significant data source, covering publications in astronomy, astrophysics, physics, and general science, including all arXiv e-prints.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt discusses the development of three new scientific benchmark datasets: CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA), and NASA-IR (information retrieval). These datasets are designed to accelerate research in interdisciplinary fields such as physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. The models developed, including INDUS, outperform both general-purpose encoders like RoBERTa and existing domain-specific encoders like SCIBERT on these new tasks and existing benchmark tasks. The focus is on creating models that address natural language understanding and information retrieval tasks using domain-specific vocabulary and corpora.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 8-9: The excerpt discusses the use of the BEIR Benchmark, which consists of 12 retrieval tasks across various domains, for evaluating NLP models. The BEIR benchmark employs the Normalized Cumulative Discount Gain (nDCG@10) as its primary metric. The performance of domain-specific sentence embedding models is compared against baselines, with significant improvements noted on the NASA-IR task and several BEIR tasks. Additionally, the INDUS-RETRIEVERSMALL model is highlighted for its superior performance and speed, being approximately 4.6 times faster than the INDUS-RETRIEVERBASE model on both NASA-IR and BEIR tasks.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 5-6: The excerpt discusses the creation of benchmark datasets to evaluate NLP models in scientific domains, highlighting the absence of tailored datasets for diverse and multidisciplinary fields. To address this, the authors introduced three new datasets for specific tasks: Named Entity Recognition (NER), Question Answering (QA), and Information Retrieval (IR). The QA dataset includes 29 paragraphs with 145 QA pairs for training and 10 paragraphs with 50 questions for evaluation. Additionally, the training set was augmented with Earth science-related paragraphs and QA pairs from the SQuAD dataset, resulting in a pruned set of 686 paragraphs with 5,081 questions.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 2-2, bhattacharjee2024induseffectiveand pages 2-3, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 8-9, bhattacharjee2024induseffectiveand pages 5-6",
                "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725."
            },
            "liu2019robertaarobustly": {
                "context": "bhattacharjee2024induseffectiveand pages 2-2: The excerpt highlights the development of the INDUS models, which include a general-purpose encoder model and a retriever, along with their distilled versions. These models are evaluated using various benchmarks, specifically mentioning new benchmarks such as NASA-QA, CLIMATE-CHANGE NER, and NASA-IR. Additionally, the excerpt lists several other benchmarks used for evaluation, including BC5-CHEM, BC5-Disease, NCBI-Disease, BC2GM, TREC-Covid, NFCorpus, NQ, HotpotQA, JNLPBA, EBM-PICO, ChemProt, DDI, GAD, HoC, PubMedQA, BioASQ, FiQA, Arguana, Touche, DBPedia, SciDocs, FEVER, Climate FEVER, and SciFact. These benchmarks are categorized under Natural Language Understanding and Information Retrieval.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt describes the creation of three new scientific benchmark datasets: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR. These datasets are designed for entity recognition, extractive question answering, and retrieval tasks, respectively, to advance research in scientific NLP applications. The models were trained using open-source data with permissive licenses, augmented with specific data from NASA and its providers, and included English Wikipedia for general English learning. The SAO/NASA Astrophysics Data System (ADS) is highlighted as a significant data source, covering publications in astronomy, astrophysics, physics, and general science, including all arXiv e-prints.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt discusses the development of three new scientific benchmark datasets: CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA), and NASA-IR (information retrieval). These datasets are designed to accelerate research in interdisciplinary fields such as physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. The models developed, including INDUS, outperform both general-purpose encoders like RoBERTa and existing domain-specific encoders like SCIBERT on these new tasks and existing benchmark tasks. The focus is on creating models that address natural language understanding and information retrieval tasks using domain-specific vocabulary and corpora.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 8-9: The excerpt discusses the use of the BEIR Benchmark, which consists of 12 retrieval tasks across various domains, for evaluating NLP models. The BEIR benchmark employs the Normalized Cumulative Discount Gain (nDCG@10) as its primary metric. The performance of domain-specific sentence embedding models is compared against baselines, with significant improvements noted on the NASA-IR task and several BEIR tasks. Additionally, the INDUS-RETRIEVERSMALL model is highlighted for its superior performance and speed, being approximately 4.6 times faster than the INDUS-RETRIEVERBASE model on both NASA-IR and BEIR tasks.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 5-6: The excerpt discusses the creation of benchmark datasets to evaluate NLP models in scientific domains, highlighting the absence of tailored datasets for diverse and multidisciplinary fields. To address this, the authors introduced three new datasets for specific tasks: Named Entity Recognition (NER), Question Answering (QA), and Information Retrieval (IR). The QA dataset includes 29 paragraphs with 145 QA pairs for training and 10 paragraphs with 50 questions for evaluation. Additionally, the training set was augmented with Earth science-related paragraphs and QA pairs from the SQuAD dataset, resulting in a pruned set of 686 paragraphs with 5,081 questions.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 2-2, bhattacharjee2024induseffectiveand pages 2-3, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 8-9, bhattacharjee2024induseffectiveand pages 5-6",
                "citation": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, Jul 2019."
            }
        }
    },
    {
        "question": "How are domain-specific language models fine-tuned and evaluated for tasks in scientific literature and what metrics are used to measure their effectiveness?",
        "answer": "Domain-specific language models for scientific literature are fine-tuned and evaluated using specialized techniques and metrics to ensure their effectiveness in handling scientific tasks. The INDUS models, for instance, are fine-tuned using a byte-pair encoding algorithm to create a customized tokenizer, INDUSBPE, from a curated scientific corpus. These models are pretrained with a contrastive learning objective to produce sentence embeddings, which are crucial for tasks like information retrieval (bhattacharjee2024induseffectiveand pages 2-2).\n\nThe evaluation of these models is conducted on various scientific benchmarks, including NASA-QA, CLIMATE-CHANGE NER, and NASA-IR. The models are assessed using metrics such as the F1 score, Recall@10, and Normalized Cumulative Discount Gain (nDCG@10). For instance, on the NASA-QA benchmark, the INDUSBASE model achieved an F1 score of 68.2, outperforming other models like RoBERTa and SCIBERT (bhattacharjee2024induseffectiveand pages 8-8). In information retrieval tasks, the models are evaluated using the BEIR Benchmark, which includes 12 retrieval tasks across various domains, with nDCG@10 as a primary metric (bhattacharjee2024induseffectiveand pages 8-9).\n\nThese models demonstrate significant improvements in latency and performance, highlighting the importance of high-quality in-domain corpora and custom tokenizers for model development (bhattacharjee2024induseffectiveand pages 2-2).",
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "context": "bhattacharjee2024induseffectiveand pages 8-9: The excerpt discusses the evaluation of domain-specific language models, specifically focusing on sentence embedding models for scientific text. These models were tested using the BEIR Benchmark, which includes 12 retrieval tasks across various domains. The primary metric used was the Normalized Cumulative Discount Gain (nDCG@10). The models significantly outperformed baselines on the NASA-IR task and maintained good performance on several BEIR tasks. Additionally, the INDUS-RETRIEVERSMALL model was noted for being about 4.6 times faster than INDUS-RETRIEVERBASE while achieving better performance on both NASA-IR and BEIR tasks.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt describes the creation and evaluation of domain-specific language models using knowledge-distillation techniques. Three new scientific benchmark datasets were developed: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, to enhance research in scientific applications. The models were trained on open-source data with permissive licenses, augmented with NASA-specific data, and included English Wikipedia for general language learning. The models demonstrated strong performance on these benchmarks, surpassing general-purpose models like RoBERTa and scientific-domain encoders like SCIBERT. The datasets cover various domains, with a total of 66.2 billion tokens, and the models' effectiveness is shown through experimental results.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt discusses the development and evaluation of the INDUS models, which are domain-specific language models for scientific applications. The models utilize a byte-pair encoding algorithm to create a customized tokenizer, INDUSBPE, from a curated scientific corpus. Multiple encoder-only large language models (LLMs) were pretrained using this tokenizer and fine-tuned with a contrastive learning objective to produce sentence embeddings. The models were evaluated on various benchmarks, including NASA-QA, CLIMATE-CHANGE NER, and NASA-IR, demonstrating significant improvements in latency while maintaining strong empirical performance. The excerpt emphasizes the importance of high-quality in-domain corpora for model development.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt discusses the development of INDUS, a collection of encoder-based language models (LLMs) tailored for scientific applications. These models are fine-tuned using domain-specific vocabulary and corpora to enhance natural language understanding tasks. They include a contrastive-learning-based text embedding model for information retrieval tasks and smaller versions created via knowledge distillation for applications with latency or resource constraints. The models are evaluated using new scientific benchmark datasets, such as CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, and are shown to outperform both general-purpose encoders and existing domain-specific models like SCIBERT and BIOBERT on these tasks.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 8-8: The excerpt discusses the fine-tuning and evaluation of domain-specific language models, specifically the INDUS models, for scientific applications. Models were fine-tuned for 15 epochs using augmented training sets with relevant SQuAD pairs. Evaluation was conducted on the NASA-QA benchmark, with metrics such as F1 score and standard deviation reported. INDUSBASE achieved an F1 score of 68.2, outperforming other models like RoBERTa (66.8) and SCIBERT (63.5). Information retrieval benchmarks were also conducted on NASA-IR and BEIR datasets, using metrics like Recall@10 and average nDCG@10. The study highlights the use of custom tokenizers and in-domain data for training.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 8-9, bhattacharjee2024induseffectiveand pages 2-3, bhattacharjee2024induseffectiveand pages 2-2, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 8-8",
                "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725."
            },
            "devlin2019bertpretrainingof": {
                "context": "bhattacharjee2024induseffectiveand pages 8-9: The excerpt discusses the evaluation of domain-specific language models, specifically focusing on sentence embedding models for scientific text. These models were tested using the BEIR Benchmark, which includes 12 retrieval tasks across various domains. The primary metric used was the Normalized Cumulative Discount Gain (nDCG@10). The models significantly outperformed baselines on the NASA-IR task and maintained good performance on several BEIR tasks. Additionally, the INDUS-RETRIEVERSMALL model was noted for being about 4.6 times faster than INDUS-RETRIEVERBASE while achieving better performance on both NASA-IR and BEIR tasks.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt describes the creation and evaluation of domain-specific language models using knowledge-distillation techniques. Three new scientific benchmark datasets were developed: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, to enhance research in scientific applications. The models were trained on open-source data with permissive licenses, augmented with NASA-specific data, and included English Wikipedia for general language learning. The models demonstrated strong performance on these benchmarks, surpassing general-purpose models like RoBERTa and scientific-domain encoders like SCIBERT. The datasets cover various domains, with a total of 66.2 billion tokens, and the models' effectiveness is shown through experimental results.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt discusses the development and evaluation of the INDUS models, which are domain-specific language models for scientific applications. The models utilize a byte-pair encoding algorithm to create a customized tokenizer, INDUSBPE, from a curated scientific corpus. Multiple encoder-only large language models (LLMs) were pretrained using this tokenizer and fine-tuned with a contrastive learning objective to produce sentence embeddings. The models were evaluated on various benchmarks, including NASA-QA, CLIMATE-CHANGE NER, and NASA-IR, demonstrating significant improvements in latency while maintaining strong empirical performance. The excerpt emphasizes the importance of high-quality in-domain corpora for model development.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt discusses the development of INDUS, a collection of encoder-based language models (LLMs) tailored for scientific applications. These models are fine-tuned using domain-specific vocabulary and corpora to enhance natural language understanding tasks. They include a contrastive-learning-based text embedding model for information retrieval tasks and smaller versions created via knowledge distillation for applications with latency or resource constraints. The models are evaluated using new scientific benchmark datasets, such as CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, and are shown to outperform both general-purpose encoders and existing domain-specific models like SCIBERT and BIOBERT on these tasks.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 8-8: The excerpt discusses the fine-tuning and evaluation of domain-specific language models, specifically the INDUS models, for scientific applications. Models were fine-tuned for 15 epochs using augmented training sets with relevant SQuAD pairs. Evaluation was conducted on the NASA-QA benchmark, with metrics such as F1 score and standard deviation reported. INDUSBASE achieved an F1 score of 68.2, outperforming other models like RoBERTa (66.8) and SCIBERT (63.5). Information retrieval benchmarks were also conducted on NASA-IR and BEIR datasets, using metrics like Recall@10 and average nDCG@10. The study highlights the use of custom tokenizers and in-domain data for training.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 8-9, bhattacharjee2024induseffectiveand pages 2-3, bhattacharjee2024induseffectiveand pages 2-2, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 8-8",
                "citation": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423."
            }
        }
    },
    {
        "question": "What are the key challenges in creating domain-focused corpora for specialized tasks in scientific literature?",
        "answer": "Creating domain-focused corpora for specialized tasks in scientific literature presents several key challenges. One major issue is the interdisciplinary nature of fields such as physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. This diversity complicates the development of comprehensive models, as existing models like SCIBERT, BIOBERT, and SCHOLARBERT only partially cover these areas, and no single model encompasses all fields of interest (bhattacharjee2024induseffectiveand pages 1-1).\n\nAnother challenge is the distributional shift of vocabulary when using general-purpose corpora, such as Wikipedia or CommonCrawl, which leads to sub-optimal results in domain-specific tasks. This necessitates the creation of models tailored to specific scientific domains, trained on curated scientific corpora to align vocabulary and content with specialized needs (bhattacharjee2024induseffectiveand pages 1-1).\n\nThe process of assembling high-quality, domain-specific datasets is also complex, requiring meticulous curation from diverse sources to ensure comprehensive and representative data coverage. This involves selecting relevant literature from various scientific fields, which is crucial for supporting efficient access to scientific literature (bhattacharjee2024induseffectiveand pages 1-2). Additionally, the use of open-source data with permissive licenses, supplemented by specific data from organizations like NASA, is essential for creating these corpora (bhattacharjee2024induseffectiveand pages 2-3).",
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "context": "bhattacharjee2024induseffectiveand pages 1-1: The excerpt highlights the challenge of creating domain-focused corpora for specialized tasks in scientific literature due to the interdisciplinary nature of fields like physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. It notes that existing models like SCIBERT, BIOBERT, and SCHOLARBERT only partially cover these fields, and there is no specific model that collectively encompasses all areas of interest. The vast and scattered literature across diverse sources further complicates the development of comprehensive models. The research introduces INDUS, a collection of encoder-based language models, to address these challenges by focusing on interdisciplinary scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-2: The excerpt discusses the development of Indus, a collection of encoder-based large language models (LLMs) tailored for specific scientific domains. These models are trained using \"meticulously curated corpora from diverse sources,\" highlighting the challenge of assembling high-quality, domain-specific datasets. The process involves selecting relevant literature from fields such as Earth Science, BioMedical, Astrophysics, Astronomy, and General Science, which suggests the difficulty in ensuring comprehensive and representative data coverage. The excerpt implies that creating such corpora requires careful curation to support efficient access to scientific literature, which is crucial for research organizations and enterprises.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt highlights that large language models (LLMs) trained on general-purpose corpora, such as Wikipedia or CommonCrawl, often underperform on specialized tasks due to a \"distributional shift of vocabulary\" leading to sub-optimal results in domain-specific natural language understanding and generation tasks. This insight led to the development of INDUS, a suite of LLMs tailored for specific scientific domains like Earth science, biology, and astrophysics. These models are trained using curated scientific corpora, addressing the challenge of aligning vocabulary and content with the specialized needs of scientific literature.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt highlights the creation of domain-focused corpora using open-source data with permissive licenses, augmented by specific data from NASA and its providers. It mentions the development of three new scientific benchmark datasets: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, to enhance research in scientific applications. The data sources include the SAO/NASA Astrophysics Data System (ADS), which is a significant data source covering astronomy, astrophysics, physics, and general science. The excerpt also discusses the use of English Wikipedia to aid in learning general English and provides statistics on token counts from various datasets, indicating computational considerations. \n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt highlights the importance of \"sufficient high-quality in-domain corpora\" for developing models that outperform those trained on open-domain corpora. It emphasizes the meticulous identification of corpora for specific domains to create English-only models. The process involves utilizing a byte-pair encoding algorithm to construct a customized tokenizer, INDUSBPE, from a curated scientific corpus. Additionally, the excerpt discusses the pretraining of encoder-only language models (LLMs) using these curated corpora and the INDUSBPE tokenizer, followed by fine-tuning with a contrastive learning objective to develop \"universal\" sentence embeddings. \n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 1-2, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 2-3, bhattacharjee2024induseffectiveand pages 2-2",
                "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725."
            },
            "devlin2019bertpretrainingof": {
                "context": "bhattacharjee2024induseffectiveand pages 1-1: The excerpt highlights the challenge of creating domain-focused corpora for specialized tasks in scientific literature due to the interdisciplinary nature of fields like physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. It notes that existing models like SCIBERT, BIOBERT, and SCHOLARBERT only partially cover these fields, and there is no specific model that collectively encompasses all areas of interest. The vast and scattered literature across diverse sources further complicates the development of comprehensive models. The research introduces INDUS, a collection of encoder-based language models, to address these challenges by focusing on interdisciplinary scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-2: The excerpt discusses the development of Indus, a collection of encoder-based large language models (LLMs) tailored for specific scientific domains. These models are trained using \"meticulously curated corpora from diverse sources,\" highlighting the challenge of assembling high-quality, domain-specific datasets. The process involves selecting relevant literature from fields such as Earth Science, BioMedical, Astrophysics, Astronomy, and General Science, which suggests the difficulty in ensuring comprehensive and representative data coverage. The excerpt implies that creating such corpora requires careful curation to support efficient access to scientific literature, which is crucial for research organizations and enterprises.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt highlights that large language models (LLMs) trained on general-purpose corpora, such as Wikipedia or CommonCrawl, often underperform on specialized tasks due to a \"distributional shift of vocabulary\" leading to sub-optimal results in domain-specific natural language understanding and generation tasks. This insight led to the development of INDUS, a suite of LLMs tailored for specific scientific domains like Earth science, biology, and astrophysics. These models are trained using curated scientific corpora, addressing the challenge of aligning vocabulary and content with the specialized needs of scientific literature.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt highlights the creation of domain-focused corpora using open-source data with permissive licenses, augmented by specific data from NASA and its providers. It mentions the development of three new scientific benchmark datasets: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, to enhance research in scientific applications. The data sources include the SAO/NASA Astrophysics Data System (ADS), which is a significant data source covering astronomy, astrophysics, physics, and general science. The excerpt also discusses the use of English Wikipedia to aid in learning general English and provides statistics on token counts from various datasets, indicating computational considerations. \n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt highlights the importance of \"sufficient high-quality in-domain corpora\" for developing models that outperform those trained on open-domain corpora. It emphasizes the meticulous identification of corpora for specific domains to create English-only models. The process involves utilizing a byte-pair encoding algorithm to construct a customized tokenizer, INDUSBPE, from a curated scientific corpus. Additionally, the excerpt discusses the pretraining of encoder-only language models (LLMs) using these curated corpora and the INDUSBPE tokenizer, followed by fine-tuning with a contrastive learning objective to develop \"universal\" sentence embeddings. \n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 1-2, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 2-3, bhattacharjee2024induseffectiveand pages 2-2",
                "citation": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423."
            },
            "liu2019robertaarobustly": {
                "context": "bhattacharjee2024induseffectiveand pages 1-1: The excerpt highlights the challenge of creating domain-focused corpora for specialized tasks in scientific literature due to the interdisciplinary nature of fields like physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. It notes that existing models like SCIBERT, BIOBERT, and SCHOLARBERT only partially cover these fields, and there is no specific model that collectively encompasses all areas of interest. The vast and scattered literature across diverse sources further complicates the development of comprehensive models. The research introduces INDUS, a collection of encoder-based language models, to address these challenges by focusing on interdisciplinary scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-2: The excerpt discusses the development of Indus, a collection of encoder-based large language models (LLMs) tailored for specific scientific domains. These models are trained using \"meticulously curated corpora from diverse sources,\" highlighting the challenge of assembling high-quality, domain-specific datasets. The process involves selecting relevant literature from fields such as Earth Science, BioMedical, Astrophysics, Astronomy, and General Science, which suggests the difficulty in ensuring comprehensive and representative data coverage. The excerpt implies that creating such corpora requires careful curation to support efficient access to scientific literature, which is crucial for research organizations and enterprises.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt highlights that large language models (LLMs) trained on general-purpose corpora, such as Wikipedia or CommonCrawl, often underperform on specialized tasks due to a \"distributional shift of vocabulary\" leading to sub-optimal results in domain-specific natural language understanding and generation tasks. This insight led to the development of INDUS, a suite of LLMs tailored for specific scientific domains like Earth science, biology, and astrophysics. These models are trained using curated scientific corpora, addressing the challenge of aligning vocabulary and content with the specialized needs of scientific literature.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt highlights the creation of domain-focused corpora using open-source data with permissive licenses, augmented by specific data from NASA and its providers. It mentions the development of three new scientific benchmark datasets: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, to enhance research in scientific applications. The data sources include the SAO/NASA Astrophysics Data System (ADS), which is a significant data source covering astronomy, astrophysics, physics, and general science. The excerpt also discusses the use of English Wikipedia to aid in learning general English and provides statistics on token counts from various datasets, indicating computational considerations. \n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt highlights the importance of \"sufficient high-quality in-domain corpora\" for developing models that outperform those trained on open-domain corpora. It emphasizes the meticulous identification of corpora for specific domains to create English-only models. The process involves utilizing a byte-pair encoding algorithm to construct a customized tokenizer, INDUSBPE, from a curated scientific corpus. Additionally, the excerpt discusses the pretraining of encoder-only language models (LLMs) using these curated corpora and the INDUSBPE tokenizer, followed by fine-tuning with a contrastive learning objective to develop \"universal\" sentence embeddings. \n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 1-2, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 2-3, bhattacharjee2024induseffectiveand pages 2-2",
                "citation": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, Jul 2019."
            }
        }
    },
    {
        "question": "What methods are most effective in fine-tuning pre-trained language models for scientific tasks?",
        "answer": "Fine-tuning pre-trained language models for scientific tasks is most effective when leveraging domain-specific data and optimizing model architecture for efficiency. The development of models like INDUS, which are tailored for scientific domains such as Earth science, biology, and astrophysics, demonstrates the importance of using curated scientific corpora. These models address the limitations of general-purpose language models that often underperform on domain-specific tasks due to vocabulary distributional shifts (bhattacharjee2024induseffectiveand pages 1-1).\n\nThe effectiveness of domain-specific sentence embedding models is further highlighted by their performance in tasks like NASA-IR and BEIR benchmarks. These models significantly outperform baselines, as evidenced by metrics such as Normalized Cumulative Discount Gain (nDCG@10). The INDUS-RETRIEVERSMALL model, in particular, is noted for its superior performance and efficiency, being approximately 4.6 times faster than its larger counterpart, the INDUS-RETRIEVERBASE model (bhattacharjee2024induseffectiveand pages 8-9).\n\nOverall, fine-tuning pre-trained language models for scientific applications benefits from both the use of domain-specific data and the optimization of model architecture to enhance performance and efficiency in specialized tasks.",
        "documents": {
            "devlin2019bertpretrainingof": {
                "context": "devlin2019bertpretrainingof pages 2-3: The excerpt discusses the evolution of pre-training general language representations, highlighting unsupervised fine-tuning approaches. It mentions that early methods focused on pre-training word embeddings from unlabeled text, which were then fine-tuned for supervised tasks. Recent advancements include sentence or document encoders that produce contextual token representations, which are pre-trained and then fine-tuned, requiring fewer parameters to be learned from scratch. This approach has led to state-of-the-art results, as demonstrated by OpenAI GPT's performance on the GLUE benchmark. The excerpt emphasizes the effectiveness of these methods in improving NLP task performance.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 8-9: The excerpt discusses the performance of domain-specific sentence embedding models in scientific tasks, specifically highlighting their effectiveness in the NASA-IR task and several BEIR benchmark tasks. The models significantly outperformed baselines, as shown in Table 8, using the Normalized Cumulative Discount Gain (nDCG@10) metric. Additionally, the INDUS-RETRIEVERSMALL model was noted for its superior performance and efficiency, being about 4.6 times faster than the INDUS-RETRIEVERBASE model on both NASA-IR and BEIR tasks. This suggests that optimizing model architecture and efficiency can enhance the fine-tuning of pre-trained language models for scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\ndevlin2019bertpretrainingof pages 1-2: The excerpt discusses the limitations of unidirectional language models, such as OpenAI GPT, which use a left-to-right architecture restricting token attention to previous tokens. This is sub-optimal for sentence-level tasks and detrimental for token-level tasks like question answering, where bidirectional context is crucial. BERT (Bidirectional Encoder Representations from Transformers) addresses this by employing a \"masked language model\" (MLM) pre-training objective. MLM randomly masks tokens and predicts them based on context, allowing the model to incorporate both left and right context. BERT improves fine-tuning approaches, achieving significant performance gains: MultiNLI accuracy to 86.7% (4.6% improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1% improvement).\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\ndevlin2019bertpretrainingof pages 1-1: The excerpt discusses two strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. BERT, a model introduced in the paper, is designed to pre-train deep bidirectional representations from unlabeled text by conditioning on both left and right context. This allows BERT to be fine-tuned with minimal task-specific parameters, requiring only one additional output layer for various tasks. BERT achieves state-of-the-art results on eleven NLP tasks, including a GLUE score of 80.5%, without substantial task-specific architecture modifications. This suggests that fine-tuning BERT is highly effective for diverse language tasks.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt discusses the development of INDUS, a suite of large language models (LLMs) specifically tailored for scientific domains such as Earth science, biology, physics, heliophysics, planetary sciences, and astrophysics. These models are trained using curated scientific corpora, addressing the limitations of general-purpose LLMs that often perform sub-optimally on domain-specific tasks due to distributional shifts in vocabulary. The excerpt highlights the importance of using domain-focused corpora for training to enhance performance on specialized tasks, as demonstrated by models like SCIBERT. This suggests that fine-tuning with domain-specific data is effective for scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: devlin2019bertpretrainingof pages 2-3, bhattacharjee2024induseffectiveand pages 8-9, devlin2019bertpretrainingof pages 1-2, devlin2019bertpretrainingof pages 1-1, bhattacharjee2024induseffectiveand pages 1-1",
                "citation": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423."
            },
            "bhattacharjee2024induseffectiveand": {
                "context": "devlin2019bertpretrainingof pages 2-3: The excerpt discusses the evolution of pre-training general language representations, highlighting unsupervised fine-tuning approaches. It mentions that early methods focused on pre-training word embeddings from unlabeled text, which were then fine-tuned for supervised tasks. Recent advancements include sentence or document encoders that produce contextual token representations, which are pre-trained and then fine-tuned, requiring fewer parameters to be learned from scratch. This approach has led to state-of-the-art results, as demonstrated by OpenAI GPT's performance on the GLUE benchmark. The excerpt emphasizes the effectiveness of these methods in improving NLP task performance.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 8-9: The excerpt discusses the performance of domain-specific sentence embedding models in scientific tasks, specifically highlighting their effectiveness in the NASA-IR task and several BEIR benchmark tasks. The models significantly outperformed baselines, as shown in Table 8, using the Normalized Cumulative Discount Gain (nDCG@10) metric. Additionally, the INDUS-RETRIEVERSMALL model was noted for its superior performance and efficiency, being about 4.6 times faster than the INDUS-RETRIEVERBASE model on both NASA-IR and BEIR tasks. This suggests that optimizing model architecture and efficiency can enhance the fine-tuning of pre-trained language models for scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\ndevlin2019bertpretrainingof pages 1-2: The excerpt discusses the limitations of unidirectional language models, such as OpenAI GPT, which use a left-to-right architecture restricting token attention to previous tokens. This is sub-optimal for sentence-level tasks and detrimental for token-level tasks like question answering, where bidirectional context is crucial. BERT (Bidirectional Encoder Representations from Transformers) addresses this by employing a \"masked language model\" (MLM) pre-training objective. MLM randomly masks tokens and predicts them based on context, allowing the model to incorporate both left and right context. BERT improves fine-tuning approaches, achieving significant performance gains: MultiNLI accuracy to 86.7% (4.6% improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1% improvement).\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\ndevlin2019bertpretrainingof pages 1-1: The excerpt discusses two strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. BERT, a model introduced in the paper, is designed to pre-train deep bidirectional representations from unlabeled text by conditioning on both left and right context. This allows BERT to be fine-tuned with minimal task-specific parameters, requiring only one additional output layer for various tasks. BERT achieves state-of-the-art results on eleven NLP tasks, including a GLUE score of 80.5%, without substantial task-specific architecture modifications. This suggests that fine-tuning BERT is highly effective for diverse language tasks.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt discusses the development of INDUS, a suite of large language models (LLMs) specifically tailored for scientific domains such as Earth science, biology, physics, heliophysics, planetary sciences, and astrophysics. These models are trained using curated scientific corpora, addressing the limitations of general-purpose LLMs that often perform sub-optimally on domain-specific tasks due to distributional shifts in vocabulary. The excerpt highlights the importance of using domain-focused corpora for training to enhance performance on specialized tasks, as demonstrated by models like SCIBERT. This suggests that fine-tuning with domain-specific data is effective for scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: devlin2019bertpretrainingof pages 2-3, bhattacharjee2024induseffectiveand pages 8-9, devlin2019bertpretrainingof pages 1-2, devlin2019bertpretrainingof pages 1-1, bhattacharjee2024induseffectiveand pages 1-1",
                "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725."
            },
            "liu2019robertaarobustly": {
                "context": "devlin2019bertpretrainingof pages 2-3: The excerpt discusses the evolution of pre-training general language representations, highlighting unsupervised fine-tuning approaches. It mentions that early methods focused on pre-training word embeddings from unlabeled text, which were then fine-tuned for supervised tasks. Recent advancements include sentence or document encoders that produce contextual token representations, which are pre-trained and then fine-tuned, requiring fewer parameters to be learned from scratch. This approach has led to state-of-the-art results, as demonstrated by OpenAI GPT's performance on the GLUE benchmark. The excerpt emphasizes the effectiveness of these methods in improving NLP task performance.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 8-9: The excerpt discusses the performance of domain-specific sentence embedding models in scientific tasks, specifically highlighting their effectiveness in the NASA-IR task and several BEIR benchmark tasks. The models significantly outperformed baselines, as shown in Table 8, using the Normalized Cumulative Discount Gain (nDCG@10) metric. Additionally, the INDUS-RETRIEVERSMALL model was noted for its superior performance and efficiency, being about 4.6 times faster than the INDUS-RETRIEVERBASE model on both NASA-IR and BEIR tasks. This suggests that optimizing model architecture and efficiency can enhance the fine-tuning of pre-trained language models for scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\ndevlin2019bertpretrainingof pages 1-2: The excerpt discusses the limitations of unidirectional language models, such as OpenAI GPT, which use a left-to-right architecture restricting token attention to previous tokens. This is sub-optimal for sentence-level tasks and detrimental for token-level tasks like question answering, where bidirectional context is crucial. BERT (Bidirectional Encoder Representations from Transformers) addresses this by employing a \"masked language model\" (MLM) pre-training objective. MLM randomly masks tokens and predicts them based on context, allowing the model to incorporate both left and right context. BERT improves fine-tuning approaches, achieving significant performance gains: MultiNLI accuracy to 86.7% (4.6% improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1% improvement).\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\ndevlin2019bertpretrainingof pages 1-1: The excerpt discusses two strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. BERT, a model introduced in the paper, is designed to pre-train deep bidirectional representations from unlabeled text by conditioning on both left and right context. This allows BERT to be fine-tuned with minimal task-specific parameters, requiring only one additional output layer for various tasks. BERT achieves state-of-the-art results on eleven NLP tasks, including a GLUE score of 80.5%, without substantial task-specific architecture modifications. This suggests that fine-tuning BERT is highly effective for diverse language tasks.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 1-1: The excerpt discusses the development of INDUS, a suite of large language models (LLMs) specifically tailored for scientific domains such as Earth science, biology, physics, heliophysics, planetary sciences, and astrophysics. These models are trained using curated scientific corpora, addressing the limitations of general-purpose LLMs that often perform sub-optimally on domain-specific tasks due to distributional shifts in vocabulary. The excerpt highlights the importance of using domain-focused corpora for training to enhance performance on specialized tasks, as demonstrated by models like SCIBERT. This suggests that fine-tuning with domain-specific data is effective for scientific applications.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: devlin2019bertpretrainingof pages 2-3, bhattacharjee2024induseffectiveand pages 8-9, devlin2019bertpretrainingof pages 1-2, devlin2019bertpretrainingof pages 1-1, bhattacharjee2024induseffectiveand pages 1-1",
                "citation": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, Jul 2019."
            }
        }
    },
    {
        "question": "Explain the gaps presented in each paper, and their commonalities.",
        "answer": "The papers by Bhattacharjee et al. and Devlin et al. address gaps in language model performance and application, particularly in scientific and general domains, respectively. Bhattacharjee et al. focus on the development of the INDUS language models, which are tailored for scientific applications. They identify a gap in the performance of general-purpose language models when applied to domain-specific tasks, such as those in Earth science. To address this, they introduce specialized datasets like NASA-QA and CLIMATE-CHANGE NER, and develop the INDUSBPE tokenizer to optimize processing of scientific text, thereby improving model efficiency and performance (bhattacharjee2024induseffectiveand pages 2-2, 2-3).\n\nDevlin et al., on the other hand, highlight the limitations in training methodologies between BERT and other models like GPT. They note that BERT's pre-training on a larger and more diverse corpus, including Wikipedia, contributes to its superior performance on various tasks. However, they also acknowledge challenges in fine-tuning BERT for specific tasks, such as those in the GLUE benchmark, where data quality and task-specific performance remain issues (devlin2019bertpretrainingof pages 14-15, 15-15).\n\nBoth papers emphasize the importance of domain-specific data and task-specific training to enhance model performance, highlighting a commonality in addressing the limitations of general-purpose models in specialized applications.",
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "context": "bhattacharjee2024induseffectiveand pages 6-7: The excerpt discusses the evaluation of the INDUS language models for scientific applications, specifically focusing on extractive question answering in the Earth science domain. The dataset used includes 534 abstracts from the Semantic Scholar Academic Graph, annotated with the IOB tagging scheme. The NASA-QA task involves 39 paragraphs from Earth science papers, with questions formulated by NASA experts. INDUS models are compared against open-source models like RoBERTaBASE, SCIBERT, MINILM, and TINYBERT. Performance metrics such as Recall@10 and Entity F1 scores are reported, with INDUSBASE and INDUSSMALL generally outperforming their counterparts across various datasets.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt discusses the development of INDUS, a set of language models tailored for scientific applications. Key contributions include the creation of INDUSBPE, a tokenizer optimized for scientific text, and the pretraining of encoder-only large language models (LLMs) using this tokenizer. These models were further refined to produce sentence embeddings through contrastive learning. The study highlights the importance of high-quality, domain-specific corpora for enhancing model performance over those trained on general corpora. The models were evaluated using various benchmarks, including new ones like NASA-QA and CLIMATE-CHANGE NER, demonstrating improved latency and empirical performance.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\ndevlin2019bertpretrainingof pages 14-15: The excerpt highlights differences in training methodologies between BERT and GPT, which contribute to their empirical improvements. BERT is trained on both the BooksCorpus (800M words) and Wikipedia (2,500M words), whereas GPT is only trained on the BooksCorpus. BERT incorporates [SEP], [CLS], and sentence A/B embeddings during pre-training, while GPT introduces these elements only at fine-tuning. BERT is trained for 1M steps with a batch size of 128,000 words, compared to GPT's 1M steps with a batch size of 32,000 words. Additionally, BERT uses a task-specific fine-tuning learning rate, unlike GPT's uniform rate of 5e-5.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt discusses the development of Indus, a language model tailored for scientific applications, using open-source data and specific NASA datasets. It introduces three new scientific benchmark datasets: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, designed to enhance research in various scientific domains. The models demonstrate strong performance on these benchmarks, surpassing general-purpose models like RoBERTa and domain-specific models like SCIBERT. The data sources include the SAO/NASA Astrophysics Data System, with a total of 66.2 billion tokens across domains such as Earth Science, Biomedical, and Astronomy. The excerpt highlights the efficiency of the INDUSBPE tokenizer in reducing computational costs.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\ndevlin2019bertpretrainingof pages 15-15: The excerpt discusses the fine-tuning of BERT on various tasks, including SST-2, CoLA, RTE, and STS-B, which are part of the GLUE benchmark. SST-2 is a binary classification task for sentiment analysis of movie reviews. CoLA involves predicting linguistic acceptability of sentences. RTE is a binary entailment task with limited training data. STS-B measures semantic similarity between sentence pairs. The excerpt notes issues with the WNLI dataset, where systems perform worse than the baseline, leading to its exclusion from certain evaluations. These tasks highlight challenges in data quality and task-specific performance.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nValid keys: bhattacharjee2024induseffectiveand pages 6-7, bhattacharjee2024induseffectiveand pages 2-2, devlin2019bertpretrainingof pages 14-15, bhattacharjee2024induseffectiveand pages 2-3, devlin2019bertpretrainingof pages 15-15",
                "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725."
            },
            "devlin2019bertpretrainingof": {
                "context": "bhattacharjee2024induseffectiveand pages 6-7: The excerpt discusses the evaluation of the INDUS language models for scientific applications, specifically focusing on extractive question answering in the Earth science domain. The dataset used includes 534 abstracts from the Semantic Scholar Academic Graph, annotated with the IOB tagging scheme. The NASA-QA task involves 39 paragraphs from Earth science papers, with questions formulated by NASA experts. INDUS models are compared against open-source models like RoBERTaBASE, SCIBERT, MINILM, and TINYBERT. Performance metrics such as Recall@10 and Entity F1 scores are reported, with INDUSBASE and INDUSSMALL generally outperforming their counterparts across various datasets.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt discusses the development of INDUS, a set of language models tailored for scientific applications. Key contributions include the creation of INDUSBPE, a tokenizer optimized for scientific text, and the pretraining of encoder-only large language models (LLMs) using this tokenizer. These models were further refined to produce sentence embeddings through contrastive learning. The study highlights the importance of high-quality, domain-specific corpora for enhancing model performance over those trained on general corpora. The models were evaluated using various benchmarks, including new ones like NASA-QA and CLIMATE-CHANGE NER, demonstrating improved latency and empirical performance.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\ndevlin2019bertpretrainingof pages 14-15: The excerpt highlights differences in training methodologies between BERT and GPT, which contribute to their empirical improvements. BERT is trained on both the BooksCorpus (800M words) and Wikipedia (2,500M words), whereas GPT is only trained on the BooksCorpus. BERT incorporates [SEP], [CLS], and sentence A/B embeddings during pre-training, while GPT introduces these elements only at fine-tuning. BERT is trained for 1M steps with a batch size of 128,000 words, compared to GPT's 1M steps with a batch size of 32,000 words. Additionally, BERT uses a task-specific fine-tuning learning rate, unlike GPT's uniform rate of 5e-5.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nbhattacharjee2024induseffectiveand pages 2-3: The excerpt discusses the development of Indus, a language model tailored for scientific applications, using open-source data and specific NASA datasets. It introduces three new scientific benchmark datasets: CLIMATE-CHANGE NER, NASA-QA, and NASA-IR, designed to enhance research in various scientific domains. The models demonstrate strong performance on these benchmarks, surpassing general-purpose models like RoBERTa and domain-specific models like SCIBERT. The data sources include the SAO/NASA Astrophysics Data System, with a total of 66.2 billion tokens across domains such as Earth Science, Biomedical, and Astronomy. The excerpt highlights the efficiency of the INDUSBPE tokenizer in reducing computational costs.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\ndevlin2019bertpretrainingof pages 15-15: The excerpt discusses the fine-tuning of BERT on various tasks, including SST-2, CoLA, RTE, and STS-B, which are part of the GLUE benchmark. SST-2 is a binary classification task for sentiment analysis of movie reviews. CoLA involves predicting linguistic acceptability of sentences. RTE is a binary entailment task with limited training data. STS-B measures semantic similarity between sentence pairs. The excerpt notes issues with the WNLI dataset, where systems perform worse than the baseline, leading to its exclusion from certain evaluations. These tasks highlight challenges in data quality and task-specific performance.\n\n8\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423.\n\nValid keys: bhattacharjee2024induseffectiveand pages 6-7, bhattacharjee2024induseffectiveand pages 2-2, devlin2019bertpretrainingof pages 14-15, bhattacharjee2024induseffectiveand pages 2-3, devlin2019bertpretrainingof pages 15-15",
                "citation": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. Unknown journal, pages 4171-4186, 2019. URL: https://doi.org/10.18653/v1/n19-1423, doi:10.18653/v1/n19-1423."
            }
        }
    },
    {
        "question": "How does INDUS compare to the other models, and what are the salient features of the models?",
        "answer": "The INDUS models are a collection of encoder-based large language models (LLMs) specifically designed for scientific applications across various interdisciplinary fields, including physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. These models are trained using domain-specific vocabulary and corpora, which enhances their performance in natural language understanding tasks (bhattacharjee2024induseffectiveand pages 1-1).\n\nINDUS models, particularly INDUSBASE and INDUSSMALL, demonstrate superior performance compared to other models. For instance, INDUSBASE achieved an F1 score of 68.2 on the NASA-QA benchmark, outperforming models like RoBERTa and SCIBERT (bhattacharjee2024induseffectiveand pages 8-8). On the BLURB benchmark, INDUSBASE scored 64.0, surpassing RoBERTa and SCIBERT, while INDUSSMALL outperformed MINILM and TINYBERT (bhattacharjee2024induseffectiveand pages 7-7).\n\nA salient feature of the INDUS models is the use of a custom tokenizer, INDUSBPE, created through a byte-pair encoding algorithm from a curated scientific corpus. This tokenizer, along with the use of knowledge distillation techniques, allows for the development of smaller model versions that maintain strong empirical performance while offering reduced latency (bhattacharjee2024induseffectiveand pages 2-2). Additionally, INDUS models excel in tasks like CLIMATE-CHANGE NER, highlighting the benefits of training on large domain-specific datasets (bhattacharjee2024induseffectiveand pages 7-8).",
        "documents": {
            "bhattacharjee2024induseffectiveand": {
                "context": "bhattacharjee2024induseffectiveand pages 8-8: The excerpt discusses the performance of the INDUS models compared to other language models in scientific applications. INDUSBASE achieved an F1 score of 68.2 (SD 2.9) on the NASA-QA benchmark, outperforming models of similar sizes like RoBERTa (66.8) and SCIBERT (63.5). INDUSSMALL also showed strong performance relative to its counterparts, with an F1 score of 47.4 (SD 1.8). The research highlights the use of a custom tokenizer and in-domain data for training, as well as the development of smaller model versions using knowledge distillation techniques. These models are designed for applications with latency or resource constraints.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 7-7: The excerpt provides evaluation results of the INDUS models compared to other models on the BLURB benchmark, which focuses on natural language understanding and reasoning tasks in the biomedical domain. INDUS-RETRIEVERBASE and INDUS-RETRIEVERSMALL were compared to BGEBASE15, RoBERTaBASE, MINILM-V216, and BGESMALL17. INDUSBASE achieved an F1 score of 64.0 (1.0), outperforming RoBERTa (60.8) and SCIBERT (61.8). INDUSSMALL scored 54.8 (0.8), surpassing MINILM (44.7) and TINYBERT (34.3). The results indicate that INDUS models generally perform better than their counterparts, with bold and underlined results indicating the highest performance and significant differences. \n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 1-1: INDUS is a collection of encoder-based large language models (LLMs) designed for scientific applications, particularly in interdisciplinary fields such as physics, Earth science, astrophysics, heliophysics, planetary sciences, and biology. It includes models trained using domain-specific vocabulary and corpora to enhance natural language understanding tasks. Additionally, INDUS features a contrastive-learning-based text embedding model for information retrieval tasks and smaller model versions created via knowledge distillation for applications with latency or resource constraints. INDUS models outperform both general-purpose encoders like RoBERTa and existing domain-specific encoders such as SCIBERT on new scientific benchmark datasets, including CLIMATE-CHANGE NER, NASA-QA, and NASA-IR. \n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 7-8: INDUS models, particularly INDUSBASE and INDUSSMALL, demonstrate superior performance compared to other models. INDUSBASE significantly outperforms the general-purpose RoBERTa model in micro/macro average scores and is competitive with SCIBERT, a bio-domain-specific model. INDUSSMALL surpasses TINYBERT and MINILM by a large margin in tasks like NER, PICO, relation extraction, and document classification, highlighting the effectiveness of knowledge distillation from the domain-specific INDUSBASE. Additionally, INDUS models excel in the CLIMATE-CHANGE NER task, outperforming baseline models, which underscores the benefits of training on large domain-specific datasets.\n\n9\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nbhattacharjee2024induseffectiveand pages 2-2: The excerpt discusses the INDUS models, highlighting their construction and evaluation. INDUS utilizes a byte-pair encoding algorithm to create a customized tokenizer, INDUSBPE, from a curated scientific corpus. Multiple encoder-only large language models (LLMs) were pretrained using this tokenizer, and sentence-embedding models were developed through fine-tuning with a contrastive learning objective. The models were evaluated on various benchmarks, including new ones like NASA-QA, CLIMATE-CHANGE NER, and NASA-IR. The knowledge-distilled models showed a significant boost in latency while maintaining strong empirical performance compared to the original models on most benchmark tasks.\n\n8\nFrom Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725.\n\nValid keys: bhattacharjee2024induseffectiveand pages 8-8, bhattacharjee2024induseffectiveand pages 7-7, bhattacharjee2024induseffectiveand pages 1-1, bhattacharjee2024induseffectiveand pages 7-8, bhattacharjee2024induseffectiveand pages 2-2",
                "citation": "Bishwaranjan Bhattacharjee, Aashka Trivedi, Masayasu Muraoka, Muthukumaran Ramasubramanian, Takuma Udagawa, I. Gurung, Rong Zhang, Bharath Dandala, Rahul Ramachandran, M. Maskey, Kayleen Bugbee, Mike Little, Elizabeth Fancher, Lauren Sanders, Sylvain Costes, Sergi Blanco-Cuaresma, Kelly Lockhart, Thomas Allen, Felix Grazes, Megan Ansdel, A. Accomazzi, Yousef El-Kurdi, Davis Wertheimer, Birgit Pfitzmann, Cesar Berrospi Ramis, Michele Dolfi, Rafael Teixeira de Lima, Panos Vegenas, S. K. Mukkavilli, P. Staar, S. Vahidinia, Ryan McGranaghan, A. Mehrabian, and Tsendgar Lee. Indus: effective and efficient language models for scientific applications. ArXiv, May 2024. URL: https://doi.org/10.48550/arxiv.2405.10725, doi:10.48550/arxiv.2405.10725."
            }
        }
    }
]
